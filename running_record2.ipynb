{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v20000\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "#%%\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import NN7 as NN\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((Rx, Rs, Hhat, Rb), f'rid{rid}_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "rid = 'v20000' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 1500\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "model = NN(M,K,N).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(epoch, Rb[0], Rb.sum()/3/128)\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v20010\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "#%%\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import NN7 as NN\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((Rx, Rs, Hhat, Rb), f'rid{rid}_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "rid = 'v20010' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 1500\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "model = NN(M,K,N).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.1)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n",
    "        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n",
    "print('done')\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v20100\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "from vae_model import NN7 as NN\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((Rx, Rs, Hhat, Rb), f'rid{rid}_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "rid = 'v20100' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-4\n",
    "opts['n_epochs'] = 1500\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "model = NN(M,K,N).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n",
    "        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n",
    "print('done')\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v20110\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "from vae_model import NN7 as NN\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((Rx, Rs, Hhat, Rb), f'rid{rid}_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "rid = 'v20110' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 1500\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "model = NN(M,K,N).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n",
    "        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n",
    "print('done')\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v20200\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "#%%\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import NN7 as NN\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((Rx, Rs, Hhat, Rb), f'rid{rid}_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "rid = 'v20200' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-4\n",
    "opts['n_epochs'] = 1500\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "model = NN(M,K,N).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(),\n",
    "                lr= opts['lr'])\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n",
    "        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n",
    "print('done')\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v20210\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "#%%\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import NN7 as NN\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((Rx, Rs, Hhat, Rb), f'rid{rid}_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "rid = 'v20210' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 1500\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "model = NN(M,K,N).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(),\n",
    "                lr= opts['lr'])\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n",
    "        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n",
    "print('done')\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v20220\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "#%%\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import NN7 as NN\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((Rx, Rs, Hhat, Rb), f'rid{rid}_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "rid = 'v20220' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-4\n",
    "opts['n_epochs'] = 1500\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "model = NN(M,K,N).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.1)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(),\n",
    "                lr= opts['lr'])\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n",
    "        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n",
    "print('done')\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v20230\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "#%%\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import NN7 as NN\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((Rx, Rs, Hhat, Rb), f'rid{rid}_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "rid = 'v20230' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 1500\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "model = NN(M,K,N).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.1)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(),\n",
    "                lr= opts['lr'])\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n",
    "        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n",
    "print('done')\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v21000\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import NN7 as NN\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "#%%\n",
    "rid = 'v21000' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 1500\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "model = NN(M,K,N).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(epoch, Rb[0], Rb.sum()/3/128)\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v22000\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN8(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        # Estimate H and coarse V\n",
    "        self.v_net = nn.Sequential(\n",
    "            DoubleConv(in_channels=M*2, out_channels=1),\n",
    "            ) \n",
    "        self.v_out = OutConv(in_channels=1, out_channels=1)\n",
    "        self.hb_net = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=1),\n",
    "            Down(in_channels=1, out_channels=1),\n",
    "            Down(in_channels=1, out_channels=1),\n",
    "            Reshape(-1, 12*12),\n",
    "            )\n",
    "        # Estimate H\n",
    "        self.h_net = nn.Sequential(\n",
    "            LinearBlock(12*12, 64),\n",
    "            LinearBlock(64, 32),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Tanh()\n",
    "            )   \n",
    "        # Estimate Rb\n",
    "        self.b_net = nn.Sequential(\n",
    "            LinearBlock(12*12, 64),\n",
    "            LinearBlock(64, 32),\n",
    "            nn.Linear(32, 1),\n",
    "            )   \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            DoubleConv(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "\n",
    "        \"Neural nets for H,V\"\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                temp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - temp.squeeze().permute(2,3,0,1)\n",
    "            temp = self.v_net(torch.cat((inp.real, inp.imag), dim=1)).exp() \n",
    "            vj = self.v_out(temp).exp() #sigma_s**2 >=0\n",
    "            vj = threshold(vj, floor=1e-3, ceiling=1e3)  # shape of [I, 1, N, F]\n",
    "            hb = self.hb_net(vj)\n",
    "            ang = self.h_net(hb)  # shape of [I,1]\n",
    "            sig_b_squared = self.b_net(hb).exp() # shape of [I,1]\n",
    "            \"Get H\"\n",
    "            ch = torch.pi*torch.arange(self.M, device=ang.device)\n",
    "            hj = ((ang @ ch[None,:])*1j).exp() # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Get Rb, the energy of the rest\"\n",
    "            Rb = threshold(sig_b_squared[:,:,None]**2, 1e-3, 1e2)*torch.ones(batch_size, \\\n",
    "                self.M, device=ch.device).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rs = vj.permute(2,3,0,1)[..., None].to(torch.cfloat)  #shape of [N,F,I,1,1]\n",
    "            Rx = hj[...,None] @ Rs @ hj[:,None].conj() + Rb # shape of [N,F,I,M,M]\n",
    "            W = Rs @ hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "        \n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "\n",
    "#%%\n",
    "rid = 'v22000' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 100\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 1500\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "NN = NN8\n",
    "model = NN(M,K,N).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(f'epoch{epoch}', Rb[0], Rb.sum()/3/128)\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v22100\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN8(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        # Estimate H and coarse V\n",
    "        self.v_net = nn.Sequential(\n",
    "            DoubleConv(in_channels=M*2, out_channels=1),\n",
    "            ) \n",
    "        self.v_out = OutConv(in_channels=1, out_channels=1)\n",
    "        self.hb_net = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=1),\n",
    "            Down(in_channels=1, out_channels=1),\n",
    "            Down(in_channels=1, out_channels=1),\n",
    "            Reshape(-1, 12*12),\n",
    "            )\n",
    "        # Estimate H\n",
    "        self.h_net = nn.Sequential(\n",
    "            LinearBlock(12*12, 64),\n",
    "            LinearBlock(64, 32),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Tanh()\n",
    "            )   \n",
    "        # Estimate Rb\n",
    "        self.b_net = nn.Sequential(\n",
    "            LinearBlock(12*12, 64),\n",
    "            LinearBlock(64, 32),\n",
    "            nn.Linear(32, 1),\n",
    "            )   \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            DoubleConv(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "\n",
    "        \"Neural nets for H,V\"\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                temp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - temp.squeeze().permute(2,3,0,1)\n",
    "            temp = self.v_net(torch.cat((inp.real, inp.imag), dim=1)).exp() \n",
    "            vj = self.v_out(temp).exp() #sigma_s**2 >=0\n",
    "            vj = threshold(vj, floor=1e-3, ceiling=1e3)  # shape of [I, 1, N, F]\n",
    "            hb = self.hb_net(vj)\n",
    "            ang = self.h_net(hb)  # shape of [I,1]\n",
    "            sig_b_squared = self.b_net(hb).exp() # shape of [I,1]\n",
    "            \"Get H\"\n",
    "            ch = torch.pi*torch.arange(self.M, device=ang.device)\n",
    "            hj = ((ang @ ch[None,:])*1j).exp() # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Get Rb, the energy of the rest\"\n",
    "            Rb = threshold(sig_b_squared[:,:,None]**2, 1e-3, 1e2)*torch.ones(batch_size, \\\n",
    "                self.M, device=ch.device).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rs = vj.permute(2,3,0,1)[..., None].to(torch.cfloat)  #shape of [N,F,I,1,1]\n",
    "            Rx = hj[...,None] @ Rs @ hj[:,None].conj() + Rb # shape of [N,F,I,M,M]\n",
    "            W = Rs @ hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "        \n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "\n",
    "#%%\n",
    "rid = 'v22100' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 100\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 1500\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "NN = NN8\n",
    "model = NN(M,K,N).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(f'epoch{epoch}', Rb[0], Rb.sum()/3/128)\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v23000\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "\n",
    "class NN9(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        # Estimate H and coarse V\n",
    "        self.v_net = nn.Sequential(\n",
    "            DoubleConv(in_channels=M*2, out_channels=32),\n",
    "            DoubleConv(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            ) \n",
    "        self.v_out = OutConv(in_channels=4, out_channels=1)\n",
    "        self.hb_net = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            Down(in_channels=16, out_channels=8),\n",
    "            Reshape(-1, 8*12*12),\n",
    "            )\n",
    "        # Estimate H\n",
    "        self.h_net = nn.Sequential(\n",
    "            LinearBlock(8*12*12, 64),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Tanh()\n",
    "            )   \n",
    "        # Estimate Rb\n",
    "        self.b_net = nn.Sequential(\n",
    "            LinearBlock(8*12*12, 64),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Linear(32, 1),\n",
    "            )   \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            DoubleConv(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "\n",
    "        \"Neural nets for H,V\"\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "            temp = self.v_net(torch.cat((inp.real, inp.imag), dim=1)).exp() \n",
    "            vj = self.v_out(temp).exp() #sigma_s**2 >=0\n",
    "            vj = threshold(vj, floor=1e-3, ceiling=1e2)  # shape of [I, 1, N, F]\n",
    "            hb = self.hb_net(vj)\n",
    "            ang = self.h_net(hb)  # shape of [I,1]\n",
    "            sig_b_squared = self.b_net(hb).exp() # shape of [I,1]\n",
    "            \"Get H\"\n",
    "            ch = torch.pi*torch.arange(self.M, device=ang.device)\n",
    "            hj = ((ang @ ch[None,:])*1j).exp() # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Get Rb, the energy of the rest\"\n",
    "            Rb = sig_b_squared[:,:,None]*torch.ones(batch_size, \\\n",
    "                self.M, device=ch.device).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rs = vj.mean(dim=(2,3)).to(torch.cfloat)[..., None]  #shape of [I,1,1]\n",
    "            Rx = hj[...,None] @ Rs @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = Rs @ hj[:, None,].conj() @ Rx.inverse()  # shape of [I,1,M]\n",
    "            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "        \n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "#%%\n",
    "rid = 'v23000' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 1500\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "NN = NN9\n",
    "model = NN(M,K,N).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(epoch, Rb[0], Rb.sum()/3/128)\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v23100\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "\n",
    "class NN10(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        # Estimate H and coarse V\n",
    "        self.v_net = nn.Sequential(\n",
    "            DoubleConv(in_channels=M*2, out_channels=1),\n",
    "            ) \n",
    "        self.v_out = OutConv(in_channels=1, out_channels=1)\n",
    "        self.hb_net = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=1),\n",
    "            Down(in_channels=1, out_channels=1),\n",
    "            Down(in_channels=1, out_channels=1),\n",
    "            Reshape(-1, 12*12),\n",
    "            )\n",
    "        # Estimate H\n",
    "        self.h_net = nn.Sequential(\n",
    "            LinearBlock(12*12, 64),\n",
    "            LinearBlock(64, 32),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Tanh()\n",
    "            )   \n",
    "        # Estimate Rb\n",
    "        self.b_net = nn.Sequential(\n",
    "            LinearBlock(12*12, 64),\n",
    "            LinearBlock(64, 32),\n",
    "            nn.Linear(32, 1),\n",
    "            )   \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            DoubleConv(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "\n",
    "        \"Neural nets for H,V\"\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "            temp = self.v_net(torch.cat((inp.real, inp.imag), dim=1)).exp() \n",
    "            vj = self.v_out(temp).exp() #sigma_s**2 >=0\n",
    "            vj = threshold(vj, floor=1e-3, ceiling=1e2)  # shape of [I, 1, N, F]\n",
    "            hb = self.hb_net(vj)\n",
    "            ang = self.h_net(hb)  # shape of [I,1]\n",
    "            sig_b_squared = self.b_net(hb).exp() # shape of [I,1]\n",
    "            \"Get H\"\n",
    "            ch = torch.pi*torch.arange(self.M, device=ang.device)\n",
    "            hj = ((ang @ ch[None,:])*1j).exp() # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Get Rb, the energy of the rest\"\n",
    "            Rb = sig_b_squared[:,:,None]*torch.ones(batch_size, \\\n",
    "                self.M, device=ch.device).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rs = vj.mean(dim=(2,3)).to(torch.cfloat)[..., None]  #shape of [I,1,1]\n",
    "            Rx = hj[...,None] @ Rs @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = Rs @ hj[:, None,].conj() @ Rx.inverse()  # shape of [I,1,M]\n",
    "            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "        \n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "#%%\n",
    "rid = 'v23100' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 1500\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "NN = NN10\n",
    "model = NN(M,K,N).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(epoch, Rb[0], Rb.sum()/3/128)\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v23200\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "\n",
    "class NN11(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        # Estimate H and coarse V\n",
    "        self.est = nn.Sequential(\n",
    "            Down(in_channels=M*2, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=8),\n",
    "            Reshape(-1, 8*12*12),\n",
    "            LinearBlock(8*12*12, 64),\n",
    "            nn.Linear(64, 3),\n",
    "            )   \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            DoubleConv(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "\n",
    "        \"Neural nets for H,V\"\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "            res = self.est(torch.cat((inp.real, inp.imag), dim=1)) #vj,Rb,ang\n",
    "            vj = threshold(res[:, 0:1].exp(), floor=1e-3, ceiling=1e2)\n",
    "            sb = threshold(res[:, 1:2].exp(), floor=1e-3, ceiling=1e2)\n",
    "            Rb = (sb*torch.ones(batch_size, self.M, \\\n",
    "                device=sb.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "\n",
    "            ch = torch.pi*torch.arange(self.M, device=res.device)\n",
    "            hj = ((res[:, 2:].tanh() @ ch[None,:])*1j).exp() # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rs = vj[..., None].to(torch.cfloat)  #shape of [I,1,1]\n",
    "            Rx = hj[...,None] @ Rs @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = Rs @ hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "        \n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "#%%\n",
    "rid = 'v23200' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 1500\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "NN = NN11\n",
    "model = NN(M,K,N).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(epoch, Rb[0], Rb.sum()/3/128)\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v23300\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN12(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        # Estimate H and coarse V\n",
    "        self.est = nn.Sequential(\n",
    "            Down(in_channels=M*2, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=4),\n",
    "            Reshape(-1, 4*12*12),\n",
    "            LinearBlock(4*12*12, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            )\n",
    "        self.b1 = nn.Linear(100, 1)\n",
    "        self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            DoubleConv(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "\n",
    "        \"Neural nets for H,V\"\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "            ang = self.est(torch.cat((inp.real, inp.imag), dim=1)) #vj,Rb,ang\n",
    "\n",
    "            sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "                device=sb.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "\n",
    "            ch = torch.pi*torch.arange(self.M, device=inp.device)\n",
    "            hj = ((ang.tanh() @ ch[None,:])*1j).exp() # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "        \n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "#%%\n",
    "rid = 'v23300' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 1500\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "NN = NN12\n",
    "model = NN(M,K,N).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(epoch, Rb[0], Rb.sum()/3/128, hh)\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)), '\\n')\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v23310\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN12(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        # Estimate H and coarse V\n",
    "        self.est = nn.Sequential(\n",
    "            Down(in_channels=M*2, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=4),\n",
    "            Reshape(-1, 4*12*12),\n",
    "            LinearBlock(4*12*12, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            )\n",
    "        self.b1 = nn.Linear(100, 1)\n",
    "        self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            DoubleConv(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "\n",
    "        \"Neural nets for H,V\"\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "            ang = self.est(torch.cat((inp.real, inp.imag), dim=1)) #vj,Rb,ang\n",
    "\n",
    "            sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "                device=sb.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "\n",
    "            ch = torch.pi*torch.arange(self.M, device=inp.device)\n",
    "            hj = ((ang.tanh() @ ch[None,:])*1j).exp() # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "        \n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "#%%\n",
    "rid = 'v23310' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 1500\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "NN = NN12\n",
    "model = NN(M,K,N).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(epoch, Rb[0], Rb.sum()/3/128, hh)\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)), '\\n')\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v23400\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN13(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        # Estimate H and coarse V\n",
    "        self.est = nn.Sequential(\n",
    "            Down(in_channels=M*2, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=4),\n",
    "            Reshape(-1, 4*12*12),\n",
    "            LinearBlock(4*12*12, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            )\n",
    "        self.b1 = nn.Linear(100, 1)\n",
    "        self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "\n",
    "        \"Neural nets for H,V\"\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "            ang = self.est(torch.cat((inp.real, inp.imag), dim=1)) #vj,Rb,ang\n",
    "\n",
    "            sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "                device=sb.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "\n",
    "            ch = torch.pi*torch.arange(self.M, device=inp.device)\n",
    "            hj = ((ang.tanh() @ ch[None,:])*1j).exp() # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "        \n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "#%%\n",
    "rid = 'v23400' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 1500\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "NN = NN13\n",
    "model = NN(M,K,N).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(epoch, Rb[0], Rb.sum()/3/128, hh)\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)), '\\n')\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v23410\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN13(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        # Estimate H and coarse V\n",
    "        self.est = nn.Sequential(\n",
    "            Down(in_channels=M*2, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=4),\n",
    "            Reshape(-1, 4*12*12),\n",
    "            LinearBlock(4*12*12, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            )\n",
    "        self.b1 = nn.Linear(100, 1)\n",
    "        self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "\n",
    "        \"Neural nets for H,V\"\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "            ang = self.est(torch.cat((inp.real, inp.imag), dim=1)) #vj,Rb,ang\n",
    "\n",
    "            sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "                device=sb.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "\n",
    "            ch = torch.pi*torch.arange(self.M, device=inp.device)\n",
    "            hj = ((ang.tanh() @ ch[None,:])*1j).exp() # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "        \n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "#%%\n",
    "rid = 'v23410' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 1500\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "NN = NN13\n",
    "model = NN(M,K,N).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(epoch, Rb[0], Rb.sum()/3/128, hh)\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)), '\\n')\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v24000\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN14(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        # Estimate H and coarse V\n",
    "        self.est = nn.Sequential(\n",
    "            Down(in_channels=M*2, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=4),\n",
    "            Reshape(-1, 4*12*12),\n",
    "            LinearBlock(4*12*12, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            )\n",
    "        self.b1 = nn.Linear(100, 1)\n",
    "        self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "\n",
    "        \"Neural nets for H,V\"\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "            ang = self.est(torch.cat((inp.real, inp.imag), dim=1)) #vj,Rb,ang\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "\n",
    "            ch = torch.pi*torch.arange(self.M, device=inp.device)\n",
    "            hj = ((ang.tanh() @ ch[None,:])*1j).exp() # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "    \n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "#%%\n",
    "rid = 'v24000' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 1500\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "NN = NN14\n",
    "model = NN(M,K,N).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(epoch, Rb[0], Rb.sum()/3/128, hh)\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)), '\\n')\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% t2 -- test sbd with fewer layers, one batch, \n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtdecoder(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        # Estimate H and coarse V\n",
    "        # self.est = nn.Sequential(\n",
    "        #     Down(in_channels=M*2, out_channels=64),\n",
    "        #     Down(in_channels=64, out_channels=32),\n",
    "        #     Down(in_channels=32, out_channels=4),\n",
    "        #     Reshape(-1, 4*12*12),\n",
    "        #     LinearBlock(4*12*12, 64),\n",
    "        #     nn.Linear(64, 1),\n",
    "        #     )\n",
    "        # self.b1 = nn.Linear(100, 1)\n",
    "        # self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "\n",
    "        \"Neural nets for H,V\"\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "#%%\n",
    "rid = 't2' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 30 # how many samples ------------------------------------ TODO\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = I # how many samples ------------------------------------ TODO\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 3000\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d[:I], snr=30, seed=1) # ------------------------------------ TODO\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:30].to(torch.cfloat).cuda()  # ------------------------------------ TODO\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "loss1, loss2, loss3 = [], [], []\n",
    "NN = NN_gtdecoder\n",
    "model = NN(M,K,N).cuda()\n",
    "\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    # loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    # loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    # loss3.append(l3.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%20 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(loss1, '-or')\n",
    "        # plt.title(f'Loss1 fuction at epoch{epoch}')\n",
    "        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(loss2, '-or')\n",
    "        # plt.title(f'Loss2 fuction at epoch{epoch}')\n",
    "        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(loss3, '-or')\n",
    "        # plt.title(f'Loss3 fuction at epoch{epoch}')\n",
    "        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss  = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/30)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(Rb[0], Rb.sum()/3/30)\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)))\n",
    "\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% t2_more -- test sbd with more layers, one batch,\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtdecoder(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "         \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            DoubleConv(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "\n",
    "        \"Neural nets for H,V\"\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "#%%\n",
    "rid = 't2_more' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 30 # how many samples ------------------------------------ TODO\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = I # how many samples ------------------------------------ TODO\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 3000\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d[:I], snr=30, seed=1) # ------------------------------------ TODO\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:30].to(torch.cfloat).cuda()  # ------------------------------------ TODO\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "loss1, loss2, loss3 = [], [], []\n",
    "NN = NN_gtdecoder\n",
    "model = NN(M,K,N).cuda()\n",
    "\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    # loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    # loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    # loss3.append(l3.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%20 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(loss1, '-or')\n",
    "        # plt.title(f'Loss1 fuction at epoch{epoch}')\n",
    "        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(loss2, '-or')\n",
    "        # plt.title(f'Loss2 fuction at epoch{epoch}')\n",
    "        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(loss3, '-or')\n",
    "        # plt.title(f'Loss3 fuction at epoch{epoch}')\n",
    "        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss  = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/30)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(Rb[0], Rb.sum()/3/30)\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)))\n",
    "\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% t3 -- upconv with fewer layers, one batch,\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtupconv(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            Up_(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "\n",
    "        \"Neural nets for H,V\"\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "#%%\n",
    "#@title\n",
    "rid = 't3' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 30 # how many samples ------------------------------------ TODO\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = I # how many samples ------------------------------------ TODO\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 3000\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d[:I], snr=30, seed=1) # ------------------------------------ TODO\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:30].to(torch.cfloat).cuda()  # ------------------------------------ TODO\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "loss1, loss2, loss3 = [], [], []\n",
    "NN = NN_gtupconv\n",
    "model = NN(M,K,N).cuda()\n",
    "\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    # loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    # loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    # loss3.append(l3.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%20 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(loss1, '-or')\n",
    "        # plt.title(f'Loss1 fuction at epoch{epoch}')\n",
    "        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(loss2, '-or')\n",
    "        # plt.title(f'Loss2 fuction at epoch{epoch}')\n",
    "        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(loss3, '-or')\n",
    "        # plt.title(f'Loss3 fuction at epoch{epoch}')\n",
    "        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss  = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/30)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(Rb[0], Rb.sum()/3/30)\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)))\n",
    "\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% t3_more --upconv with more layers, one batch,\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtupconv(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Up_(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "\n",
    "        \"Neural nets for H,V\"\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "#%%\n",
    "rid = 't3_more' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 30 # how many samples ------------------------------------ TODO\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = I # how many samples ------------------------------------ TODO\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 3000\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d[:I], snr=30, seed=1) # ------------------------------------ TODO\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:30].to(torch.cfloat).cuda()  # ------------------------------------ TODO\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "loss1, loss2, loss3 = [], [], []\n",
    "NN = NN_gtupconv\n",
    "model = NN(M,K,N).cuda()\n",
    "\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    # loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    # loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    # loss3.append(l3.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%20 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(loss1, '-or')\n",
    "        # plt.title(f'Loss1 fuction at epoch{epoch}')\n",
    "        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(loss2, '-or')\n",
    "        # plt.title(f'Loss2 fuction at epoch{epoch}')\n",
    "        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(loss3, '-or')\n",
    "        # plt.title(f'Loss3 fuction at epoch{epoch}')\n",
    "        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss  = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/30)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(Rb[0], Rb.sum()/3/30)\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)))\n",
    "\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% t2_05 -- test sbd with fewer layers, one batch, beta=0.5(t2 as 1e-3)\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtdecoder(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        # Estimate H and coarse V\n",
    "        # self.est = nn.Sequential(\n",
    "        #     Down(in_channels=M*2, out_channels=64),\n",
    "        #     Down(in_channels=64, out_channels=32),\n",
    "        #     Down(in_channels=32, out_channels=4),\n",
    "        #     Reshape(-1, 4*12*12),\n",
    "        #     LinearBlock(4*12*12, 64),\n",
    "        #     nn.Linear(64, 1),\n",
    "        #     )\n",
    "        # self.b1 = nn.Linear(100, 1)\n",
    "        # self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "\n",
    "        \"Neural nets for H,V\"\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "#%%\n",
    "rid = 't2_05' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 30 # how many samples ------------------------------------ TODO\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = I # how many samples ------------------------------------ TODO\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 2000\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d[:I], snr=30, seed=1) # ------------------------------------ TODO\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:30].to(torch.cfloat).cuda()  # ------------------------------------ TODO\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "loss1, loss2, loss3 = [], [], []\n",
    "NN = NN_gtdecoder\n",
    "model = NN(M,K,N).cuda()\n",
    "\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    # loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    # loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    # loss3.append(l3.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%40 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(loss1, '-or')\n",
    "        # plt.title(f'Loss1 fuction at epoch{epoch}')\n",
    "        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(loss2, '-or')\n",
    "        # plt.title(f'Loss2 fuction at epoch{epoch}')\n",
    "        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(loss3, '-or')\n",
    "        # plt.title(f'Loss3 fuction at epoch{epoch}')\n",
    "        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss  = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/30)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(Rb[0], Rb.sum()/3/30)\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)))\n",
    "\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% t2_05sig -- test sbd with fewer layers, one batch, beta=0.5(t2 as 1e-3), sigmoid*10\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtdecoder(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        # Estimate H and coarse V\n",
    "        # self.est = nn.Sequential(\n",
    "        #     Down(in_channels=M*2, out_channels=64),\n",
    "        #     Down(in_channels=64, out_channels=32),\n",
    "        #     Down(in_channels=32, out_channels=4),\n",
    "        #     Reshape(-1, 4*12*12),\n",
    "        #     LinearBlock(4*12*12, 64),\n",
    "        #     nn.Linear(64, 1),\n",
    "        #     )\n",
    "        # self.b1 = nn.Linear(100, 1)\n",
    "        # self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "\n",
    "        \"Neural nets for H,V\"\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).sigmoid()*10\n",
    "            v_all.append(threshold(v, floor=1e-3)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "#%%\n",
    "rid = 't2_05sig' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 30 # how many samples ------------------------------------ TODO\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = I # how many samples ------------------------------------ TODO\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 2000\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d[:I], snr=30, seed=1) # ------------------------------------ TODO\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:30].to(torch.cfloat).cuda()  # ------------------------------------ TODO\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "loss1, loss2, loss3 = [], [], []\n",
    "NN = NN_gtdecoder\n",
    "model = NN(M,K,N).cuda()\n",
    "\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    # loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    # loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    # loss3.append(l3.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%40 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(loss1, '-or')\n",
    "        # plt.title(f'Loss1 fuction at epoch{epoch}')\n",
    "        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(loss2, '-or')\n",
    "        # plt.title(f'Loss2 fuction at epoch{epoch}')\n",
    "        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(loss3, '-or')\n",
    "        # plt.title(f'Loss3 fuction at epoch{epoch}')\n",
    "        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss  = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/30)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(Rb[0], Rb.sum()/3/30)\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)))\n",
    "\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% t2_05sig_e-4 -- test sbd with fewer layers, one batch, beta=0.5(t2 as 1e-3), sigmoid*10, lr=1e-4\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtdecoder(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        # Estimate H and coarse V\n",
    "        # self.est = nn.Sequential(\n",
    "        #     Down(in_channels=M*2, out_channels=64),\n",
    "        #     Down(in_channels=64, out_channels=32),\n",
    "        #     Down(in_channels=32, out_channels=4),\n",
    "        #     Reshape(-1, 4*12*12),\n",
    "        #     LinearBlock(4*12*12, 64),\n",
    "        #     nn.Linear(64, 1),\n",
    "        #     )\n",
    "        # self.b1 = nn.Linear(100, 1)\n",
    "        # self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "\n",
    "        \"Neural nets for H,V\"\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).sigmoid()*10\n",
    "            v_all.append(threshold(v, floor=1e-3)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "#%%\n",
    "rid = 't2_05sige-4' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 30 # how many samples ------------------------------------ TODO\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = I # how many samples ------------------------------------ TODO\n",
    "opts['lr'] = 1e-4\n",
    "opts['n_epochs'] = 2000\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d[:I], snr=30, seed=1) # ------------------------------------ TODO\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:30].to(torch.cfloat).cuda()  # ------------------------------------ TODO\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "loss1, loss2, loss3 = [], [], []\n",
    "NN = NN_gtdecoder\n",
    "model = NN(M,K,N).cuda()\n",
    "\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    # loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    # loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    # loss3.append(l3.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%40 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(loss1, '-or')\n",
    "        # plt.title(f'Loss1 fuction at epoch{epoch}')\n",
    "        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(loss2, '-or')\n",
    "        # plt.title(f'Loss2 fuction at epoch{epoch}')\n",
    "        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.plot(loss3, '-or')\n",
    "        # plt.title(f'Loss3 fuction at epoch{epoch}')\n",
    "        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss  = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/30)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(Rb[0], Rb.sum()/3/30)\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)))\n",
    "\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
