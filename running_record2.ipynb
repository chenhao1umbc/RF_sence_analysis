{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v20000\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "#%%\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import NN7 as NN\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    return -ll.sum().real + beta*kl\n",
    "\n",
    "rid = 'v20000' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "I = 300 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 1500\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss_eval = [], [], []\n",
    "model = NN(M,K,N).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append(loss.cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n",
    "        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n",
    "print('done')\n",
    "# %%\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
