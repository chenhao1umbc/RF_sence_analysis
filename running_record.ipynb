{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s0\n",
    "rid = 's0' # running id\n",
    "from math import ceil\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtsbd(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        # Estimate H and coarse V\n",
    "        # self.est = nn.Sequential(\n",
    "        #     Down(in_channels=M*2, out_channels=64),\n",
    "        #     Down(in_channels=64, out_channels=32),\n",
    "        #     Down(in_channels=32, out_channels=4),\n",
    "        #     Reshape(-1, 4*12*12),\n",
    "        #     LinearBlock(4*12*12, 64),\n",
    "        #     nn.Linear(64, 1),\n",
    "        #     )\n",
    "        # self.b1 = nn.Linear(100, 1)\n",
    "        # self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat_all[i][:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 150\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_gtsbd\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s1\n",
    "rid = 's1' # running id\n",
    "from math import ceil\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtsbd(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        # Estimate H and coarse V\n",
    "        # self.est = nn.Sequential(\n",
    "        #     Down(in_channels=M*2, out_channels=64),\n",
    "        #     Down(in_channels=64, out_channels=32),\n",
    "        #     Down(in_channels=32, out_channels=4),\n",
    "        #     Reshape(-1, 4*12*12),\n",
    "        #     LinearBlock(4*12*12, 64),\n",
    "        #     nn.Linear(64, 1),\n",
    "        #     )\n",
    "        # self.b1 = nn.Linear(100, 1)\n",
    "        # self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat_all[i][:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-2):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 150\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_gtsbd\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s2\n",
    "rid = 's2' # running id\n",
    "from math import ceil\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtsbd(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        # Estimate H and coarse V\n",
    "        # self.est = nn.Sequential(\n",
    "        #     Down(in_channels=M*2, out_channels=64),\n",
    "        #     Down(in_channels=64, out_channels=32),\n",
    "        #     Down(in_channels=32, out_channels=4),\n",
    "        #     Reshape(-1, 4*12*12),\n",
    "        #     LinearBlock(4*12*12, 64),\n",
    "        #     nn.Linear(64, 1),\n",
    "        #     )\n",
    "        # self.b1 = nn.Linear(100, 1)\n",
    "        # self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat_all[i][:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.1):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 150\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_gtsbd\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s3\n",
    "rid = 's3' # running id\n",
    "from math import ceil\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtsbd(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        # Estimate H and coarse V\n",
    "        # self.est = nn.Sequential(\n",
    "        #     Down(in_channels=M*2, out_channels=64),\n",
    "        #     Down(in_channels=64, out_channels=32),\n",
    "        #     Down(in_channels=32, out_channels=4),\n",
    "        #     Reshape(-1, 4*12*12),\n",
    "        #     LinearBlock(4*12*12, 64),\n",
    "        #     nn.Linear(64, 1),\n",
    "        #     )\n",
    "        # self.b1 = nn.Linear(100, 1)\n",
    "        # self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat_all[i][:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 150\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_gtsbd\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s4\n",
    "rid = 's4' # running id\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtupconv(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Up_(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 150\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_gtupconv\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s5\n",
    "rid = 's5' # running id\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtupconv(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Up_(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-2):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 150\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_gtupconv\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s6\n",
    "rid = 's6' # running id\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtupconv(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Up_(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.1):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 150\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_gtupconv\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s7\n",
    "rid = 's7' # running id\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtupconv(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Up_(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 150\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_gtupconv\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s8\n",
    "rid = 's8' # running id\n",
    "from math import ceil\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtsbd(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        # Estimate H and coarse V\n",
    "        # self.est = nn.Sequential(\n",
    "        #     Down(in_channels=M*2, out_channels=64),\n",
    "        #     Down(in_channels=64, out_channels=32),\n",
    "        #     Down(in_channels=32, out_channels=4),\n",
    "        #     Reshape(-1, 4*12*12),\n",
    "        #     LinearBlock(4*12*12, 64),\n",
    "        #     nn.Linear(64, 1),\n",
    "        #     )\n",
    "        # self.b1 = nn.Linear(100, 1)\n",
    "        # self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            DoubleConv(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat_all[i][:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 150\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_gtsbd\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s9\n",
    "rid = 's9' # running id\n",
    "from math import ceil\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtsbd(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        # Estimate H and coarse V\n",
    "        # self.est = nn.Sequential(\n",
    "        #     Down(in_channels=M*2, out_channels=64),\n",
    "        #     Down(in_channels=64, out_channels=32),\n",
    "        #     Down(in_channels=32, out_channels=4),\n",
    "        #     Reshape(-1, 4*12*12),\n",
    "        #     LinearBlock(4*12*12, 64),\n",
    "        #     nn.Linear(64, 1),\n",
    "        #     )\n",
    "        # self.b1 = nn.Linear(100, 1)\n",
    "        # self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            DoubleConv(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat_all[i][:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-2):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 150\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_gtsbd\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s10\n",
    "rid = 's10' # running id\n",
    "from math import ceil\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtsbd(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        # Estimate H and coarse V\n",
    "        # self.est = nn.Sequential(\n",
    "        #     Down(in_channels=M*2, out_channels=64),\n",
    "        #     Down(in_channels=64, out_channels=32),\n",
    "        #     Down(in_channels=32, out_channels=4),\n",
    "        #     Reshape(-1, 4*12*12),\n",
    "        #     LinearBlock(4*12*12, 64),\n",
    "        #     nn.Linear(64, 1),\n",
    "        #     )\n",
    "        # self.b1 = nn.Linear(100, 1)\n",
    "        # self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            DoubleConv(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat_all[i][:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.1):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 150\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_gtsbd\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s11\n",
    "rid = 's11' # running id\n",
    "from math import ceil\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtsbd(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        # Estimate H and coarse V\n",
    "        # self.est = nn.Sequential(\n",
    "        #     Down(in_channels=M*2, out_channels=64),\n",
    "        #     Down(in_channels=64, out_channels=32),\n",
    "        #     Down(in_channels=32, out_channels=4),\n",
    "        #     Reshape(-1, 4*12*12),\n",
    "        #     LinearBlock(4*12*12, 64),\n",
    "        #     nn.Linear(64, 1),\n",
    "        #     )\n",
    "        # self.b1 = nn.Linear(100, 1)\n",
    "        # self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DoubleConv(in_channels=self.dz+2, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            DoubleConv(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "        self.im_size = im_size\n",
    "        x = torch.linspace(-1, 1, im_size)\n",
    "        y = torch.linspace(-1, 1, im_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y)\n",
    "        # Add as constant, with extra dims for N and C\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat_all[i][:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            # View z as 4D tensor to be tiled across new N and F dimensions            \n",
    "            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n",
    "            # Tile across to match image size\n",
    "            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n",
    "            # Expand grids to batches and concatenate on the channel dimension\n",
    "            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n",
    "                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n",
    "            v = self.decoder(zbd).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 150\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_gtsbd\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s12\n",
    "rid = 's12' # running id\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtupconv_less(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            Up_(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 150\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_gtupconv_less\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s13\n",
    "rid = 's13' # running id\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtupconv_less(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            Up_(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-2):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 150\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_gtupconv_less\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s14\n",
    "rid = 's14' # running id\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtupconv_less(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            Up_(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.1):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 150\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_gtupconv_less\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s15\n",
    "rid = 's15' # running id\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtupconv_less(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            Up_(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 150\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_gtupconv_less\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s4_\n",
    "rid = 's4_' # running id\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtupconv(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Up_(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat_all[i][:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 150\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_gtupconv\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s5_\n",
    "rid = 's5_' # running id\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtupconv(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Up_(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat_all[i][:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-2):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 150\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_gtupconv\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s6_\n",
    "rid = 's6_' # running id\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtupconv(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Up_(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat_all[i][:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.1):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 150\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_gtupconv\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s7_\n",
    "rid = 's7_' # running id\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtupconv(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Up_(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat_all[i][:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 150\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_gtupconv\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s12_\n",
    "rid = 's12_' # running id\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtupconv_less(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            Up_(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat_all[i][:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 150\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_gtupconv_less\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s15_\n",
    "rid = 's15_' # running id\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_gtupconv_less(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            Up_(in_channels=64, out_channels=16),\n",
    "            OutConv(in_channels=16, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "\n",
    "            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n",
    "            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n",
    "            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat_all[i][:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 150\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_gtupconv_less\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s16\n",
    "rid = 's16' # running id\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_upconv(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "        # Estimate H \n",
    "        self.est = nn.Sequential(\n",
    "            Down(in_channels=M*2, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=4),\n",
    "            Reshape(-1, 4*12*12),\n",
    "            LinearBlock(4*12*12, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            )\n",
    "        # self.b1 = nn.Linear(100, 1)\n",
    "        # self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Up_(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        ch = torch.pi*torch.arange(self.M, device=x.device)\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "                # inp = (inp - tmp.squeeze().permute(2,3,0,1)).detach()\n",
    "\n",
    "            ang = self.est(torch.cat((inp.real, inp.imag), dim=1)) #vj,Rb,ang\n",
    "            hj = ((ang.tanh() @ ch[None,:])*1j).exp() # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat_all[i][:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 1000\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_upconv\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)), '\\n')\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s17\n",
    "rid = 's17' # running id\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_upconv(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "        # Estimate H \n",
    "        self.est = nn.Sequential(\n",
    "            Down(in_channels=M*2, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=4),\n",
    "            Reshape(-1, 4*12*12),\n",
    "            LinearBlock(4*12*12, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            )\n",
    "        # self.b1 = nn.Linear(100, 1)\n",
    "        # self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Up_(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        ch = torch.pi*torch.arange(self.M, device=x.device)\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                # inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "                inp = (inp - tmp.squeeze().permute(2,3,0,1)).detach()\n",
    "\n",
    "            ang = self.est(torch.cat((inp.real, inp.imag), dim=1)) #vj,Rb,ang\n",
    "            hj = ((ang.tanh() @ ch[None,:])*1j).exp() # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat_all[i][:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 1000\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_upconv\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)), '\\n')\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s16_1\n",
    "rid = 's16_1' # running id\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_upconv(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "        # Estimate H \n",
    "        self.est = nn.Sequential(\n",
    "            Down(in_channels=M*2, out_channels=1),\n",
    "            Down(in_channels=1, out_channels=1),\n",
    "            Down(in_channels=1, out_channels=1),\n",
    "            Reshape(-1, 12*12),\n",
    "            LinearBlock(12*12, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            )\n",
    "        # self.b1 = nn.Linear(100, 1)\n",
    "        # self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Up_(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        ch = torch.pi*torch.arange(self.M, device=x.device)\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "                # inp = (inp - tmp.squeeze().permute(2,3,0,1)).detach()\n",
    "\n",
    "            ang = self.est(torch.cat((inp.real, inp.imag), dim=1)) #vj,Rb,ang\n",
    "            hj = ((ang.tanh() @ ch[None,:])*1j).exp() # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat_all[i][:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 1000\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_upconv\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        if epoch <=100:\n",
    "            l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3)\n",
    "        else:\n",
    "            l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3*epoch)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)), '\\n')\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s16_2\n",
    "rid = 's16_2' # running id\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_upconv(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "        # Estimate H \n",
    "        self.est1 = nn.Linear(100, 1)\n",
    "        self.est2 = nn.Sequential(\n",
    "            nn.Linear(100, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        # self.b1 = nn.Linear(100, 1)\n",
    "        # self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Up_(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        ch = torch.arange(self.M, device=x.device).to(torch.float) #not *torch.pi here\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "                # inp = (inp - tmp.squeeze().permute(2,3,0,1)).detach()\n",
    "\n",
    "            r_ang = self.est2(self.est1(inp.real).squeeze()) #vj,Rb,ang\n",
    "            i_ang = self.est2(self.est1(inp.real).squeeze())\n",
    "            ang = (r_ang+1j*i_ang).squeeze().mean(-1).angle()\n",
    "            hj = ((ang[...,None]@ch[None,:])*1j).exp() # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat_all[i][:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 1000\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_upconv\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        if epoch <=100:\n",
    "            l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3)\n",
    "        else:\n",
    "            l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3*epoch)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)), '\\n')\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s16_3\n",
    "rid = 's16_3' # running id\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_upconv(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "        # Estimate H \n",
    "        self.est = nn.Sequential(\n",
    "            Down(in_channels=M*2, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=4),\n",
    "            Reshape(-1, 4*12*12),\n",
    "            LinearBlock(4*12*12, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            )\n",
    "        # self.b1 = nn.Linear(100, 1)\n",
    "        # self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Up_(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        ch = torch.pi*torch.arange(self.M, device=x.device)\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "                # inp = (inp - tmp.squeeze().permute(2,3,0,1)).detach()\n",
    "\n",
    "            ang = self.est(torch.cat((inp.real, inp.imag), dim=1)) #vj,Rb,ang\n",
    "            hj = ((ang.tanh() @ ch[None,:])*1j).exp() # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat_all[i][:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 1000\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_upconv\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        if epoch <=100:\n",
    "            l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3)\n",
    "        else:\n",
    "            l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3*epoch)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)), '\\n')\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s16_cont\n",
    "rid = 's16_cont' # running id\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_upconv(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.K, self.M = K, M\n",
    "        # Estimate H \n",
    "        self.est = nn.Sequential(\n",
    "            Down(in_channels=M*2, out_channels=64),\n",
    "            Down(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=4),\n",
    "            Reshape(-1, 4*12*12),\n",
    "            LinearBlock(4*12*12, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            )\n",
    "        # self.b1 = nn.Linear(100, 1)\n",
    "        # self.b2 = nn.Linear(100, 1)\n",
    "           \n",
    "        # Estimate V using auto encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Up_(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        I = x.shape[0]\n",
    "        \"Neural nets for H,V\"\n",
    "        shat_all = []\n",
    "        ch = torch.pi*torch.arange(self.M, device=x.device)\n",
    "        for i in range(self.K):\n",
    "            if i == 0:\n",
    "                inp = x\n",
    "            else:\n",
    "                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n",
    "                inp = inp - tmp.squeeze().permute(2,3,0,1)\n",
    "                # inp = (inp - tmp.squeeze().permute(2,3,0,1)).detach()\n",
    "\n",
    "            ang = self.est(torch.cat((inp.real, inp.imag), dim=1)) #vj,Rb,ang\n",
    "            hj = ((ang.tanh() @ ch[None,:])*1j).exp() # shape:[I, M]\n",
    "            h_all.append(hj)\n",
    "\n",
    "            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n",
    "                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n",
    "\n",
    "            \"Wienter filter to get coarse shat\"\n",
    "            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n",
    "            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n",
    "            shat = (W @ inp.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n",
    "            shat = shat/shat.detach().abs().max()\n",
    "            shat_all.append(shat)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat_all[i][:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(batch_size,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).exp()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.sum().real, beta*kl\n",
    "\n",
    "#%%\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, K = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 2000\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_upconv\n",
    "model = NN(M,K,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval, '-xb')\n",
    "            plt.title(f'Accumulated validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "\n",
    "            hh, rs0= Hhat[0], Rs[0]\n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)), '\\n')\n",
    "            for ii in range(K):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "\n",
    "                plt.figure()\n",
    "                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                \n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v1 \n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "rid = 'v1'\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "\n",
    "#%%\n",
    "M = 3\n",
    "dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "v0 = dicts['v'][..., 0]\n",
    "v1 = dicts['v'][..., 1]\n",
    "from skimage.transform import resize\n",
    "v0 = torch.tensor(resize(v0, (100, 100), preserve_range=True))\n",
    "v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "plt.imshow(v0.abs())\n",
    "plt.colorbar()\n",
    "v1 = torch.tensor(resize(v1, (100, 100), preserve_range=True))\n",
    "v1 = awgn(v1, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "\n",
    "snr=0; n_data=int(1.1e4)\n",
    "delta = v0.mean()*10**(-snr/10)\n",
    "angs = (torch.rand(n_data,1)*20 +10)/180*np.pi  # signal aoa [10, 30]\n",
    "h = (1j*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "angs_n1 = (torch.rand(n_data,1)*20 -70)/180*np.pi  # noise aoa [-70, -50]\n",
    "hs_n1 = (1j*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "angs_n2 = (torch.rand(n_data,1)*20 +120)/180*np.pi  # noise aoa [120, 140]\n",
    "hs_n2 = (1j*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "signal = (h[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:]).reshape(n_data, M, 100, 100)\n",
    "# n1 = (hs_n1[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:]).reshape(n_data, M, 100, 100)\n",
    "n1 = hs_n1[...,None] @ torch.randn(1, torch.tensor(v0.shape).prod(), dtype=torch.cfloat)*delta**0.5 \n",
    "n2 = hs_n2[...,None] @ torch.randn(1, torch.tensor(v0.shape).prod(), dtype=torch.cfloat)*delta**0.5\n",
    "mix =  signal + n1.reshape(n_data, M, 100, 100) + n2.reshape(n_data, M, 100, 100)   \n",
    "mix_all, sig_all = mix.permute(0,2,3,1), signal.permute(0,2,3,1)\n",
    "\n",
    "# torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "plt.figure()\n",
    "plt.imshow(mix_all[0,:,:,0].abs())\n",
    "plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(10000, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "#%% load data and model\n",
    "class DOA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rx_net = nn.Sequential(\n",
    "            nn.Linear(18,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Linear(128,128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.Linear(128,128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.Linear(128,128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,18),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(18,18)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        rx_inv = self.rx_net(x)\n",
    "\n",
    "        return rx_inv \n",
    "model = DOA().cuda()\n",
    "\n",
    "#%%\n",
    "h_all, M = h, sig_all.shape[-1]\n",
    "n_tr = int(1e4)\n",
    "const_range = torch.arange(M).to(torch.cfloat)[None,:].cuda()\n",
    "data = Data.TensorDataset(mix_all[:n_tr], sig_all[:n_tr], h_all[:n_tr])\n",
    "val0 = mix_all[n_tr:n_tr+200]\n",
    "rx_val = (val0[...,None] @ val0[:,:,:,None,:].conj()).mean(dim=(1,2))\n",
    "rx_val_cuda = rx_val.cuda()\n",
    "inp_val = torch.stack((rx_val.real, rx_val.imag), dim=1).reshape(rx_val.shape[0], -1).cuda()\n",
    "tr = Data.DataLoader(data, batch_size=32, drop_last=True, shuffle=True)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= 1e-4,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "Is = torch.stack([torch.eye(3,3)]*32, dim=0).to(torch.cfloat).cuda()\n",
    "Is2 = torch.stack([torch.eye(3,3)]*200, dim=0).to(torch.cfloat).cuda()\n",
    "\n",
    "loss_all, loss_val_all = [], []\n",
    "for epoch in range(2001):\n",
    "    for i, (mix, sig, _) in enumerate(tr):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \"prepare for input to the NN\"\n",
    "        mix = mix.cuda()\n",
    "        sig = sig.cuda()\n",
    "        Rx = mix[...,None] @ mix[:,:,:,None,:].conj()\n",
    "        rx = Rx.mean(dim=(1,2))\n",
    "        inp = torch.stack((rx.real, rx.imag), dim=1).reshape(mix.shape[0], -1)\n",
    "        rx_raw = model(inp)\n",
    "        rx_reshape = rx_raw.reshape(mix.shape[0],M,M,2)\n",
    "        rx_inv_hat = rx_reshape[...,0] + 1j*rx_reshape[...,1]\n",
    "\n",
    "        \"Calc gradient and update\"\n",
    "        loss = ((Is-rx_inv_hat@rx).abs()**2).mean()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        if i % 30 == 0:\n",
    "            loss_all.append(loss.detach().cpu().item())\n",
    "        \n",
    "    if epoch % 20 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_all, '-x')\n",
    "        plt.title(f'epoch {epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        if epoch >50:\n",
    "            plt.figure()\n",
    "            plt.plot(loss_all[-100:], '-rx')\n",
    "            plt.title(f'epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_100_at_epoch{epoch}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rx_raw = model(inp_val)\n",
    "            rx_reshape = rx_raw.reshape(inp_val.shape[0],M,M,2)\n",
    "            rx_inv_hat = rx_reshape[...,0] + 1j*rx_reshape[...,1]\n",
    "            loss_val = ((Is2-rx_inv_hat@rx_val_cuda).abs()**2).mean()\n",
    "            loss_val_all.append(loss_val.cpu().item())\n",
    "            print('epoch', epoch, rx_inv_hat[:3]@rx_val_cuda[:3])\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all, '-x')\n",
    "            plt.title(f'val loss at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_validation_loss')\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all[-50:], '-rx')\n",
    "            plt.title(f'last 50 val loss epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_50val_at_epoch{epoch}')\n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"v2 For now, I need to verify that the covariance inverse works\"\n",
    "#%% \n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "rid = 'v2'\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "\n",
    "#%%\n",
    "M = 3\n",
    "dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "v0 = dicts['v'][..., 0]\n",
    "v1 = dicts['v'][..., 1]\n",
    "from skimage.transform import resize\n",
    "v0 = torch.tensor(resize(v0, (100, 100), preserve_range=True))\n",
    "v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "plt.imshow(v0.abs())\n",
    "plt.colorbar()\n",
    "v1 = torch.tensor(resize(v1, (100, 100), preserve_range=True))\n",
    "v1 = awgn(v1, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "\n",
    "snr=0; n_data=int(1.1e4)\n",
    "delta = v0.mean()*10**(-snr/10)\n",
    "angs = (torch.rand(n_data,1)*20 +10)/180*np.pi  # signal aoa [10, 30]\n",
    "h = (1j*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "angs_n1 = (torch.rand(n_data,1)*20 -70)/180*np.pi  # noise aoa [-70, -50]\n",
    "hs_n1 = (1j*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "angs_n2 = (torch.rand(n_data,1)*20 +120)/180*np.pi  # noise aoa [120, 140]\n",
    "hs_n2 = (1j*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "signal = (h[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:]).reshape(n_data, M, 100, 100)\n",
    "# n1 = (hs_n1[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:]).reshape(n_data, M, 100, 100)\n",
    "n1 = hs_n1[...,None] @ torch.randn(1, torch.tensor(v0.shape).prod(), dtype=torch.cfloat)*delta**0.5 \n",
    "n2 = hs_n2[...,None] @ torch.randn(1, torch.tensor(v0.shape).prod(), dtype=torch.cfloat)*delta**0.5\n",
    "mix =  signal + n1.reshape(n_data, M, 100, 100) + n2.reshape(n_data, M, 100, 100)   \n",
    "mix_all, sig_all = mix.permute(0,2,3,1), signal.permute(0,2,3,1)\n",
    "\n",
    "# torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "plt.figure()\n",
    "plt.imshow(mix_all[0,:,:,0].abs())\n",
    "plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(10000, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "#%% load data and model\n",
    "class DOA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rx_net = nn.Sequential(\n",
    "            nn.Linear(18,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Linear(128,128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.Linear(128,128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,18),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(18,18)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        rx_inv = self.rx_net(x)\n",
    "\n",
    "        return rx_inv \n",
    "model = DOA().cuda()\n",
    "\n",
    "#%%\n",
    "h_all, M = h, sig_all.shape[-1]\n",
    "n_tr = int(1e4)\n",
    "const_range = torch.arange(M).to(torch.cfloat)[None,:].cuda()\n",
    "data = Data.TensorDataset(mix_all[:n_tr], sig_all[:n_tr], h_all[:n_tr])\n",
    "val0 = mix_all[n_tr:n_tr+200]\n",
    "rx_val = (val0[...,None] @ val0[:,:,:,None,:].conj()).mean(dim=(1,2))\n",
    "rx_val_cuda = rx_val.cuda()\n",
    "inp_val = torch.stack((rx_val.real, rx_val.imag), dim=1).reshape(rx_val.shape[0], -1).cuda()\n",
    "tr = Data.DataLoader(data, batch_size=32, drop_last=True, shuffle=True)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= 1e-4,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "Is = torch.stack([torch.eye(3,3)]*32, dim=0).to(torch.cfloat).cuda()\n",
    "Is2 = torch.stack([torch.eye(3,3)]*200, dim=0).to(torch.cfloat).cuda()\n",
    "\n",
    "loss_all, loss_val_all = [], []\n",
    "for epoch in range(2001):\n",
    "    for i, (mix, sig, _) in enumerate(tr):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \"prepare for input to the NN\"\n",
    "        mix = mix.cuda()\n",
    "        sig = sig.cuda()\n",
    "        Rx = mix[...,None] @ mix[:,:,:,None,:].conj()\n",
    "        rx = Rx.mean(dim=(1,2))\n",
    "        inp = torch.stack((rx.real, rx.imag), dim=1).reshape(mix.shape[0], -1)\n",
    "        rx_raw = model(inp)\n",
    "        rx_reshape = rx_raw.reshape(mix.shape[0],M,M,2)\n",
    "        rx_inv_hat = rx_reshape[...,0] + 1j*rx_reshape[...,1]\n",
    "\n",
    "        \"Calc gradient and update\"\n",
    "        loss = ((Is-rx_inv_hat@rx).abs()**2).mean()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        if i % 30 == 0:\n",
    "            loss_all.append(loss.detach().cpu().item())\n",
    "        \n",
    "    if epoch % 20 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_all, '-x')\n",
    "        plt.title(f'epoch {epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        if epoch >50:\n",
    "            plt.figure()\n",
    "            plt.plot(loss_all[-100:], '-rx')\n",
    "            plt.title(f'epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_100_at_epoch{epoch}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rx_raw = model(inp_val)\n",
    "            rx_reshape = rx_raw.reshape(inp_val.shape[0],M,M,2)\n",
    "            rx_inv_hat = rx_reshape[...,0] + 1j*rx_reshape[...,1]\n",
    "            loss_val = ((Is2-rx_inv_hat@rx_val_cuda).abs()**2).mean()\n",
    "            loss_val_all.append(loss_val.cpu().item())\n",
    "            print('epoch', epoch, rx_inv_hat[:3]@rx_val_cuda[:3])\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all, '-x')\n",
    "            plt.title(f'val loss at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_validation_loss')\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all[-50:], '-rx')\n",
    "            plt.title(f'last 50 val loss epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_50val_at_epoch{epoch}')\n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%v3\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "rid = 'v3'\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "\n",
    "#%%\n",
    "M = 3\n",
    "dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "v0 = dicts['v'][..., 0]\n",
    "v1 = dicts['v'][..., 1]\n",
    "from skimage.transform import resize\n",
    "v0 = torch.tensor(resize(v0, (100, 100), preserve_range=True))\n",
    "v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "plt.imshow(v0.abs())\n",
    "plt.colorbar()\n",
    "v1 = torch.tensor(resize(v1, (100, 100), preserve_range=True))\n",
    "v1 = awgn(v1, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "\n",
    "snr=0; n_data=int(1.1e4)\n",
    "delta = v0.mean()*10**(-snr/10)\n",
    "angs = (torch.rand(n_data,1)*20 +10)/180*np.pi  # signal aoa [10, 30]\n",
    "h = (1j*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "angs_n1 = (torch.rand(n_data,1)*20 -70)/180*np.pi  # noise aoa [-70, -50]\n",
    "hs_n1 = (1j*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "angs_n2 = (torch.rand(n_data,1)*20 +120)/180*np.pi  # noise aoa [120, 140]\n",
    "hs_n2 = (1j*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "signal = (h[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:]).reshape(n_data, M, 100, 100)\n",
    "# n1 = (hs_n1[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:]).reshape(n_data, M, 100, 100)\n",
    "n1 = hs_n1[...,None] @ torch.randn(1, torch.tensor(v0.shape).prod(), dtype=torch.cfloat)*delta**0.5 \n",
    "n2 = hs_n2[...,None] @ torch.randn(1, torch.tensor(v0.shape).prod(), dtype=torch.cfloat)*delta**0.5\n",
    "mix =  signal + n1.reshape(n_data, M, 100, 100) + n2.reshape(n_data, M, 100, 100)   \n",
    "mix_all, sig_all = mix.permute(0,2,3,1), signal.permute(0,2,3,1)\n",
    "\n",
    "# torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "plt.figure()\n",
    "plt.imshow(mix_all[0,:,:,0].abs())\n",
    "plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(10000, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "#%% load data and model\n",
    "class DOA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rx_net = nn.Sequential(\n",
    "            nn.Linear(18,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,18),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(18,18)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        rx_inv = self.rx_net(x)\n",
    "\n",
    "        return rx_inv \n",
    "model = DOA().cuda()\n",
    "\n",
    "#%%\n",
    "h_all, M = h, sig_all.shape[-1]\n",
    "n_tr = int(1e4)\n",
    "const_range = torch.arange(M).to(torch.cfloat)[None,:].cuda()\n",
    "data = Data.TensorDataset(mix_all[:n_tr], sig_all[:n_tr], h_all[:n_tr])\n",
    "val0 = mix_all[n_tr:n_tr+200]\n",
    "rx_val = (val0[...,None] @ val0[:,:,:,None,:].conj()).mean(dim=(1,2))\n",
    "rx_val_cuda = rx_val.cuda()\n",
    "inp_val = torch.stack((rx_val.real, rx_val.imag), dim=1).reshape(rx_val.shape[0], -1).cuda()\n",
    "tr = Data.DataLoader(data, batch_size=32, drop_last=True, shuffle=True)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= 1e-4,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "Is = torch.stack([torch.eye(3,3)]*32, dim=0).to(torch.cfloat).cuda()\n",
    "Is2 = torch.stack([torch.eye(3,3)]*200, dim=0).to(torch.cfloat).cuda()\n",
    "\n",
    "loss_all, loss_val_all = [], []\n",
    "for epoch in range(2001):\n",
    "    for i, (mix, sig, _) in enumerate(tr):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \"prepare for input to the NN\"\n",
    "        mix = mix.cuda()\n",
    "        sig = sig.cuda()\n",
    "        Rx = mix[...,None] @ mix[:,:,:,None,:].conj()\n",
    "        rx = Rx.mean(dim=(1,2))\n",
    "        inp = torch.stack((rx.real, rx.imag), dim=1).reshape(mix.shape[0], -1)\n",
    "        rx_raw = model(inp)\n",
    "        rx_reshape = rx_raw.reshape(mix.shape[0],M,M,2)\n",
    "        rx_inv_hat = rx_reshape[...,0] + 1j*rx_reshape[...,1]\n",
    "\n",
    "        \"Calc gradient and update\"\n",
    "        loss = ((Is-rx_inv_hat@rx).abs()**2).mean()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        if i % 30 == 0:\n",
    "            loss_all.append(loss.detach().cpu().item())\n",
    "        \n",
    "    if epoch % 20 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_all, '-x')\n",
    "        plt.title(f'epoch {epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        if epoch >50:\n",
    "            plt.figure()\n",
    "            plt.plot(loss_all[-100:], '-rx')\n",
    "            plt.title(f'epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_100_at_epoch{epoch}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rx_raw = model(inp_val)\n",
    "            rx_reshape = rx_raw.reshape(inp_val.shape[0],M,M,2)\n",
    "            rx_inv_hat = rx_reshape[...,0] + 1j*rx_reshape[...,1]\n",
    "            loss_val = ((Is2-rx_inv_hat@rx_val_cuda).abs()**2).mean()\n",
    "            loss_val_all.append(loss_val.cpu().item())\n",
    "            print('epoch', epoch, rx_inv_hat[:3]@rx_val_cuda[:3])\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all, '-x')\n",
    "            plt.title(f'val loss at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_validation_loss')\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all[-50:], '-rx')\n",
    "            plt.title(f'last 50 val loss epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_50val_at_epoch{epoch}')\n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%v4 \n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "rid = 'v4'\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "\n",
    "#%%\n",
    "M = 3\n",
    "dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "v0 = dicts['v'][..., 0]\n",
    "v1 = dicts['v'][..., 1]\n",
    "from skimage.transform import resize\n",
    "v0 = torch.tensor(resize(v0, (100, 100), preserve_range=True))\n",
    "v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "plt.imshow(v0.abs())\n",
    "plt.colorbar()\n",
    "v1 = torch.tensor(resize(v1, (100, 100), preserve_range=True))\n",
    "v1 = awgn(v1, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "\n",
    "snr=0; n_data=int(1.1e4)\n",
    "delta = v0.mean()*10**(-snr/10)\n",
    "angs = (torch.rand(n_data,1)*20 +10)/180*np.pi  # signal aoa [10, 30]\n",
    "h = (1j*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "angs_n1 = (torch.rand(n_data,1)*20 -70)/180*np.pi  # noise aoa [-70, -50]\n",
    "hs_n1 = (1j*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "angs_n2 = (torch.rand(n_data,1)*20 +120)/180*np.pi  # noise aoa [120, 140]\n",
    "hs_n2 = (1j*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "signal = (h[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:]).reshape(n_data, M, 100, 100)\n",
    "# n1 = (hs_n1[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:]).reshape(n_data, M, 100, 100)\n",
    "n1 = hs_n1[...,None] @ torch.randn(1, torch.tensor(v0.shape).prod(), dtype=torch.cfloat)*delta**0.5 \n",
    "n2 = hs_n2[...,None] @ torch.randn(1, torch.tensor(v0.shape).prod(), dtype=torch.cfloat)*delta**0.5\n",
    "mix =  signal + n1.reshape(n_data, M, 100, 100) + n2.reshape(n_data, M, 100, 100)   \n",
    "mix_all, sig_all = mix.permute(0,2,3,1), signal.permute(0,2,3,1)\n",
    "\n",
    "# torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "plt.figure()\n",
    "plt.imshow(mix_all[0,:,:,0].abs())\n",
    "plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(10000, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "#%% load data and model\n",
    "class DOA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rx_net = nn.Sequential(\n",
    "            nn.Linear(18,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Linear(128,128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.Linear(128,128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.Linear(128,128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,18),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(18,18)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        rx_inv = self.rx_net(x)\n",
    "\n",
    "        return rx_inv \n",
    "model = DOA().cuda()\n",
    "\n",
    "#%%\n",
    "h_all, M = h, sig_all.shape[-1]\n",
    "n_tr = int(1e4)\n",
    "const_range = torch.arange(M).to(torch.cfloat)[None,:].cuda()\n",
    "data = Data.TensorDataset(mix_all[:n_tr], sig_all[:n_tr], h_all[:n_tr])\n",
    "val0 = mix_all[n_tr:n_tr+200]\n",
    "rx_val = (val0[...,None] @ val0[:,:,:,None,:].conj()).mean(dim=(1,2))\n",
    "rx_val_cuda = rx_val.cuda()\n",
    "inp_val = torch.stack((rx_val.real, rx_val.imag), dim=1).reshape(rx_val.shape[0], -1).cuda()\n",
    "tr = Data.DataLoader(data, batch_size=32, drop_last=True, shuffle=True)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= 1e-4,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "Is = torch.stack([torch.eye(3,3)]*32, dim=0).to(torch.cfloat).cuda()\n",
    "Is2 = torch.stack([torch.eye(3,3)]*200, dim=0).to(torch.cfloat).cuda()\n",
    "\n",
    "loss_all, loss_val_all = [], []\n",
    "for epoch in range(2001):\n",
    "    for i, (mix, sig, _) in enumerate(tr):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \"prepare for input to the NN\"\n",
    "        mix = mix.cuda()\n",
    "        sig = sig.cuda()\n",
    "        Rx = mix[...,None] @ mix[:,:,:,None,:].conj()\n",
    "        rx = Rx.mean(dim=(1,2))\n",
    "        inp = torch.stack((rx.real, rx.imag), dim=1).reshape(mix.shape[0], -1)\n",
    "        rx_raw = model(inp)\n",
    "        rx_reshape = rx_raw.reshape(mix.shape[0],M,M,2)\n",
    "        rx_inv_hat = rx_reshape[...,0] + 1j*rx_reshape[...,1]\n",
    "        rx_inv_hat = (rx_inv_hat + rx_inv_hat.conj().permute(0,2,1))/2\n",
    "\n",
    "        \"Calc gradient and update\"\n",
    "        loss = ((Is-rx_inv_hat@rx).abs()**2).mean()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        if i % 30 == 0:\n",
    "            loss_all.append(loss.detach().cpu().item())\n",
    "        \n",
    "    if epoch % 20 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_all, '-x')\n",
    "        plt.title(f'epoch {epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        if epoch >50:\n",
    "            plt.figure()\n",
    "            plt.plot(loss_all[-100:], '-rx')\n",
    "            plt.title(f'epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_100_at_epoch{epoch}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rx_raw = model(inp_val)\n",
    "            rx_reshape = rx_raw.reshape(inp_val.shape[0],M,M,2)\n",
    "            rx_inv_hat = rx_reshape[...,0] + 1j*rx_reshape[...,1]\n",
    "            rx_inv_hat = (rx_inv_hat + rx_inv_hat.conj().permute(0,2,1))/2\n",
    "            loss_val = ((Is2-rx_inv_hat@rx_val_cuda).abs()**2).mean()\n",
    "            loss_val_all.append(loss_val.cpu().item())\n",
    "            print('epoch', epoch, rx_inv_hat[:3]@rx_val_cuda[:3])\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all, '-x')\n",
    "            plt.title(f'val loss at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_validation_loss')\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all[-50:], '-rx')\n",
    "            plt.title(f'last 50 val loss epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_50val_at_epoch{epoch}')\n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v5\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "rid = 'v5'\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "\n",
    "#%%\n",
    "M = 3\n",
    "dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "v0 = dicts['v'][..., 0]\n",
    "v1 = dicts['v'][..., 1]\n",
    "from skimage.transform import resize\n",
    "v0 = torch.tensor(resize(v0, (100, 100), preserve_range=True))\n",
    "v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "plt.imshow(v0.abs())\n",
    "plt.colorbar()\n",
    "v1 = torch.tensor(resize(v1, (100, 100), preserve_range=True))\n",
    "v1 = awgn(v1, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "\n",
    "snr=0; n_data=int(1.1e4)\n",
    "delta = v0.mean()*10**(-snr/10)\n",
    "angs = (torch.rand(n_data,1)*20 +10)/180*np.pi  # signal aoa [10, 30]\n",
    "h = (1j*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "angs_n1 = (torch.rand(n_data,1)*20 -70)/180*np.pi  # noise aoa [-70, -50]\n",
    "hs_n1 = (1j*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "angs_n2 = (torch.rand(n_data,1)*20 +120)/180*np.pi  # noise aoa [120, 140]\n",
    "hs_n2 = (1j*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "signal = (h[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:]).reshape(n_data, M, 100, 100)\n",
    "# n1 = (hs_n1[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:]).reshape(n_data, M, 100, 100)\n",
    "n1 = hs_n1[...,None] @ torch.randn(1, torch.tensor(v0.shape).prod(), dtype=torch.cfloat)*delta**0.5 \n",
    "n2 = hs_n2[...,None] @ torch.randn(1, torch.tensor(v0.shape).prod(), dtype=torch.cfloat)*delta**0.5\n",
    "mix =  signal + n1.reshape(n_data, M, 100, 100) + n2.reshape(n_data, M, 100, 100)   \n",
    "mix_all, sig_all = mix.permute(0,2,3,1), signal.permute(0,2,3,1)\n",
    "\n",
    "# torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "plt.figure()\n",
    "plt.imshow(mix_all[0,:,:,0].abs())\n",
    "plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(10000, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "#%% load data and model\n",
    "class DOA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rx_net = nn.Sequential(\n",
    "            nn.Linear(18,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Linear(128,128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.Linear(128,128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,18),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(18,18)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        rx_inv = self.rx_net(x)\n",
    "\n",
    "        return rx_inv \n",
    "model = DOA().cuda()\n",
    "\n",
    "#%%\n",
    "h_all, M = h, sig_all.shape[-1]\n",
    "n_tr = int(1e4)\n",
    "const_range = torch.arange(M).to(torch.cfloat)[None,:].cuda()\n",
    "data = Data.TensorDataset(mix_all[:n_tr], sig_all[:n_tr], h_all[:n_tr])\n",
    "val0 = mix_all[n_tr:n_tr+200]\n",
    "rx_val = (val0[...,None] @ val0[:,:,:,None,:].conj()).mean(dim=(1,2))\n",
    "rx_val_cuda = rx_val.cuda()\n",
    "inp_val = torch.stack((rx_val.real, rx_val.imag), dim=1).reshape(rx_val.shape[0], -1).cuda()\n",
    "tr = Data.DataLoader(data, batch_size=32, drop_last=True, shuffle=True)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= 1e-4,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "Is = torch.stack([torch.eye(3,3)]*32, dim=0).to(torch.cfloat).cuda()\n",
    "Is2 = torch.stack([torch.eye(3,3)]*200, dim=0).to(torch.cfloat).cuda()\n",
    "\n",
    "loss_all, loss_val_all = [], []\n",
    "for epoch in range(2001):\n",
    "    for i, (mix, sig, _) in enumerate(tr):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \"prepare for input to the NN\"\n",
    "        mix = mix.cuda()\n",
    "        sig = sig.cuda()\n",
    "        Rx = mix[...,None] @ mix[:,:,:,None,:].conj()\n",
    "        rx = Rx.mean(dim=(1,2))\n",
    "        inp = torch.stack((rx.real, rx.imag), dim=1).reshape(mix.shape[0], -1)\n",
    "        rx_raw = model(inp)\n",
    "        rx_reshape = rx_raw.reshape(mix.shape[0],M,M,2)\n",
    "        rx_inv_hat = rx_reshape[...,0] + 1j*rx_reshape[...,1]\n",
    "        rx_inv_hat = (rx_inv_hat + rx_inv_hat.conj().permute(0,2,1))/2\n",
    "\n",
    "        \"Calc gradient and update\"\n",
    "        loss = ((Is-rx_inv_hat@rx).abs()**2).mean()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        if i % 30 == 0:\n",
    "            loss_all.append(loss.detach().cpu().item())\n",
    "        \n",
    "    if epoch % 20 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_all, '-x')\n",
    "        plt.title(f'epoch {epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        if epoch >50:\n",
    "            plt.figure()\n",
    "            plt.plot(loss_all[-100:], '-rx')\n",
    "            plt.title(f'epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_100_at_epoch{epoch}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rx_raw = model(inp_val)\n",
    "            rx_reshape = rx_raw.reshape(inp_val.shape[0],M,M,2)\n",
    "            rx_inv_hat = rx_reshape[...,0] + 1j*rx_reshape[...,1]\n",
    "            rx_inv_hat = (rx_inv_hat + rx_inv_hat.conj().permute(0,2,1))/2\n",
    "            loss_val = ((Is2-rx_inv_hat@rx_val_cuda).abs()**2).mean()\n",
    "            loss_val_all.append(loss_val.cpu().item())\n",
    "            print('epoch', epoch, rx_inv_hat[:3]@rx_val_cuda[:3])\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all, '-x')\n",
    "            plt.title(f'val loss at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_validation_loss')\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all[-50:], '-rx')\n",
    "            plt.title(f'last 50 val loss epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_50val_at_epoch{epoch}')\n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v6\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "rid = 'v6'\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "\n",
    "#%%\n",
    "M = 3\n",
    "dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "v0 = dicts['v'][..., 0]\n",
    "v1 = dicts['v'][..., 1]\n",
    "from skimage.transform import resize\n",
    "v0 = torch.tensor(resize(v0, (100, 100), preserve_range=True))\n",
    "v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "plt.imshow(v0.abs())\n",
    "plt.colorbar()\n",
    "v1 = torch.tensor(resize(v1, (100, 100), preserve_range=True))\n",
    "v1 = awgn(v1, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "\n",
    "snr=0; n_data=int(1.1e4)\n",
    "delta = v0.mean()*10**(-snr/10)\n",
    "angs = (torch.rand(n_data,1)*20 +10)/180*np.pi  # signal aoa [10, 30]\n",
    "h = (1j*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "angs_n1 = (torch.rand(n_data,1)*20 -70)/180*np.pi  # noise aoa [-70, -50]\n",
    "hs_n1 = (1j*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "angs_n2 = (torch.rand(n_data,1)*20 +120)/180*np.pi  # noise aoa [120, 140]\n",
    "hs_n2 = (1j*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "signal = (h[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:]).reshape(n_data, M, 100, 100)\n",
    "# n1 = (hs_n1[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:]).reshape(n_data, M, 100, 100)\n",
    "n1 = hs_n1[...,None] @ torch.randn(1, torch.tensor(v0.shape).prod(), dtype=torch.cfloat)*delta**0.5 \n",
    "n2 = hs_n2[...,None] @ torch.randn(1, torch.tensor(v0.shape).prod(), dtype=torch.cfloat)*delta**0.5\n",
    "mix =  signal + n1.reshape(n_data, M, 100, 100) + n2.reshape(n_data, M, 100, 100)   \n",
    "mix_all, sig_all = mix.permute(0,2,3,1), signal.permute(0,2,3,1)\n",
    "\n",
    "# torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "plt.figure()\n",
    "plt.imshow(mix_all[0,:,:,0].abs())\n",
    "plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(10000, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "#%% load data and model\n",
    "class DOA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rx_net = nn.Sequential(\n",
    "            nn.Linear(18,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,18),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(18,18)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        rx_inv = self.rx_net(x)\n",
    "\n",
    "        return rx_inv \n",
    "model = DOA().cuda()\n",
    "\n",
    "#%%\n",
    "h_all, M = h, sig_all.shape[-1]\n",
    "n_tr = int(1e4)\n",
    "const_range = torch.arange(M).to(torch.cfloat)[None,:].cuda()\n",
    "data = Data.TensorDataset(mix_all[:n_tr], sig_all[:n_tr], h_all[:n_tr])\n",
    "val0 = mix_all[n_tr:n_tr+200]\n",
    "rx_val = (val0[...,None] @ val0[:,:,:,None,:].conj()).mean(dim=(1,2))\n",
    "rx_val_cuda = rx_val.cuda()\n",
    "inp_val = torch.stack((rx_val.real, rx_val.imag), dim=1).reshape(rx_val.shape[0], -1).cuda()\n",
    "tr = Data.DataLoader(data, batch_size=32, drop_last=True, shuffle=True)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= 1e-4,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "Is = torch.stack([torch.eye(3,3)]*32, dim=0).to(torch.cfloat).cuda()\n",
    "Is2 = torch.stack([torch.eye(3,3)]*200, dim=0).to(torch.cfloat).cuda()\n",
    "\n",
    "loss_all, loss_val_all = [], []\n",
    "for epoch in range(2001):\n",
    "    for i, (mix, sig, _) in enumerate(tr):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \"prepare for input to the NN\"\n",
    "        mix = mix.cuda()\n",
    "        sig = sig.cuda()\n",
    "        Rx = mix[...,None] @ mix[:,:,:,None,:].conj()\n",
    "        rx = Rx.mean(dim=(1,2))\n",
    "        inp = torch.stack((rx.real, rx.imag), dim=1).reshape(mix.shape[0], -1)\n",
    "        rx_raw = model(inp)\n",
    "        rx_reshape = rx_raw.reshape(mix.shape[0],M,M,2)\n",
    "        rx_inv_hat = rx_reshape[...,0] + 1j*rx_reshape[...,1]\n",
    "        rx_inv_hat = (rx_inv_hat + rx_inv_hat.conj().permute(0,2,1))/2\n",
    "\n",
    "        \"Calc gradient and update\"\n",
    "        loss = ((Is-rx_inv_hat@rx).abs()**2).mean()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        if i % 30 == 0:\n",
    "            loss_all.append(loss.detach().cpu().item())\n",
    "        \n",
    "    if epoch % 20 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_all, '-x')\n",
    "        plt.title(f'epoch {epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        if epoch >50:\n",
    "            plt.figure()\n",
    "            plt.plot(loss_all[-100:], '-rx')\n",
    "            plt.title(f'epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_100_at_epoch{epoch}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rx_raw = model(inp_val)\n",
    "            rx_reshape = rx_raw.reshape(inp_val.shape[0],M,M,2)\n",
    "            rx_inv_hat = rx_reshape[...,0] + 1j*rx_reshape[...,1]\n",
    "            rx_inv_hat = (rx_inv_hat + rx_inv_hat.conj().permute(0,2,1))/2\n",
    "            loss_val = ((Is2-rx_inv_hat@rx_val_cuda).abs()**2).mean()\n",
    "            loss_val_all.append(loss_val.cpu().item())\n",
    "            print('epoch', epoch, rx_inv_hat[:3]@rx_val_cuda[:3])\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all, '-x')\n",
    "            plt.title(f'val loss at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_validation_loss')\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all[-50:], '-rx')\n",
    "            plt.title(f'last 50 val loss epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_50val_at_epoch{epoch}')\n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%v7\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "rid = 'v7'\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "\n",
    "#%%\n",
    "M = 3\n",
    "dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "v0 = dicts['v'][..., 0]\n",
    "v1 = dicts['v'][..., 1]\n",
    "from skimage.transform import resize\n",
    "v0 = torch.tensor(resize(v0, (100, 100), preserve_range=True))\n",
    "v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "plt.imshow(v0.abs())\n",
    "plt.colorbar()\n",
    "v1 = torch.tensor(resize(v1, (100, 100), preserve_range=True))\n",
    "v1 = awgn(v1, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "\n",
    "snr=0; n_data=int(1.1e4)\n",
    "delta = v0.mean()*10**(-snr/10)\n",
    "angs = (torch.rand(n_data,1)*20 +10)/180*np.pi  # signal aoa [10, 30]\n",
    "h = (1j*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "angs_n1 = (torch.rand(n_data,1)*20 -70)/180*np.pi  # noise aoa [-70, -50]\n",
    "hs_n1 = (1j*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "angs_n2 = (torch.rand(n_data,1)*20 +120)/180*np.pi  # noise aoa [120, 140]\n",
    "hs_n2 = (1j*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "signal = (h[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:]).reshape(n_data, M, 100, 100)\n",
    "# n1 = (hs_n1[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:]).reshape(n_data, M, 100, 100)\n",
    "n1 = hs_n1[...,None] @ torch.randn(1, torch.tensor(v0.shape).prod(), dtype=torch.cfloat)*delta**0.5 \n",
    "n2 = hs_n2[...,None] @ torch.randn(1, torch.tensor(v0.shape).prod(), dtype=torch.cfloat)*delta**0.5\n",
    "mix =  signal + n1.reshape(n_data, M, 100, 100) + n2.reshape(n_data, M, 100, 100)   \n",
    "mix_all, sig_all = mix.permute(0,2,3,1), signal.permute(0,2,3,1)\n",
    "\n",
    "# torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "plt.figure()\n",
    "plt.imshow(mix_all[0,:,:,0].abs())\n",
    "plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(10000, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "#%% load data and model\n",
    "class DOA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rx_net = nn.Sequential(\n",
    "            nn.Linear(18,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Linear(128,128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.Linear(128,128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.Linear(128,128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,18),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(18,18)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        rx_inv = self.rx_net(x)\n",
    "\n",
    "        return rx_inv \n",
    "model = DOA().cuda()\n",
    "\n",
    "#%%\n",
    "h_all, M = h, sig_all.shape[-1]\n",
    "n_tr = int(1e4)\n",
    "const_range = torch.arange(M).to(torch.cfloat)[None,:].cuda()\n",
    "data = Data.TensorDataset(mix_all[:n_tr], sig_all[:n_tr], h_all[:n_tr])\n",
    "val0 = mix_all[n_tr:n_tr+200]\n",
    "rx_val = (val0[...,None] @ val0[:,:,:,None,:].conj()).mean(dim=(1,2))\n",
    "rx_inv_cuda = rx_val.inverse().cuda()\n",
    "\n",
    "#%%\n",
    "inp_val = torch.stack((rx_val.real, rx_val.imag), dim=1).reshape(rx_val.shape[0], -1).cuda()\n",
    "tr = Data.DataLoader(data, batch_size=32, drop_last=True, shuffle=True)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= 1e-4,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "Is = torch.stack([torch.eye(3,3)]*32, dim=0).to(torch.cfloat).cuda()\n",
    "Is2 = torch.stack([torch.eye(3,3)]*200, dim=0).to(torch.cfloat).cuda()\n",
    "\n",
    "loss_all, loss_val_all = [], []\n",
    "for epoch in range(2001):\n",
    "    for i, (mix, sig, _) in enumerate(tr):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \"prepare for input to the NN\"\n",
    "        mix = mix.cuda()\n",
    "        sig = sig.cuda()\n",
    "        Rx = mix[...,None] @ mix[:,:,:,None,:].conj()\n",
    "        rx = Rx.mean(dim=(1,2))\n",
    "        inp = torch.stack((rx.real, rx.imag), dim=1).reshape(mix.shape[0], -1)\n",
    "        rx_raw = model(inp)\n",
    "        rx_reshape = rx_raw.reshape(mix.shape[0],M,M,2)\n",
    "        rx_inv_hat = rx_reshape[...,0] + 1j*rx_reshape[...,1]\n",
    "        rx_inv_hat = (rx_inv_hat + rx_inv_hat.conj().permute(0,2,1))/2\n",
    "\n",
    "        \"Calc gradient and update\"\n",
    "        loss = ((rx_inv_hat-rx.inverse()).abs()**2).mean()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        if i % 30 == 0:\n",
    "            loss_all.append(loss.detach().cpu().item())\n",
    "        \n",
    "    if epoch % 20 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_all, '-x')\n",
    "        plt.title(f'epoch {epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        if epoch >50:\n",
    "            plt.figure()\n",
    "            plt.plot(loss_all[-100:], '-rx')\n",
    "            plt.title(f'epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_100_at_epoch{epoch}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rx_raw = model(inp_val)\n",
    "            rx_reshape = rx_raw.reshape(inp_val.shape[0],M,M,2)\n",
    "            rx_inv_hat = rx_reshape[...,0] + 1j*rx_reshape[...,1]\n",
    "            rx_inv_hat = (rx_inv_hat + rx_inv_hat.conj().permute(0,2,1))/2\n",
    "            loss_val = ((rx_inv_hat - rx_inv_cuda).abs()**2).mean()\n",
    "            loss_val_all.append(loss_val.cpu().item())\n",
    "            print('epoch', epoch, rx_inv_hat[:3], '\\n',rx_inv_cuda[:3])\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all, '-x')\n",
    "            plt.title(f'val loss at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_validation_loss')\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all[-50:], '-rx')\n",
    "            plt.title(f'last 50 val loss epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_50val_at_epoch{epoch}')\n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%v8\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "rid = 'v8'\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "\n",
    "#%%\n",
    "M = 3\n",
    "dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "v0 = dicts['v'][..., 0]\n",
    "v1 = dicts['v'][..., 1]\n",
    "from skimage.transform import resize\n",
    "v0 = torch.tensor(resize(v0, (100, 100), preserve_range=True))\n",
    "v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "plt.imshow(v0.abs())\n",
    "plt.colorbar()\n",
    "v1 = torch.tensor(resize(v1, (100, 100), preserve_range=True))\n",
    "v1 = awgn(v1, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "\n",
    "snr=0; n_data=int(1.1e4)\n",
    "delta = v0.mean()*10**(-snr/10)\n",
    "angs = (torch.rand(n_data,1)*20 +10)/180*np.pi  # signal aoa [10, 30]\n",
    "h = (1j*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "angs_n1 = (torch.rand(n_data,1)*20 -70)/180*np.pi  # noise aoa [-70, -50]\n",
    "hs_n1 = (1j*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "angs_n2 = (torch.rand(n_data,1)*20 +120)/180*np.pi  # noise aoa [120, 140]\n",
    "hs_n2 = (1j*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "signal = (h[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:]).reshape(n_data, M, 100, 100)\n",
    "# n1 = (hs_n1[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:]).reshape(n_data, M, 100, 100)\n",
    "n1 = hs_n1[...,None] @ torch.randn(1, torch.tensor(v0.shape).prod(), dtype=torch.cfloat)*delta**0.5 \n",
    "n2 = hs_n2[...,None] @ torch.randn(1, torch.tensor(v0.shape).prod(), dtype=torch.cfloat)*delta**0.5\n",
    "mix =  signal + n1.reshape(n_data, M, 100, 100) + n2.reshape(n_data, M, 100, 100)   \n",
    "mix_all, sig_all = mix.permute(0,2,3,1), signal.permute(0,2,3,1)\n",
    "\n",
    "# torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "plt.figure()\n",
    "plt.imshow(mix_all[0,:,:,0].abs())\n",
    "plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(10000, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "#%% load data and model\n",
    "class DOA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rx_net = nn.Sequential(\n",
    "            nn.Linear(18,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Linear(128,128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.Linear(128,128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,18),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(18,18)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        rx_inv = self.rx_net(x)\n",
    "\n",
    "        return rx_inv \n",
    "model = DOA().cuda()\n",
    "\n",
    "#%%\n",
    "h_all, M = h, sig_all.shape[-1]\n",
    "n_tr = int(1e4)\n",
    "const_range = torch.arange(M).to(torch.cfloat)[None,:].cuda()\n",
    "data = Data.TensorDataset(mix_all[:n_tr], sig_all[:n_tr], h_all[:n_tr])\n",
    "val0 = mix_all[n_tr:n_tr+200]\n",
    "rx_val = (val0[...,None] @ val0[:,:,:,None,:].conj()).mean(dim=(1,2))\n",
    "rx_inv_cuda = rx_val.inverse().cuda()\n",
    "\n",
    "#%%\n",
    "inp_val = torch.stack((rx_val.real, rx_val.imag), dim=1).reshape(rx_val.shape[0], -1).cuda()\n",
    "tr = Data.DataLoader(data, batch_size=32, drop_last=True, shuffle=True)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= 1e-4,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "Is = torch.stack([torch.eye(3,3)]*32, dim=0).to(torch.cfloat).cuda()\n",
    "Is2 = torch.stack([torch.eye(3,3)]*200, dim=0).to(torch.cfloat).cuda()\n",
    "\n",
    "loss_all, loss_val_all = [], []\n",
    "for epoch in range(2001):\n",
    "    for i, (mix, sig, _) in enumerate(tr):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \"prepare for input to the NN\"\n",
    "        mix = mix.cuda()\n",
    "        sig = sig.cuda()\n",
    "        Rx = mix[...,None] @ mix[:,:,:,None,:].conj()\n",
    "        rx = Rx.mean(dim=(1,2))\n",
    "        inp = torch.stack((rx.real, rx.imag), dim=1).reshape(mix.shape[0], -1)\n",
    "        rx_raw = model(inp)\n",
    "        rx_reshape = rx_raw.reshape(mix.shape[0],M,M,2)\n",
    "        rx_inv_hat = rx_reshape[...,0] + 1j*rx_reshape[...,1]\n",
    "        rx_inv_hat = (rx_inv_hat + rx_inv_hat.conj().permute(0,2,1))/2\n",
    "\n",
    "        \"Calc gradient and update\"\n",
    "        loss = ((rx_inv_hat-rx.inverse()).abs()**2).mean()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        if i % 30 == 0:\n",
    "            loss_all.append(loss.detach().cpu().item())\n",
    "        \n",
    "    if epoch % 20 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_all, '-x')\n",
    "        plt.title(f'epoch {epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        if epoch >50:\n",
    "            plt.figure()\n",
    "            plt.plot(loss_all[-100:], '-rx')\n",
    "            plt.title(f'epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_100_at_epoch{epoch}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rx_raw = model(inp_val)\n",
    "            rx_reshape = rx_raw.reshape(inp_val.shape[0],M,M,2)\n",
    "            rx_inv_hat = rx_reshape[...,0] + 1j*rx_reshape[...,1]\n",
    "            rx_inv_hat = (rx_inv_hat + rx_inv_hat.conj().permute(0,2,1))/2\n",
    "            loss_val = ((rx_inv_hat - rx_inv_cuda).abs()**2).mean()\n",
    "            loss_val_all.append(loss_val.cpu().item())\n",
    "            print('epoch', epoch, rx_inv_hat[:3], '\\n',rx_inv_cuda[:3])\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all, '-x')\n",
    "            plt.title(f'val loss at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_validation_loss')\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all[-50:], '-rx')\n",
    "            plt.title(f'last 50 val loss epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_50val_at_epoch{epoch}')\n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%v9\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "rid = 'v9'\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "\n",
    "#%%\n",
    "M = 3\n",
    "dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "v0 = dicts['v'][..., 0]\n",
    "v1 = dicts['v'][..., 1]\n",
    "from skimage.transform import resize\n",
    "v0 = torch.tensor(resize(v0, (100, 100), preserve_range=True))\n",
    "v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "plt.imshow(v0.abs())\n",
    "plt.colorbar()\n",
    "v1 = torch.tensor(resize(v1, (100, 100), preserve_range=True))\n",
    "v1 = awgn(v1, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "\n",
    "snr=0; n_data=int(1.1e4)\n",
    "delta = v0.mean()*10**(-snr/10)\n",
    "angs = (torch.rand(n_data,1)*20 +10)/180*np.pi  # signal aoa [10, 30]\n",
    "h = (1j*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "angs_n1 = (torch.rand(n_data,1)*20 -70)/180*np.pi  # noise aoa [-70, -50]\n",
    "hs_n1 = (1j*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "angs_n2 = (torch.rand(n_data,1)*20 +120)/180*np.pi  # noise aoa [120, 140]\n",
    "hs_n2 = (1j*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "signal = (h[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:]).reshape(n_data, M, 100, 100)\n",
    "# n1 = (hs_n1[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:]).reshape(n_data, M, 100, 100)\n",
    "n1 = hs_n1[...,None] @ torch.randn(1, torch.tensor(v0.shape).prod(), dtype=torch.cfloat)*delta**0.5 \n",
    "n2 = hs_n2[...,None] @ torch.randn(1, torch.tensor(v0.shape).prod(), dtype=torch.cfloat)*delta**0.5\n",
    "mix =  signal + n1.reshape(n_data, M, 100, 100) + n2.reshape(n_data, M, 100, 100)   \n",
    "mix_all, sig_all = mix.permute(0,2,3,1), signal.permute(0,2,3,1)\n",
    "\n",
    "# torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "plt.figure()\n",
    "plt.imshow(mix_all[0,:,:,0].abs())\n",
    "plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(10000, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "#%% load data and model\n",
    "class DOA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rx_net = nn.Sequential(\n",
    "            nn.Linear(18,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,18),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(18,18)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        rx_inv = self.rx_net(x)\n",
    "\n",
    "        return rx_inv \n",
    "model = DOA().cuda()\n",
    "\n",
    "#%%\n",
    "h_all, M = h, sig_all.shape[-1]\n",
    "n_tr = int(1e4)\n",
    "const_range = torch.arange(M).to(torch.cfloat)[None,:].cuda()\n",
    "data = Data.TensorDataset(mix_all[:n_tr], sig_all[:n_tr], h_all[:n_tr])\n",
    "val0 = mix_all[n_tr:n_tr+200]\n",
    "rx_val = (val0[...,None] @ val0[:,:,:,None,:].conj()).mean(dim=(1,2))\n",
    "rx_inv_cuda = rx_val.inverse().cuda()\n",
    "\n",
    "#%%\n",
    "inp_val = torch.stack((rx_val.real, rx_val.imag), dim=1).reshape(rx_val.shape[0], -1).cuda()\n",
    "tr = Data.DataLoader(data, batch_size=32, drop_last=True, shuffle=True)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= 1e-4,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "Is = torch.stack([torch.eye(3,3)]*32, dim=0).to(torch.cfloat).cuda()\n",
    "Is2 = torch.stack([torch.eye(3,3)]*200, dim=0).to(torch.cfloat).cuda()\n",
    "\n",
    "loss_all, loss_val_all = [], []\n",
    "for epoch in range(2001):\n",
    "    for i, (mix, sig, _) in enumerate(tr):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \"prepare for input to the NN\"\n",
    "        mix = mix.cuda()\n",
    "        sig = sig.cuda()\n",
    "        Rx = mix[...,None] @ mix[:,:,:,None,:].conj()\n",
    "        rx = Rx.mean(dim=(1,2))\n",
    "        inp = torch.stack((rx.real, rx.imag), dim=1).reshape(mix.shape[0], -1)\n",
    "        rx_raw = model(inp)\n",
    "        rx_reshape = rx_raw.reshape(mix.shape[0],M,M,2)\n",
    "        rx_inv_hat = rx_reshape[...,0] + 1j*rx_reshape[...,1]\n",
    "        rx_inv_hat = (rx_inv_hat + rx_inv_hat.conj().permute(0,2,1))/2\n",
    "\n",
    "        \"Calc gradient and update\"\n",
    "        loss = ((rx_inv_hat-rx.inverse()).abs()**2).mean()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        if i % 30 == 0:\n",
    "            loss_all.append(loss.detach().cpu().item())\n",
    "        \n",
    "    if epoch % 20 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_all, '-x')\n",
    "        plt.title(f'epoch {epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        if epoch >50:\n",
    "            plt.figure()\n",
    "            plt.plot(loss_all[-100:], '-rx')\n",
    "            plt.title(f'epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_100_at_epoch{epoch}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rx_raw = model(inp_val)\n",
    "            rx_reshape = rx_raw.reshape(inp_val.shape[0],M,M,2)\n",
    "            rx_inv_hat = rx_reshape[...,0] + 1j*rx_reshape[...,1]\n",
    "            rx_inv_hat = (rx_inv_hat + rx_inv_hat.conj().permute(0,2,1))/2\n",
    "            loss_val = ((rx_inv_hat - rx_inv_cuda).abs()**2).mean()\n",
    "            loss_val_all.append(loss_val.cpu().item())\n",
    "            print('epoch', epoch, rx_inv_hat[:3], '\\n',rx_inv_cuda[:3])\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all, '-x')\n",
    "            plt.title(f'val loss at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_validation_loss')\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all[-50:], '-rx')\n",
    "            plt.title(f'last 50 val loss epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_50val_at_epoch{epoch}')\n",
    "            plt.close('all')           \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v10\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "rid = 'v10'\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "\n",
    "#%% generate data\n",
    "if True:\n",
    "    M, n_data= 3, int(1.2e4)\n",
    "    dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "    v0 = dicts['v'][..., 0]\n",
    "    v1 = dicts['v'][..., 1]\n",
    "    v2 = dicts['v'][..., 2]\n",
    "    from skimage.transform import resize\n",
    "    v0 = torch.tensor(resize(v0, (100, 100), preserve_range=True))\n",
    "    v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "    plt.imshow(v0.abs())\n",
    "    plt.colorbar()\n",
    "    v1 = torch.tensor(resize(v1, (100, 100), preserve_range=True))\n",
    "    v1 = awgn(v1, snr=30, seed=1).abs().to(torch.cfloat)\n",
    "    v2 = torch.tensor(resize(v2, (100, 100), preserve_range=True))\n",
    "    v2 = awgn(v2, snr=30, seed=2).abs().to(torch.cfloat)\n",
    "\n",
    "    snr=0\n",
    "    delta = v0.mean()*10**(-snr/10)\n",
    "    angs = (torch.rand(n_data,1)*20 +10)/180*np.pi  # signal aoa [10, 30]\n",
    "    H = (1j*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    angs_n1 = (torch.rand(n_data,1)*20 -70)/180*np.pi  # noise aoa [-70, -50]\n",
    "    hs_n1 = (1j*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    angs_n2 = (torch.rand(n_data,1)*20 +120)/180*np.pi  # noise aoa [120, 140]\n",
    "    hs_n2 = (1j*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    signal = (H[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:])\n",
    "    n1 = (hs_n1[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:])\n",
    "    n2 = (hs_n2[..., None]@(torch.randn(v2.shape, dtype=torch.cfloat)*(v2**0.5)).flatten()[None,:])\n",
    "    mix =  (signal + n1 + n2).reshape(n_data, M, 100, 100)   \n",
    "    mix_all, sig_all = mix.permute(0,2,3,1), signal.reshape(n_data, M, 100, 100).permute(0,2,3,1)\n",
    "    mixn = awgn_batch(mix_all)\n",
    "    H_all = torch.stack((H, hs_n1, hs_n2), dim=-1)\n",
    "\n",
    "    # torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "    plt.figure()\n",
    "    plt.imshow(mix_all[0,:,:,0].abs())\n",
    "    plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(10000, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "\n",
    "#%% load data and model\n",
    "class DOA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mainnet = nn.Sequential(\n",
    "            nn.Linear(12,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64,6)\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        m = self.mainnet(x)\n",
    "        h6 = self.hnet(m)\n",
    "        rx12 = self.rxnet(m)\n",
    "\n",
    "        return h6, rx12 \n",
    "\n",
    "def lower2matrix(rx12):\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[0,0], rx_inv_hat[1,1], rx_inv_hat[2,2] = \\\n",
    "        rx_inv_hat[0,0]/2, rx_inv_hat[1,1]/2, rx_inv_hat[2,2]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "model = DOA().cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "#%%\n",
    "M, btsize, n_tr = 3, 64, int(1e4)\n",
    "lamb = 1\n",
    "const_range = torch.arange(M).to(torch.cfloat)[None,:].cuda()\n",
    "data = Data.TensorDataset(mix_all[:n_tr], H_all[:n_tr])\n",
    "tr = Data.DataLoader(data, batch_size=btsize, drop_last=True, shuffle=True)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= 1e-4,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "\"Pre calc constants\"\n",
    "Is = torch.stack([torch.eye(3,3)]*btsize, dim=0).to(torch.cfloat).cuda()\n",
    "Is2 = torch.stack([torch.eye(3,3)]*200, dim=0).to(torch.cfloat).cuda()\n",
    "ind = torch.tril_indices(3,3)\n",
    "indx = ind[0]*3+ind[1]\n",
    "\n",
    "\"validation\"\n",
    "val0, h0 = mix_all[n_tr:n_tr+200], H_all[n_tr:n_tr+200].cuda()\n",
    "rx_val = (val0[...,None] @ val0[:,:,:,None,:].conj()).mean(dim=(1,2))\n",
    "rx_val_cuda = rx_val.cuda()\n",
    "\n",
    "loss_all, loss_val_all = [], []\n",
    "for epoch in range(2001):\n",
    "    for i, (mix, H) in enumerate(tr):\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        mix, H = mix.cuda(), H.cuda()\n",
    "        for j in range(3): # recursive for each source\n",
    "            if j == 0:\n",
    "                Rx = mix[...,None] @ mix[:,:,:,None,:].conj()\n",
    "                rx = Rx.mean(dim=(1,2))\n",
    "            else:\n",
    "                w = rx_inv_hat@hhat[...,None] / \\\n",
    "                        (hhat[:,None,:].conj()@rx_inv_hat@hhat[...,None])\n",
    "                p = Is - hhat[...,None]@w.permute(0,2,1).conj()\n",
    "                rx = p@rx@p.permute(0,2,1).conj()\n",
    "\n",
    "            inp = torch.stack((rx.real, rx.imag), dim=1)[:,:,ind[0], ind[1]].reshape(btsize, -1)\n",
    "            h6, rx12 = model(inp)\n",
    "            hhat = h6[:,:3] + 1j*h6[:,3:]\n",
    "            rx_inv_hat = lower2matrix(rx12)\n",
    "            loss = loss + ((Is-rx_inv_hat@rx).abs()**2).mean() + \\\n",
    "                lamb*((H[:,:,j]-hhat).abs()**2).mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        if i % 30 == 0:\n",
    "            loss_all.append(loss.detach().cpu().item())\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_all, '-x')\n",
    "        plt.title(f'epoch {epoch}')\n",
    "        plt.show()\n",
    "\n",
    "        if epoch >50:\n",
    "            plt.figure()\n",
    "            plt.plot(loss_all[-100:], '-rx')\n",
    "            plt.title(f'epoch {epoch}')\n",
    "            plt.show()\n",
    "\n",
    "        \"Validation\"\n",
    "        with torch.no_grad():\n",
    "            loss_val = 0\n",
    "            for j in range(3): # recursive for each source\n",
    "                if j ==0 :\n",
    "                    rx = rx_val_cuda\n",
    "                else:\n",
    "                    w = rx_inv_hat_val@hhat_val[...,None] / \\\n",
    "                            (hhat_val[:,None,:].conj()@rx_inv_hat_val@hhat_val[...,None])\n",
    "                    p = Is2 - hhat_val[...,None]@w.permute(0,2,1).conj()\n",
    "                    rx = p@rx@p.permute(0,2,1).conj()\n",
    "\n",
    "                rl = rx.reshape(200, -1)[:,indx] # take lower triangle\n",
    "                inp = torch.stack((rl.real, rl.imag), dim=1).reshape(200, -1)\n",
    "                h6, rx12 = model(inp)\n",
    "                hhat_val = h6[:,:3] + 1j*h6[:,3:]\n",
    "                rx_inv_hat_val = lower2matrix(rx12)\n",
    "\n",
    "                loss_val = loss_val + ((Is2-rx_inv_hat_val@rx).abs()**2).mean() + \\\n",
    "                    lamb*((h0[:,:,j]-hhat_val).abs()**2).mean()\n",
    "                    \n",
    "            loss_val_all.append(loss_val.cpu().item())\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all, '-x')\n",
    "            plt.title(f'val loss at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_validation_loss')\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all[-50:], '-rx')\n",
    "            plt.title(f'last 50 val loss epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_50val_at_epoch{epoch}')\n",
    "            plt.close('all')   \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v11\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "rid = 'v11'\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "\n",
    "#%% generate data\n",
    "if True:\n",
    "    M, n_data= 3, int(1.2e4)\n",
    "    dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "    v0 = dicts['v'][..., 0]\n",
    "    v1 = dicts['v'][..., 1]\n",
    "    v2 = dicts['v'][..., 2]\n",
    "    from skimage.transform import resize\n",
    "    v0 = torch.tensor(resize(v0, (100, 100), preserve_range=True))\n",
    "    v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "    plt.imshow(v0.abs())\n",
    "    plt.colorbar()\n",
    "    v1 = torch.tensor(resize(v1, (100, 100), preserve_range=True))\n",
    "    v1 = awgn(v1, snr=30, seed=1).abs().to(torch.cfloat)\n",
    "    v2 = torch.tensor(resize(v2, (100, 100), preserve_range=True))\n",
    "    v2 = awgn(v2, snr=30, seed=2).abs().to(torch.cfloat)\n",
    "\n",
    "    snr=0\n",
    "    delta = v0.mean()*10**(-snr/10)\n",
    "    angs = (torch.rand(n_data,1)*20 +10)/180*np.pi  # signal aoa [10, 30]\n",
    "    H = (1j*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    angs_n1 = (torch.rand(n_data,1)*20 -70)/180*np.pi  # noise aoa [-70, -50]\n",
    "    hs_n1 = (1j*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    angs_n2 = (torch.rand(n_data,1)*20 +120)/180*np.pi  # noise aoa [120, 140]\n",
    "    hs_n2 = (1j*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    signal = (H[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:])\n",
    "    n1 = (hs_n1[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:])\n",
    "    n2 = (hs_n2[..., None]@(torch.randn(v2.shape, dtype=torch.cfloat)*(v2**0.5)).flatten()[None,:])\n",
    "    mix =  (signal + n1 + n2).reshape(n_data, M, 100, 100)   \n",
    "    mix_all, sig_all = mix.permute(0,2,3,1), signal.reshape(n_data, M, 100, 100).permute(0,2,3,1)\n",
    "    mixn = awgn_batch(mix_all)\n",
    "    H_all = torch.stack((H, hs_n1, hs_n2), dim=-1)\n",
    "\n",
    "    # torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "    plt.figure()\n",
    "    plt.imshow(mix_all[0,:,:,0].abs())\n",
    "    plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(10000, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "\n",
    "#%% load data and model\n",
    "class DOA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mainnet = nn.Sequential(\n",
    "            nn.Linear(12,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64,6)\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        m = self.mainnet(x)\n",
    "        h6 = self.hnet(m)\n",
    "        rx12 = self.rxnet(m)\n",
    "\n",
    "        return h6, rx12 \n",
    "\n",
    "def lower2matrix(rx12):\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[0,0], rx_inv_hat[1,1], rx_inv_hat[2,2] = \\\n",
    "        rx_inv_hat[0,0]/2, rx_inv_hat[1,1]/2, rx_inv_hat[2,2]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "model = DOA().cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "#%%\n",
    "M, btsize, n_tr = 3, 64, int(1e4)\n",
    "lamb = 1\n",
    "const_range = torch.arange(M).to(torch.cfloat)[None,:].cuda()\n",
    "data = Data.TensorDataset(mix_all[:n_tr], H_all[:n_tr])\n",
    "tr = Data.DataLoader(data, batch_size=btsize, drop_last=True, shuffle=True)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= 1e-4,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "\"Pre calc constants\"\n",
    "Is = torch.stack([torch.eye(3,3)]*btsize, dim=0).to(torch.cfloat).cuda()\n",
    "Is2 = torch.stack([torch.eye(3,3)]*200, dim=0).to(torch.cfloat).cuda()\n",
    "ind = torch.tril_indices(3,3)\n",
    "indx = ind[0]*3+ind[1]\n",
    "\n",
    "\"validation\"\n",
    "val0, h0 = mix_all[n_tr:n_tr+200], H_all[n_tr:n_tr+200].cuda()\n",
    "rx_val = (val0[...,None] @ val0[:,:,:,None,:].conj()).mean(dim=(1,2))\n",
    "rx_val_cuda = rx_val.cuda()\n",
    "\n",
    "loss_all, loss_val_all = [], []\n",
    "for epoch in range(2001):\n",
    "    for i, (mix, H) in enumerate(tr):\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        mix, H = mix.cuda(), H.cuda()\n",
    "        for j in range(3): # recursive for each source\n",
    "            if j == 0:\n",
    "                Rx = mix[...,None] @ mix[:,:,:,None,:].conj()\n",
    "                rx = Rx.mean(dim=(1,2))\n",
    "            else:\n",
    "                w = rx_inv_hat@hhat[...,None] / \\\n",
    "                        (hhat[:,None,:].conj()@rx_inv_hat@hhat[...,None])\n",
    "                p = Is - hhat[...,None]@w.permute(0,2,1).conj()\n",
    "                rx = p@rx@p.permute(0,2,1).conj()\n",
    "\n",
    "            inp = torch.stack((rx.real, rx.imag), dim=1)[:,:,ind[0], ind[1]].reshape(btsize, -1)\n",
    "            h6, rx12 = model(inp)\n",
    "            hhat = h6[:,:3] + 1j*h6[:,3:]\n",
    "            rx_inv_hat = lower2matrix(rx12)\n",
    "            loss = loss + ((Is-rx_inv_hat@rx).abs()**2).mean() + \\\n",
    "                lamb*((H[:,:,j]-hhat).abs()**2).mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        if i % 30 == 0:\n",
    "            loss_all.append(loss.detach().cpu().item())\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_all, '-x')\n",
    "        plt.title(f'epoch {epoch}')\n",
    "        plt.show()\n",
    "\n",
    "        if epoch >50:\n",
    "            plt.figure()\n",
    "            plt.plot(loss_all[-100:], '-rx')\n",
    "            plt.title(f'epoch {epoch}')\n",
    "            plt.show()\n",
    "\n",
    "        \"Validation\"\n",
    "        with torch.no_grad():\n",
    "            loss_val = 0\n",
    "            for j in range(3): # recursive for each source\n",
    "                if j ==0 :\n",
    "                    rx = rx_val_cuda\n",
    "                else:\n",
    "                    w = rx_inv_hat_val@hhat_val[...,None] / \\\n",
    "                            (hhat_val[:,None,:].conj()@rx_inv_hat_val@hhat_val[...,None])\n",
    "                    p = Is2 - hhat_val[...,None]@w.permute(0,2,1).conj()\n",
    "                    rx = p@rx@p.permute(0,2,1).conj()\n",
    "\n",
    "                rl = rx.reshape(200, -1)[:,indx] # take lower triangle\n",
    "                inp = torch.stack((rl.real, rl.imag), dim=1).reshape(200, -1)\n",
    "                h6, rx12 = model(inp)\n",
    "                hhat_val = h6[:,:3] + 1j*h6[:,3:]\n",
    "                rx_inv_hat_val = lower2matrix(rx12)\n",
    "\n",
    "                loss_val = loss_val + ((Is2-rx_inv_hat_val@rx).abs()**2).mean() + \\\n",
    "                    lamb*((h0[:,:,j]-hhat_val).abs()**2).mean()\n",
    "\n",
    "            loss_val_all.append(loss_val.cpu().item())\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all, '-x')\n",
    "            plt.title(f'val loss at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_validation_loss')\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all[-50:], '-rx')\n",
    "            plt.title(f'last 50 val loss epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_50val_at_epoch{epoch}')\n",
    "            plt.close('all')   \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v12\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "rid = 'v12'\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "\n",
    "#%% generate data\n",
    "if True:\n",
    "    M, n_data= 3, int(1.2e4)\n",
    "    dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "    v0 = dicts['v'][..., 0]\n",
    "    v1 = dicts['v'][..., 1]\n",
    "    v2 = dicts['v'][..., 2]\n",
    "    from skimage.transform import resize\n",
    "    v0 = torch.tensor(resize(v0, (100, 100), preserve_range=True))\n",
    "    v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "    plt.imshow(v0.abs())\n",
    "    plt.colorbar()\n",
    "    v1 = torch.tensor(resize(v1, (100, 100), preserve_range=True))\n",
    "    v1 = awgn(v1, snr=30, seed=1).abs().to(torch.cfloat)\n",
    "    v2 = torch.tensor(resize(v2, (100, 100), preserve_range=True))\n",
    "    v2 = awgn(v2, snr=30, seed=2).abs().to(torch.cfloat)\n",
    "\n",
    "    snr=0\n",
    "    delta = v0.mean()*10**(-snr/10)\n",
    "    angs = (torch.rand(n_data,1)*20 +10)/180*np.pi  # signal aoa [10, 30]\n",
    "    H = (1j*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    angs_n1 = (torch.rand(n_data,1)*20 -70)/180*np.pi  # noise aoa [-70, -50]\n",
    "    hs_n1 = (1j*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    angs_n2 = (torch.rand(n_data,1)*20 +120)/180*np.pi  # noise aoa [120, 140]\n",
    "    hs_n2 = (1j*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    signal = (H[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:])\n",
    "    n1 = (hs_n1[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:])\n",
    "    n2 = (hs_n2[..., None]@(torch.randn(v2.shape, dtype=torch.cfloat)*(v2**0.5)).flatten()[None,:])\n",
    "    mix =  (signal + n1 + n2).reshape(n_data, M, 100, 100)   \n",
    "    mix_all, sig_all = mix.permute(0,2,3,1), signal.reshape(n_data, M, 100, 100).permute(0,2,3,1)\n",
    "    mixn = awgn_batch(mix_all)\n",
    "    H_all = torch.stack((H, hs_n1, hs_n2), dim=-1)\n",
    "\n",
    "    # torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "    plt.figure()\n",
    "    plt.imshow(mix_all[0,:,:,0].abs())\n",
    "    plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(10000, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "\n",
    "#%% load data and model\n",
    "class DOA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mainnet = nn.Sequential(\n",
    "            nn.Linear(12,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64,6)\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        m = self.mainnet(x)\n",
    "        h6 = self.hnet(m)\n",
    "        rx12 = self.rxnet(m)\n",
    "\n",
    "        return h6, rx12 \n",
    "\n",
    "def lower2matrix(rx12):\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[0,0], rx_inv_hat[1,1], rx_inv_hat[2,2] = \\\n",
    "        rx_inv_hat[0,0]/2, rx_inv_hat[1,1]/2, rx_inv_hat[2,2]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "model = DOA().cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "#%%\n",
    "M, btsize, n_tr = 3, 64, int(1e4)\n",
    "lamb = 1\n",
    "const_range = torch.arange(M).to(torch.cfloat)[None,:].cuda()\n",
    "data = Data.TensorDataset(mix_all[:n_tr], H_all[:n_tr])\n",
    "tr = Data.DataLoader(data, batch_size=btsize, drop_last=True, shuffle=True)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= 1e-4,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "\"Pre calc constants\"\n",
    "Is = torch.stack([torch.eye(3,3)]*btsize, dim=0).to(torch.cfloat).cuda()\n",
    "Is2 = torch.stack([torch.eye(3,3)]*200, dim=0).to(torch.cfloat).cuda()\n",
    "ind = torch.tril_indices(3,3)\n",
    "indx = ind[0]*3+ind[1]\n",
    "\n",
    "\"validation\"\n",
    "val0, h0 = mix_all[n_tr:n_tr+200], H_all[n_tr:n_tr+200].cuda()\n",
    "rx_val = (val0[...,None] @ val0[:,:,:,None,:].conj()).mean(dim=(1,2))\n",
    "rx_val_cuda = rx_val.cuda()\n",
    "\n",
    "loss_all, loss_val_all = [], []\n",
    "for epoch in range(2001):\n",
    "    for i, (mix, H) in enumerate(tr):\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        mix, H = mix.cuda(), H.cuda()\n",
    "        for j in range(3): # recursive for each source\n",
    "            if j == 0:\n",
    "                Rx = mix[...,None] @ mix[:,:,:,None,:].conj()\n",
    "                rx = Rx.mean(dim=(1,2))\n",
    "            else:\n",
    "                w = rx_inv_hat@hhat[...,None] / \\\n",
    "                        (hhat[:,None,:].conj()@rx_inv_hat@hhat[...,None])\n",
    "                p = Is - hhat[...,None]@w.permute(0,2,1).conj()\n",
    "                rx = p@rx@p.permute(0,2,1).conj()\n",
    "\n",
    "            inp = torch.stack((rx.real, rx.imag), dim=1)[:,:,ind[0], ind[1]].reshape(btsize, -1)\n",
    "            h6, rx12 = model(inp)\n",
    "            hhat = h6[:,:3] + 1j*h6[:,3:]\n",
    "            rx_inv_hat = lower2matrix(rx12)\n",
    "            loss = loss + ((Is-rx_inv_hat@rx).abs()**2).mean() + \\\n",
    "                lamb*((H[:,:,j]-hhat).abs()**2).mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        if i % 30 == 0:\n",
    "            loss_all.append(loss.detach().cpu().item())\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_all, '-x')\n",
    "        plt.title(f'epoch {epoch}')\n",
    "        plt.show()\n",
    "\n",
    "        if epoch >50:\n",
    "            plt.figure()\n",
    "            plt.plot(loss_all[-100:], '-rx')\n",
    "            plt.title(f'epoch {epoch}')\n",
    "            plt.show()\n",
    "\n",
    "        \"Validation\"\n",
    "        with torch.no_grad():\n",
    "            loss_val = 0\n",
    "            for j in range(3): # recursive for each source\n",
    "                if j ==0 :\n",
    "                    rx = rx_val_cuda\n",
    "                else:\n",
    "                    w = rx_inv_hat_val@hhat_val[...,None] / \\\n",
    "                            (hhat_val[:,None,:].conj()@rx_inv_hat_val@hhat_val[...,None])\n",
    "                    p = Is2 - hhat_val[...,None]@w.permute(0,2,1).conj()\n",
    "                    rx = p@rx@p.permute(0,2,1).conj()\n",
    "\n",
    "                rl = rx.reshape(200, -1)[:,indx] # take lower triangle\n",
    "                inp = torch.stack((rl.real, rl.imag), dim=1).reshape(200, -1)\n",
    "                h6, rx12 = model(inp)\n",
    "                hhat_val = h6[:,:3] + 1j*h6[:,3:]\n",
    "                rx_inv_hat_val = lower2matrix(rx12)\n",
    "\n",
    "                loss_val = loss_val + ((Is2-rx_inv_hat_val@rx).abs()**2).mean() + \\\n",
    "                    lamb*((h0[:,:,j]-hhat_val).abs()**2).mean()\n",
    "\n",
    "            loss_val_all.append(loss_val.cpu().item())\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all, '-x')\n",
    "            plt.title(f'val loss at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_validation_loss')\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all[-50:], '-rx')\n",
    "            plt.title(f'last 50 val loss epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_50val_at_epoch{epoch}')\n",
    "            plt.close('all')   \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v13\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "rid = 'v13'\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "\n",
    "#%% generate data\n",
    "if True:\n",
    "    M, n_data= 3, int(1.2e4)\n",
    "    dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "    v0 = dicts['v'][..., 0]\n",
    "    v1 = dicts['v'][..., 1]\n",
    "    v2 = dicts['v'][..., 2]\n",
    "    from skimage.transform import resize\n",
    "    v0 = torch.tensor(resize(v0, (100, 100), preserve_range=True))\n",
    "    v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "    plt.imshow(v0.abs())\n",
    "    plt.colorbar()\n",
    "    v1 = torch.tensor(resize(v1, (100, 100), preserve_range=True))\n",
    "    v1 = awgn(v1, snr=30, seed=1).abs().to(torch.cfloat)\n",
    "    v2 = torch.tensor(resize(v2, (100, 100), preserve_range=True))\n",
    "    v2 = awgn(v2, snr=30, seed=2).abs().to(torch.cfloat)\n",
    "\n",
    "    snr=0\n",
    "    delta = v0.mean()*10**(-snr/10)\n",
    "    angs = (torch.rand(n_data,1)*20 +10)/180*np.pi  # signal aoa [10, 30]\n",
    "    H = (1j*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    angs_n1 = (torch.rand(n_data,1)*20 -70)/180*np.pi  # noise aoa [-70, -50]\n",
    "    hs_n1 = (1j*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    angs_n2 = (torch.rand(n_data,1)*20 +120)/180*np.pi  # noise aoa [120, 140]\n",
    "    hs_n2 = (1j*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    signal = (H[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:])\n",
    "    n1 = (hs_n1[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:])\n",
    "    n2 = (hs_n2[..., None]@(torch.randn(v2.shape, dtype=torch.cfloat)*(v2**0.5)).flatten()[None,:])\n",
    "    mix =  (signal + n1 + n2).reshape(n_data, M, 100, 100)   \n",
    "    mix_all, sig_all = mix.permute(0,2,3,1), signal.reshape(n_data, M, 100, 100).permute(0,2,3,1)\n",
    "    mixn = awgn_batch(mix_all)\n",
    "    H_all = torch.stack((H, hs_n1, hs_n2), dim=-1)\n",
    "\n",
    "    # torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "    plt.figure()\n",
    "    plt.imshow(mix_all[0,:,:,0].abs())\n",
    "    plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(10000, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "\n",
    "#%% load data and model\n",
    "class DOA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mainnet = nn.Sequential(\n",
    "            nn.Linear(12,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64,6)\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        m = self.mainnet(x)\n",
    "        h6 = self.hnet(m)\n",
    "        rx12 = self.rxnet(m)\n",
    "\n",
    "        return h6, rx12 \n",
    "\n",
    "def lower2matrix(rx12):\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[0,0], rx_inv_hat[1,1], rx_inv_hat[2,2] = \\\n",
    "        rx_inv_hat[0,0]/2, rx_inv_hat[1,1]/2, rx_inv_hat[2,2]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "model = DOA().cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "#%%\n",
    "M, btsize, n_tr = 3, 64, int(1e4)\n",
    "lamb = 1\n",
    "const_range = torch.arange(M).to(torch.cfloat)[None,:].cuda()\n",
    "data = Data.TensorDataset(mix_all[:n_tr], H_all[:n_tr])\n",
    "tr = Data.DataLoader(data, batch_size=btsize, drop_last=True, shuffle=True)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= 1e-4,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "\"Pre calc constants\"\n",
    "Is = torch.stack([torch.eye(3,3)]*btsize, dim=0).to(torch.cfloat).cuda()\n",
    "Is2 = torch.stack([torch.eye(3,3)]*200, dim=0).to(torch.cfloat).cuda()\n",
    "ind = torch.tril_indices(3,3)\n",
    "indx = ind[0]*3+ind[1]\n",
    "\n",
    "\"validation\"\n",
    "val0, h0 = mix_all[n_tr:n_tr+200], H_all[n_tr:n_tr+200].cuda()\n",
    "rx_val = (val0[...,None] @ val0[:,:,:,None,:].conj()).mean(dim=(1,2))\n",
    "rx_val_cuda = rx_val.cuda()\n",
    "\n",
    "loss_all, loss_val_all = [], []\n",
    "for epoch in range(2001):\n",
    "    for i, (mix, H) in enumerate(tr):\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        mix, H = mix.cuda(), H.cuda()\n",
    "        for j in range(3): # recursive for each source\n",
    "            if j == 0:\n",
    "                Rx = mix[...,None] @ mix[:,:,:,None,:].conj()\n",
    "                rx = Rx.mean(dim=(1,2))\n",
    "            else:\n",
    "                w = rx_inv_hat@hhat[...,None] / \\\n",
    "                        (hhat[:,None,:].conj()@rx_inv_hat@hhat[...,None])\n",
    "                p = Is - hhat[...,None]@w.permute(0,2,1).conj()\n",
    "                rx = p@rx@p.permute(0,2,1).conj()\n",
    "\n",
    "            inp = torch.stack((rx.real, rx.imag), dim=1)[:,:,ind[0], ind[1]].reshape(btsize, -1)\n",
    "            h6, rx12 = model(inp)\n",
    "            hhat = h6[:,:3] + 1j*h6[:,3:]\n",
    "            rx_inv_hat = lower2matrix(rx12)\n",
    "            loss = loss + ((Is-rx_inv_hat@rx).abs()**2).mean() + \\\n",
    "                lamb*((H[:,:,j]-hhat).abs()**2).mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        if i % 30 == 0:\n",
    "            loss_all.append(loss.detach().cpu().item())\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_all, '-x')\n",
    "        plt.title(f'epoch {epoch}')\n",
    "        plt.show()\n",
    "\n",
    "        if epoch >50:\n",
    "            plt.figure()\n",
    "            plt.plot(loss_all[-100:], '-rx')\n",
    "            plt.title(f'epoch {epoch}')\n",
    "            plt.show()\n",
    "\n",
    "        \"Validation\"\n",
    "        with torch.no_grad():\n",
    "            loss_val = 0\n",
    "            for j in range(3): # recursive for each source\n",
    "                if j ==0 :\n",
    "                    rx = rx_val_cuda\n",
    "                else:\n",
    "                    w = rx_inv_hat_val@hhat_val[...,None] / \\\n",
    "                            (hhat_val[:,None,:].conj()@rx_inv_hat_val@hhat_val[...,None])\n",
    "                    p = Is2 - hhat_val[...,None]@w.permute(0,2,1).conj()\n",
    "                    rx = p@rx@p.permute(0,2,1).conj()\n",
    "\n",
    "                rl = rx.reshape(200, -1)[:,indx] # take lower triangle\n",
    "                inp = torch.stack((rl.real, rl.imag), dim=1).reshape(200, -1)\n",
    "                h6, rx12 = model(inp)\n",
    "                hhat_val = h6[:,:3] + 1j*h6[:,3:]\n",
    "                rx_inv_hat_val = lower2matrix(rx12)\n",
    "\n",
    "                loss_val = loss_val + ((Is2-rx_inv_hat_val@rx).abs()**2).mean() + \\\n",
    "                    lamb*((h0[:,:,j]-hhat_val).abs()**2).mean()\n",
    "\n",
    "            loss_val_all.append(loss_val.cpu().item())\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all, '-x')\n",
    "            plt.title(f'val loss at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_validation_loss')\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all[-50:], '-rx')\n",
    "            plt.title(f'last 50 val loss epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_50val_at_epoch{epoch}')\n",
    "            plt.close('all')   \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v14\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "rid = 'v14'\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "\n",
    "#%% generate data\n",
    "if True:\n",
    "    M, n_data= 3, int(1.2e4)\n",
    "    dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "    v0 = dicts['v'][..., 0]\n",
    "    v1 = dicts['v'][..., 1]\n",
    "    v2 = dicts['v'][..., 2]\n",
    "    from skimage.transform import resize\n",
    "    v0 = torch.tensor(resize(v0, (100, 100), preserve_range=True))\n",
    "    v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "    plt.imshow(v0.abs())\n",
    "    plt.colorbar()\n",
    "    v1 = torch.tensor(resize(v1, (100, 100), preserve_range=True))\n",
    "    v1 = awgn(v1, snr=30, seed=1).abs().to(torch.cfloat)\n",
    "    v2 = torch.tensor(resize(v2, (100, 100), preserve_range=True))\n",
    "    v2 = awgn(v2, snr=30, seed=2).abs().to(torch.cfloat)\n",
    "\n",
    "    snr=0\n",
    "    delta = v0.mean()*10**(-snr/10)\n",
    "    angs = (torch.rand(n_data,1)*20 +10)/180*np.pi  # signal aoa [10, 30]\n",
    "    H = (1j*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    angs_n1 = (torch.rand(n_data,1)*20 -70)/180*np.pi  # noise aoa [-70, -50]\n",
    "    hs_n1 = (1j*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    angs_n2 = (torch.rand(n_data,1)*20 +120)/180*np.pi  # noise aoa [120, 140]\n",
    "    hs_n2 = (1j*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    signal = (H[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:])\n",
    "    n1 = (hs_n1[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:])\n",
    "    n2 = (hs_n2[..., None]@(torch.randn(v2.shape, dtype=torch.cfloat)*(v2**0.5)).flatten()[None,:])\n",
    "    mix =  (signal + n1 + n2).reshape(n_data, M, 100, 100)   \n",
    "    mix_all, sig_all = mix.permute(0,2,3,1), signal.reshape(n_data, M, 100, 100).permute(0,2,3,1)\n",
    "    mixn = awgn_batch(mix_all)\n",
    "    H_all = torch.stack((H, hs_n1, hs_n2), dim=-1)\n",
    "\n",
    "    # torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "    plt.figure()\n",
    "    plt.imshow(mix_all[0,:,:,0].abs())\n",
    "    plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(10000, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "\n",
    "#%% load data and model\n",
    "class DOA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mainnet = nn.Sequential(\n",
    "            nn.Linear(12,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64,6)\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        m = self.mainnet(x)\n",
    "        h6 = self.hnet(m)\n",
    "        rx12 = self.rxnet(m)\n",
    "\n",
    "        return h6, rx12 \n",
    "\n",
    "def lower2matrix(rx12):\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[0,0], rx_inv_hat[1,1], rx_inv_hat[2,2] = \\\n",
    "        rx_inv_hat[0,0]/2, rx_inv_hat[1,1]/2, rx_inv_hat[2,2]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "model = DOA().cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "#%%\n",
    "M, btsize, n_tr = 3, 64, int(1e4)\n",
    "lamb = 0.1\n",
    "const_range = torch.arange(M).to(torch.cfloat)[None,:].cuda()\n",
    "data = Data.TensorDataset(mix_all[:n_tr], H_all[:n_tr])\n",
    "tr = Data.DataLoader(data, batch_size=btsize, drop_last=True, shuffle=True)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= 1e-4,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "\"Pre calc constants\"\n",
    "Is = torch.stack([torch.eye(3,3)]*btsize, dim=0).to(torch.cfloat).cuda()\n",
    "Is2 = torch.stack([torch.eye(3,3)]*200, dim=0).to(torch.cfloat).cuda()\n",
    "ind = torch.tril_indices(3,3)\n",
    "indx = ind[0]*3+ind[1]\n",
    "\n",
    "\"validation\"\n",
    "val0, h0 = mix_all[n_tr:n_tr+200], H_all[n_tr:n_tr+200].cuda()\n",
    "rx_val = (val0[...,None] @ val0[:,:,:,None,:].conj()).mean(dim=(1,2))\n",
    "rx_val_cuda = rx_val.cuda()\n",
    "\n",
    "loss_all, loss_val_all = [], []\n",
    "for epoch in range(2001):\n",
    "    for i, (mix, H) in enumerate(tr):\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        mix, H = mix.cuda(), H.cuda()\n",
    "        for j in range(3): # recursive for each source\n",
    "            if j == 0:\n",
    "                Rx = mix[...,None] @ mix[:,:,:,None,:].conj()\n",
    "                rx = Rx.mean(dim=(1,2))\n",
    "            else:\n",
    "                w = rx_inv_hat@hhat[...,None] / \\\n",
    "                        (hhat[:,None,:].conj()@rx_inv_hat@hhat[...,None])\n",
    "                p = Is - hhat[...,None]@w.permute(0,2,1).conj()\n",
    "                rx = p@rx@p.permute(0,2,1).conj()\n",
    "\n",
    "            inp = torch.stack((rx.real, rx.imag), dim=1)[:,:,ind[0], ind[1]].reshape(btsize, -1)\n",
    "            h6, rx12 = model(inp)\n",
    "            hhat = h6[:,:3] + 1j*h6[:,3:]\n",
    "            rx_inv_hat = lower2matrix(rx12)\n",
    "            loss = loss + ((Is-rx_inv_hat@rx).abs()**2).mean() + \\\n",
    "                lamb*((H[:,:,j]-hhat).abs()**2).mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        if i % 30 == 0:\n",
    "            loss_all.append(loss.detach().cpu().item())\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_all, '-x')\n",
    "        plt.title(f'epoch {epoch}')\n",
    "        plt.show()\n",
    "\n",
    "        if epoch >50:\n",
    "            plt.figure()\n",
    "            plt.plot(loss_all[-100:], '-rx')\n",
    "            plt.title(f'epoch {epoch}')\n",
    "            plt.show()\n",
    "\n",
    "        \"Validation\"\n",
    "        with torch.no_grad():\n",
    "            loss_val = 0\n",
    "            for j in range(3): # recursive for each source\n",
    "                if j ==0 :\n",
    "                    rx = rx_val_cuda\n",
    "                else:\n",
    "                    w = rx_inv_hat_val@hhat_val[...,None] / \\\n",
    "                            (hhat_val[:,None,:].conj()@rx_inv_hat_val@hhat_val[...,None])\n",
    "                    p = Is2 - hhat_val[...,None]@w.permute(0,2,1).conj()\n",
    "                    rx = p@rx@p.permute(0,2,1).conj()\n",
    "\n",
    "                rl = rx.reshape(200, -1)[:,indx] # take lower triangle\n",
    "                inp = torch.stack((rl.real, rl.imag), dim=1).reshape(200, -1)\n",
    "                h6, rx12 = model(inp)\n",
    "                hhat_val = h6[:,:3] + 1j*h6[:,3:]\n",
    "                rx_inv_hat_val = lower2matrix(rx12)\n",
    "\n",
    "                loss_val = loss_val + ((Is2-rx_inv_hat_val@rx).abs()**2).mean() + \\\n",
    "                    lamb*((h0[:,:,j]-hhat_val).abs()**2).mean()\n",
    "\n",
    "            loss_val_all.append(loss_val.cpu().item())\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all, '-x')\n",
    "            plt.title(f'val loss at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_validation_loss')\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all[-50:], '-rx')\n",
    "            plt.title(f'last 50 val loss epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_50val_at_epoch{epoch}')\n",
    "            plt.close('all')   \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v15\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "rid = 'v15'\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "\n",
    "#%% generate data\n",
    "if True:\n",
    "    M, n_data= 3, int(1.2e4)\n",
    "    dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "    v0 = dicts['v'][..., 0]\n",
    "    v1 = dicts['v'][..., 1]\n",
    "    v2 = dicts['v'][..., 2]\n",
    "    from skimage.transform import resize\n",
    "    v0 = torch.tensor(resize(v0, (100, 100), preserve_range=True))\n",
    "    v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "    plt.imshow(v0.abs())\n",
    "    plt.colorbar()\n",
    "    v1 = torch.tensor(resize(v1, (100, 100), preserve_range=True))\n",
    "    v1 = awgn(v1, snr=30, seed=1).abs().to(torch.cfloat)\n",
    "    v2 = torch.tensor(resize(v2, (100, 100), preserve_range=True))\n",
    "    v2 = awgn(v2, snr=30, seed=2).abs().to(torch.cfloat)\n",
    "\n",
    "    snr=0\n",
    "    delta = v0.mean()*10**(-snr/10)\n",
    "    angs = (torch.rand(n_data,1)*20 +10)/180*np.pi  # signal aoa [10, 30]\n",
    "    H = (1j*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    angs_n1 = (torch.rand(n_data,1)*20 -70)/180*np.pi  # noise aoa [-70, -50]\n",
    "    hs_n1 = (1j*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    angs_n2 = (torch.rand(n_data,1)*20 +120)/180*np.pi  # noise aoa [120, 140]\n",
    "    hs_n2 = (1j*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    signal = (H[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:])\n",
    "    n1 = (hs_n1[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:])\n",
    "    n2 = (hs_n2[..., None]@(torch.randn(v2.shape, dtype=torch.cfloat)*(v2**0.5)).flatten()[None,:])\n",
    "    mix =  (signal + n1 + n2).reshape(n_data, M, 100, 100)   \n",
    "    mix_all, sig_all = mix.permute(0,2,3,1), signal.reshape(n_data, M, 100, 100).permute(0,2,3,1)\n",
    "    mixn = awgn_batch(mix_all)\n",
    "    H_all = torch.stack((H, hs_n1, hs_n2), dim=-1)\n",
    "\n",
    "    # torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "    plt.figure()\n",
    "    plt.imshow(mix_all[0,:,:,0].abs())\n",
    "    plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(10000, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "\n",
    "#%% load data and model\n",
    "class DOA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mainnet = nn.Sequential(\n",
    "            nn.Linear(12,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64,6)\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        m = self.mainnet(x)\n",
    "        h6 = self.hnet(m)\n",
    "        rx12 = self.rxnet(m)\n",
    "\n",
    "        return h6, rx12 \n",
    "\n",
    "def lower2matrix(rx12):\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[0,0], rx_inv_hat[1,1], rx_inv_hat[2,2] = \\\n",
    "        rx_inv_hat[0,0]/2, rx_inv_hat[1,1]/2, rx_inv_hat[2,2]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "model = DOA().cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "#%%\n",
    "M, btsize, n_tr = 3, 64, int(1e4)\n",
    "lamb = 10\n",
    "const_range = torch.arange(M).to(torch.cfloat)[None,:].cuda()\n",
    "data = Data.TensorDataset(mix_all[:n_tr], H_all[:n_tr])\n",
    "tr = Data.DataLoader(data, batch_size=btsize, drop_last=True, shuffle=True)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= 1e-4,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "\"Pre calc constants\"\n",
    "Is = torch.stack([torch.eye(3,3)]*btsize, dim=0).to(torch.cfloat).cuda()\n",
    "Is2 = torch.stack([torch.eye(3,3)]*200, dim=0).to(torch.cfloat).cuda()\n",
    "ind = torch.tril_indices(3,3)\n",
    "indx = ind[0]*3+ind[1]\n",
    "\n",
    "\"validation\"\n",
    "val0, h0 = mix_all[n_tr:n_tr+200], H_all[n_tr:n_tr+200].cuda()\n",
    "rx_val = (val0[...,None] @ val0[:,:,:,None,:].conj()).mean(dim=(1,2))\n",
    "rx_val_cuda = rx_val.cuda()\n",
    "\n",
    "loss_all, loss_val_all = [], []\n",
    "for epoch in range(2001):\n",
    "    for i, (mix, H) in enumerate(tr):\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        mix, H = mix.cuda(), H.cuda()\n",
    "        for j in range(3): # recursive for each source\n",
    "            if j == 0:\n",
    "                Rx = mix[...,None] @ mix[:,:,:,None,:].conj()\n",
    "                rx = Rx.mean(dim=(1,2))\n",
    "            else:\n",
    "                w = rx_inv_hat@hhat[...,None] / \\\n",
    "                        (hhat[:,None,:].conj()@rx_inv_hat@hhat[...,None])\n",
    "                p = Is - hhat[...,None]@w.permute(0,2,1).conj()\n",
    "                rx = p@rx@p.permute(0,2,1).conj()\n",
    "\n",
    "            inp = torch.stack((rx.real, rx.imag), dim=1)[:,:,ind[0], ind[1]].reshape(btsize, -1)\n",
    "            h6, rx12 = model(inp)\n",
    "            hhat = h6[:,:3] + 1j*h6[:,3:]\n",
    "            rx_inv_hat = lower2matrix(rx12)\n",
    "            loss = loss + ((Is-rx_inv_hat@rx).abs()**2).mean() + \\\n",
    "                lamb*((H[:,:,j]-hhat).abs()**2).mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        if i % 30 == 0:\n",
    "            loss_all.append(loss.detach().cpu().item())\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_all, '-x')\n",
    "        plt.title(f'epoch {epoch}')\n",
    "        plt.show()\n",
    "\n",
    "        if epoch >50:\n",
    "            plt.figure()\n",
    "            plt.plot(loss_all[-100:], '-rx')\n",
    "            plt.title(f'epoch {epoch}')\n",
    "            plt.show()\n",
    "\n",
    "        \"Validation\"\n",
    "        with torch.no_grad():\n",
    "            loss_val = 0\n",
    "            for j in range(3): # recursive for each source\n",
    "                if j ==0 :\n",
    "                    rx = rx_val_cuda\n",
    "                else:\n",
    "                    w = rx_inv_hat_val@hhat_val[...,None] / \\\n",
    "                            (hhat_val[:,None,:].conj()@rx_inv_hat_val@hhat_val[...,None])\n",
    "                    p = Is2 - hhat_val[...,None]@w.permute(0,2,1).conj()\n",
    "                    rx = p@rx@p.permute(0,2,1).conj()\n",
    "\n",
    "                rl = rx.reshape(200, -1)[:,indx] # take lower triangle\n",
    "                inp = torch.stack((rl.real, rl.imag), dim=1).reshape(200, -1)\n",
    "                h6, rx12 = model(inp)\n",
    "                hhat_val = h6[:,:3] + 1j*h6[:,3:]\n",
    "                rx_inv_hat_val = lower2matrix(rx12)\n",
    "\n",
    "                loss_val = loss_val + ((Is2-rx_inv_hat_val@rx).abs()**2).mean() + \\\n",
    "                    lamb*((h0[:,:,j]-hhat_val).abs()**2).mean()\n",
    "\n",
    "            loss_val_all.append(loss_val.cpu().item())\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all, '-x')\n",
    "            plt.title(f'val loss at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_validation_loss')\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all[-50:], '-rx')\n",
    "            plt.title(f'last 50 val loss epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_50val_at_epoch{epoch}')\n",
    "            plt.close('all')   \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%  v16\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "rid = 'v16'\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "\n",
    "#%% generate data\n",
    "if True:\n",
    "    M, n_data= 3, int(1.2e4)\n",
    "    dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "    v0 = dicts['v'][..., 0]\n",
    "    v1 = dicts['v'][..., 1]\n",
    "    v2 = dicts['v'][..., 2]\n",
    "    from skimage.transform import resize\n",
    "    v0 = torch.tensor(resize(v0, (100, 100), preserve_range=True))\n",
    "    v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "    plt.imshow(v0.abs())\n",
    "    plt.colorbar()\n",
    "    v1 = torch.tensor(resize(v1, (100, 100), preserve_range=True))\n",
    "    v1 = awgn(v1, snr=30, seed=1).abs().to(torch.cfloat)\n",
    "    v2 = torch.tensor(resize(v2, (100, 100), preserve_range=True))\n",
    "    v2 = awgn(v2, snr=30, seed=2).abs().to(torch.cfloat)\n",
    "\n",
    "    snr=0\n",
    "    delta = v0.mean()*10**(-snr/10)\n",
    "    angs = (torch.rand(n_data,1)*20 +10)/180*np.pi  # signal aoa [10, 30]\n",
    "    H = (1j*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    angs_n1 = (torch.rand(n_data,1)*20 -70)/180*np.pi  # noise aoa [-70, -50]\n",
    "    hs_n1 = (1j*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    angs_n2 = (torch.rand(n_data,1)*20 +120)/180*np.pi  # noise aoa [120, 140]\n",
    "    hs_n2 = (1j*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    signal = (H[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:])\n",
    "    n1 = (hs_n1[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:])\n",
    "    n2 = (hs_n2[..., None]@(torch.randn(v2.shape, dtype=torch.cfloat)*(v2**0.5)).flatten()[None,:])\n",
    "    mix =  (signal + n1 + n2).reshape(n_data, M, 100, 100)   \n",
    "    mix_all, sig_all = mix.permute(0,2,3,1), signal.reshape(n_data, M, 100, 100).permute(0,2,3,1)\n",
    "    mixn = awgn_batch(mix_all)\n",
    "    H_all = torch.stack((H, hs_n1, hs_n2), dim=-1)\n",
    "\n",
    "    # torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "    plt.figure()\n",
    "    plt.imshow(mix_all[0,:,:,0].abs())\n",
    "    plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(10000, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "\n",
    "#%% load data and model\n",
    "class DOA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mainnet = nn.Sequential(\n",
    "            nn.Linear(12,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64,6)\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        m = self.mainnet(x)\n",
    "        h6 = self.hnet(m)\n",
    "        rx12 = self.rxnet(m)\n",
    "\n",
    "        return h6, rx12 \n",
    "\n",
    "def lower2matrix(rx12):\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[0,0], rx_inv_hat[1,1], rx_inv_hat[2,2] = \\\n",
    "        rx_inv_hat[0,0]/2, rx_inv_hat[1,1]/2, rx_inv_hat[2,2]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "model = DOA().cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "#%%\n",
    "M, btsize, n_tr = 3, 64, int(1e4)\n",
    "lamb = 1\n",
    "const_range = torch.arange(M).to(torch.cfloat)[None,:].cuda()\n",
    "data = Data.TensorDataset(mix_all[:n_tr], H_all[:n_tr])\n",
    "tr = Data.DataLoader(data, batch_size=btsize, drop_last=True, shuffle=True)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= 1e-4,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "\"Pre calc constants\"\n",
    "Is = torch.stack([torch.eye(3,3)]*btsize, dim=0).to(torch.cfloat).cuda()\n",
    "Is2 = torch.stack([torch.eye(3,3)]*200, dim=0).to(torch.cfloat).cuda()\n",
    "ind = torch.tril_indices(3,3)\n",
    "indx = ind[0]*3+ind[1]\n",
    "\n",
    "\"validation\"\n",
    "val0, h0 = mix_all[n_tr:n_tr+200], H_all[n_tr:n_tr+200].cuda()\n",
    "rx_val = (val0[...,None] @ val0[:,:,:,None,:].conj()).mean(dim=(1,2))\n",
    "rx_val_cuda = rx_val.cuda()\n",
    "\n",
    "loss_all, loss_val_all = [], []\n",
    "for epoch in range(2001):\n",
    "    for i, (mix, H) in enumerate(tr):\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        mix, H = mix.cuda(), H.cuda()\n",
    "        for j in range(3): # recursive for each source\n",
    "            if j == 0:\n",
    "                Rx = mix[...,None] @ mix[:,:,:,None,:].conj()\n",
    "                rx = Rx.mean(dim=(1,2))\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    w = rx_inv_hat@hhat[...,None] / \\\n",
    "                            (hhat[:,None,:].conj()@rx_inv_hat@hhat[...,None])\n",
    "                    p = Is - hhat[...,None]@w.permute(0,2,1).conj()\n",
    "                    rx = p@rx@p.permute(0,2,1).conj()\n",
    "\n",
    "            inp = torch.stack((rx.real, rx.imag), dim=1)[:,:,ind[0], ind[1]].reshape(btsize, -1)\n",
    "            h6, rx12 = model(inp)\n",
    "            hhat = h6[:,:3] + 1j*h6[:,3:]\n",
    "            rx_inv_hat = lower2matrix(rx12)\n",
    "            loss = loss + ((Is-rx_inv_hat@rx).abs()**2).mean() + \\\n",
    "                lamb*((H[:,:,j]-hhat).abs()**2).mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        if i % 30 == 0:\n",
    "            loss_all.append(loss.detach().cpu().item())\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_all, '-x')\n",
    "        plt.title(f'epoch {epoch}')\n",
    "        plt.show()\n",
    "\n",
    "        if epoch >50:\n",
    "            plt.figure()\n",
    "            plt.plot(loss_all[-100:], '-rx')\n",
    "            plt.title(f'epoch {epoch}')\n",
    "            plt.show()\n",
    "\n",
    "        \"Validation\"\n",
    "        with torch.no_grad():\n",
    "            loss_val = 0\n",
    "            for j in range(3): # recursive for each source\n",
    "                if j ==0 :\n",
    "                    rx = rx_val_cuda\n",
    "                else:\n",
    "                    w = rx_inv_hat_val@hhat_val[...,None] / \\\n",
    "                            (hhat_val[:,None,:].conj()@rx_inv_hat_val@hhat_val[...,None])\n",
    "                    p = Is2 - hhat_val[...,None]@w.permute(0,2,1).conj()\n",
    "                    rx = p@rx@p.permute(0,2,1).conj()\n",
    "\n",
    "                rl = rx.reshape(200, -1)[:,indx] # take lower triangle\n",
    "                inp = torch.stack((rl.real, rl.imag), dim=1).reshape(200, -1)\n",
    "                h6, rx12 = model(inp)\n",
    "                hhat_val = h6[:,:3] + 1j*h6[:,3:]\n",
    "                rx_inv_hat_val = lower2matrix(rx12)\n",
    "\n",
    "                loss_val = loss_val + ((Is2-rx_inv_hat_val@rx).abs()**2).mean() + \\\n",
    "                    lamb*((h0[:,:,j]-hhat_val).abs()**2).mean()\n",
    "\n",
    "            loss_val_all.append(loss_val.cpu().item())\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all, '-x')\n",
    "            plt.title(f'val loss at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_validation_loss')\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all[-50:], '-rx')\n",
    "            plt.title(f'last 50 val loss epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_50val_at_epoch{epoch}')\n",
    "            plt.close('all')   \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v17\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "rid = 'v17'\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "\n",
    "#%% generate data\n",
    "if True:\n",
    "    M, n_data= 3, int(1.2e4)\n",
    "    dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "    v0 = dicts['v'][..., 0]\n",
    "    v1 = dicts['v'][..., 1]\n",
    "    v2 = dicts['v'][..., 2]\n",
    "    from skimage.transform import resize\n",
    "    v0 = torch.tensor(resize(v0, (100, 100), preserve_range=True))\n",
    "    v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "    plt.imshow(v0.abs())\n",
    "    plt.colorbar()\n",
    "    v1 = torch.tensor(resize(v1, (100, 100), preserve_range=True))\n",
    "    v1 = awgn(v1, snr=30, seed=1).abs().to(torch.cfloat)\n",
    "    v2 = torch.tensor(resize(v2, (100, 100), preserve_range=True))\n",
    "    v2 = awgn(v2, snr=30, seed=2).abs().to(torch.cfloat)\n",
    "\n",
    "    snr=0\n",
    "    delta = v0.mean()*10**(-snr/10)\n",
    "    angs = (torch.rand(n_data,1)*20 +10)/180*np.pi  # signal aoa [10, 30]\n",
    "    H = (1j*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    angs_n1 = (torch.rand(n_data,1)*20 -70)/180*np.pi  # noise aoa [-70, -50]\n",
    "    hs_n1 = (1j*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    angs_n2 = (torch.rand(n_data,1)*20 +120)/180*np.pi  # noise aoa [120, 140]\n",
    "    hs_n2 = (1j*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    signal = (H[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:])\n",
    "    n1 = (hs_n1[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:])\n",
    "    n2 = (hs_n2[..., None]@(torch.randn(v2.shape, dtype=torch.cfloat)*(v2**0.5)).flatten()[None,:])\n",
    "    mix =  (signal + n1 + n2).reshape(n_data, M, 100, 100)   \n",
    "    mix_all, sig_all = mix.permute(0,2,3,1), signal.reshape(n_data, M, 100, 100).permute(0,2,3,1)\n",
    "    mixn = awgn_batch(mix_all)\n",
    "    H_all = torch.stack((H, hs_n1, hs_n2), dim=-1)\n",
    "\n",
    "    # torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "    plt.figure()\n",
    "    plt.imshow(mix_all[0,:,:,0].abs())\n",
    "    plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(10000, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "\n",
    "#%% load data and model\n",
    "class DOA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mainnet = nn.Sequential(\n",
    "            nn.Linear(12,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64,1)\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        m = self.mainnet(x)\n",
    "        ang = self.hnet(m)\n",
    "        rx12 = self.rxnet(m)\n",
    "        ch = torch.pi*torch.arange(3, device=x.device)\n",
    "        hj = ((ang.tanh() @ ch[None,:])*1j).exp() # shape:[I, M]\n",
    "        return hj, rx12 \n",
    "\n",
    "def lower2matrix(rx12):\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[0,0], rx_inv_hat[1,1], rx_inv_hat[2,2] = \\\n",
    "        rx_inv_hat[0,0]/2, rx_inv_hat[1,1]/2, rx_inv_hat[2,2]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "model = DOA().cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "#%%\n",
    "M, btsize, n_tr = 3, 64, int(1e4)\n",
    "lamb = 1\n",
    "const_range = torch.arange(M).to(torch.cfloat)[None,:].cuda()\n",
    "data = Data.TensorDataset(mix_all[:n_tr], H_all[:n_tr])\n",
    "tr = Data.DataLoader(data, batch_size=btsize, drop_last=True, shuffle=True)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= 1e-4,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "\"Pre calc constants\"\n",
    "Is = torch.stack([torch.eye(3,3)]*btsize, dim=0).to(torch.cfloat).cuda()\n",
    "Is2 = torch.stack([torch.eye(3,3)]*200, dim=0).to(torch.cfloat).cuda()\n",
    "ind = torch.tril_indices(3,3)\n",
    "indx = ind[0]*3+ind[1]\n",
    "\n",
    "\"validation\"\n",
    "val0, h0 = mix_all[n_tr:n_tr+200], H_all[n_tr:n_tr+200].cuda()\n",
    "rx_val = (val0[...,None] @ val0[:,:,:,None,:].conj()).mean(dim=(1,2))\n",
    "rx_val_cuda = rx_val.cuda()\n",
    "\n",
    "loss_all, loss_val_all = [], []\n",
    "for epoch in range(2001):\n",
    "    for i, (mix, H) in enumerate(tr):\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        mix, H = mix.cuda(), H.cuda()\n",
    "        for j in range(3): # recursive for each source\n",
    "            if j == 0:\n",
    "                Rx = mix[...,None] @ mix[:,:,:,None,:].conj()\n",
    "                rx = Rx.mean(dim=(1,2))\n",
    "            else:\n",
    "                w = rx_inv_hat@hhat[...,None] / \\\n",
    "                        (hhat[:,None,:].conj()@rx_inv_hat@hhat[...,None])\n",
    "                p = Is - hhat[...,None]@w.permute(0,2,1).conj()\n",
    "                rx = p@rx@p.permute(0,2,1).conj()\n",
    "\n",
    "            inp = torch.stack((rx.real, rx.imag), dim=1)[:,:,ind[0], ind[1]].reshape(btsize, -1)\n",
    "            hhat, rx12 = model(inp)\n",
    "            rx_inv_hat = lower2matrix(rx12)\n",
    "            loss = loss + ((Is-rx_inv_hat@rx).abs()**2).mean() + \\\n",
    "                lamb*((H[:,:,j]-hhat).abs()**2).mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        if i % 30 == 0:\n",
    "            loss_all.append(loss.detach().cpu().item())\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_all, '-x')\n",
    "        plt.title(f'epoch {epoch}')\n",
    "        plt.show()\n",
    "\n",
    "        if epoch >50:\n",
    "            plt.figure()\n",
    "            plt.plot(loss_all[-100:], '-rx')\n",
    "            plt.title(f'epoch {epoch}')\n",
    "            plt.show()\n",
    "\n",
    "        \"Validation\"\n",
    "        with torch.no_grad():\n",
    "            loss_val = 0\n",
    "            for j in range(3): # recursive for each source\n",
    "                if j ==0 :\n",
    "                    rx = rx_val_cuda\n",
    "                else:\n",
    "                    w = rx_inv_hat_val@hhat_val[...,None] / \\\n",
    "                            (hhat_val[:,None,:].conj()@rx_inv_hat_val@hhat_val[...,None])\n",
    "                    p = Is2 - hhat_val[...,None]@w.permute(0,2,1).conj()\n",
    "                    rx = p@rx@p.permute(0,2,1).conj()\n",
    "\n",
    "                rl = rx.reshape(200, -1)[:,indx] # take lower triangle\n",
    "                inp = torch.stack((rl.real, rl.imag), dim=1).reshape(200, -1)\n",
    "                hhat_val, rx12 = model(inp)\n",
    "                rx_inv_hat_val = lower2matrix(rx12)\n",
    "\n",
    "                loss_val = loss_val + ((Is2-rx_inv_hat_val@rx).abs()**2).mean() + \\\n",
    "                    lamb*((h0[:,:,j]-hhat_val).abs()**2).mean()\n",
    "\n",
    "            loss_val_all.append(loss_val.cpu().item())\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all, '-x')\n",
    "            plt.title(f'val loss at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_validation_loss')\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all[-50:], '-rx')\n",
    "            plt.title(f'last 50 val loss epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_50val_at_epoch{epoch}')\n",
    "            plt.close('all')   \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v18\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "rid = 'v18'\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "\n",
    "#%% generate data\n",
    "if True:\n",
    "    M, n_data= 3, int(1.2e4)\n",
    "    dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "    v0 = dicts['v'][..., 0]\n",
    "    v1 = dicts['v'][..., 1]\n",
    "    v2 = dicts['v'][..., 2]\n",
    "    from skimage.transform import resize\n",
    "    v0 = torch.tensor(resize(v0, (100, 100), preserve_range=True))\n",
    "    v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "    plt.imshow(v0.abs())\n",
    "    plt.colorbar()\n",
    "    v1 = torch.tensor(resize(v1, (100, 100), preserve_range=True))\n",
    "    v1 = awgn(v1, snr=30, seed=1).abs().to(torch.cfloat)\n",
    "    v2 = torch.tensor(resize(v2, (100, 100), preserve_range=True))\n",
    "    v2 = awgn(v2, snr=30, seed=2).abs().to(torch.cfloat)\n",
    "\n",
    "    snr=0\n",
    "    delta = v0.mean()*10**(-snr/10)\n",
    "    angs = (torch.rand(n_data,1)*20 +10)/180*np.pi  # signal aoa [10, 30]\n",
    "    H = (1j*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    angs_n1 = (torch.rand(n_data,1)*20 -70)/180*np.pi  # noise aoa [-70, -50]\n",
    "    hs_n1 = (1j*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    angs_n2 = (torch.rand(n_data,1)*20 +120)/180*np.pi  # noise aoa [120, 140]\n",
    "    hs_n2 = (1j*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    signal = (H[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:])\n",
    "    n1 = (hs_n1[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:])\n",
    "    n2 = (hs_n2[..., None]@(torch.randn(v2.shape, dtype=torch.cfloat)*(v2**0.5)).flatten()[None,:])\n",
    "    mix =  (signal + n1 + n2).reshape(n_data, M, 100, 100)   \n",
    "    mix_all, sig_all = mix.permute(0,2,3,1), signal.reshape(n_data, M, 100, 100).permute(0,2,3,1)\n",
    "    mixn = awgn_batch(mix_all)\n",
    "    H_all = torch.stack((H, hs_n1, hs_n2), dim=-1)\n",
    "\n",
    "    # torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "    plt.figure()\n",
    "    plt.imshow(mix_all[0,:,:,0].abs())\n",
    "    plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(10000, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "\n",
    "#%% load data and model\n",
    "class DOA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mainnet = nn.Sequential(\n",
    "            nn.Linear(12,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64,1)\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        m = self.mainnet(x)\n",
    "        ang = self.hnet(m)\n",
    "        rx12 = self.rxnet(m)\n",
    "        ch = torch.pi*torch.arange(3, device=x.device)\n",
    "        hj = ((ang.tanh() @ ch[None,:])*1j).exp() # shape:[I, M]\n",
    "        return hj, rx12 \n",
    "\n",
    "def lower2matrix(rx12):\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[0,0], rx_inv_hat[1,1], rx_inv_hat[2,2] = \\\n",
    "        rx_inv_hat[0,0]/2, rx_inv_hat[1,1]/2, rx_inv_hat[2,2]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "model = DOA().cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "#%%\n",
    "M, btsize, n_tr = 3, 64, int(1e4)\n",
    "lamb = 0.1\n",
    "const_range = torch.arange(M).to(torch.cfloat)[None,:].cuda()\n",
    "data = Data.TensorDataset(mix_all[:n_tr], H_all[:n_tr])\n",
    "tr = Data.DataLoader(data, batch_size=btsize, drop_last=True, shuffle=True)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= 1e-4,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "\"Pre calc constants\"\n",
    "Is = torch.stack([torch.eye(3,3)]*btsize, dim=0).to(torch.cfloat).cuda()\n",
    "Is2 = torch.stack([torch.eye(3,3)]*200, dim=0).to(torch.cfloat).cuda()\n",
    "ind = torch.tril_indices(3,3)\n",
    "indx = ind[0]*3+ind[1]\n",
    "\n",
    "\"validation\"\n",
    "val0, h0 = mix_all[n_tr:n_tr+200], H_all[n_tr:n_tr+200].cuda()\n",
    "rx_val = (val0[...,None] @ val0[:,:,:,None,:].conj()).mean(dim=(1,2))\n",
    "rx_val_cuda = rx_val.cuda()\n",
    "\n",
    "loss_all, loss_val_all = [], []\n",
    "for epoch in range(2001):\n",
    "    for i, (mix, H) in enumerate(tr):\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        mix, H = mix.cuda(), H.cuda()\n",
    "        for j in range(3): # recursive for each source\n",
    "            if j == 0:\n",
    "                Rx = mix[...,None] @ mix[:,:,:,None,:].conj()\n",
    "                rx = Rx.mean(dim=(1,2))\n",
    "            else:\n",
    "                w = rx_inv_hat@hhat[...,None] / \\\n",
    "                        (hhat[:,None,:].conj()@rx_inv_hat@hhat[...,None])\n",
    "                p = Is - hhat[...,None]@w.permute(0,2,1).conj()\n",
    "                rx = p@rx@p.permute(0,2,1).conj()\n",
    "\n",
    "            inp = torch.stack((rx.real, rx.imag), dim=1)[:,:,ind[0], ind[1]].reshape(btsize, -1)\n",
    "            hhat, rx12 = model(inp)\n",
    "            rx_inv_hat = lower2matrix(rx12)\n",
    "            loss = loss + ((Is-rx_inv_hat@rx).abs()**2).mean() + \\\n",
    "                lamb*((H[:,:,j]-hhat).abs()**2).mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        if i % 30 == 0:\n",
    "            loss_all.append(loss.detach().cpu().item())\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_all, '-x')\n",
    "        plt.title(f'epoch {epoch}')\n",
    "        plt.show()\n",
    "\n",
    "        if epoch >50:\n",
    "            plt.figure()\n",
    "            plt.plot(loss_all[-100:], '-rx')\n",
    "            plt.title(f'epoch {epoch}')\n",
    "            plt.show()\n",
    "\n",
    "        \"Validation\"\n",
    "        with torch.no_grad():\n",
    "            loss_val = 0\n",
    "            for j in range(3): # recursive for each source\n",
    "                if j ==0 :\n",
    "                    rx = rx_val_cuda\n",
    "                else:\n",
    "                    w = rx_inv_hat_val@hhat_val[...,None] / \\\n",
    "                            (hhat_val[:,None,:].conj()@rx_inv_hat_val@hhat_val[...,None])\n",
    "                    p = Is2 - hhat_val[...,None]@w.permute(0,2,1).conj()\n",
    "                    rx = p@rx@p.permute(0,2,1).conj()\n",
    "\n",
    "                rl = rx.reshape(200, -1)[:,indx] # take lower triangle\n",
    "                inp = torch.stack((rl.real, rl.imag), dim=1).reshape(200, -1)\n",
    "                hhat_val, rx12 = model(inp)\n",
    "                rx_inv_hat_val = lower2matrix(rx12)\n",
    "\n",
    "                loss_val = loss_val + ((Is2-rx_inv_hat_val@rx).abs()**2).mean() + \\\n",
    "                    lamb*((h0[:,:,j]-hhat_val).abs()**2).mean()\n",
    "\n",
    "            loss_val_all.append(loss_val.cpu().item())\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all, '-x')\n",
    "            plt.title(f'val loss at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_validation_loss')\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all[-50:], '-rx')\n",
    "            plt.title(f'last 50 val loss epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_50val_at_epoch{epoch}')\n",
    "            plt.close('all')   \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v19\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "rid = 'v19'\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "\n",
    "#%% generate data\n",
    "if True:\n",
    "    M, n_data= 3, int(1.2e4)\n",
    "    dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "    v0 = dicts['v'][..., 0]\n",
    "    v1 = dicts['v'][..., 1]\n",
    "    v2 = dicts['v'][..., 2]\n",
    "    from skimage.transform import resize\n",
    "    v0 = torch.tensor(resize(v0, (100, 100), preserve_range=True))\n",
    "    v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "    plt.imshow(v0.abs())\n",
    "    plt.colorbar()\n",
    "    v1 = torch.tensor(resize(v1, (100, 100), preserve_range=True))\n",
    "    v1 = awgn(v1, snr=30, seed=1).abs().to(torch.cfloat)\n",
    "    v2 = torch.tensor(resize(v2, (100, 100), preserve_range=True))\n",
    "    v2 = awgn(v2, snr=30, seed=2).abs().to(torch.cfloat)\n",
    "\n",
    "    snr=0\n",
    "    delta = v0.mean()*10**(-snr/10)\n",
    "    angs = (torch.rand(n_data,1)*20 +10)/180*np.pi  # signal aoa [10, 30]\n",
    "    H = (1j*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    angs_n1 = (torch.rand(n_data,1)*20 -70)/180*np.pi  # noise aoa [-70, -50]\n",
    "    hs_n1 = (1j*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    angs_n2 = (torch.rand(n_data,1)*20 +120)/180*np.pi  # noise aoa [120, 140]\n",
    "    hs_n2 = (1j*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    signal = (H[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:])\n",
    "    n1 = (hs_n1[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:])\n",
    "    n2 = (hs_n2[..., None]@(torch.randn(v2.shape, dtype=torch.cfloat)*(v2**0.5)).flatten()[None,:])\n",
    "    mix =  (signal + n1 + n2).reshape(n_data, M, 100, 100)   \n",
    "    mix_all, sig_all = mix.permute(0,2,3,1), signal.reshape(n_data, M, 100, 100).permute(0,2,3,1)\n",
    "    mixn = awgn_batch(mix_all)\n",
    "    H_all = torch.stack((H, hs_n1, hs_n2), dim=-1)\n",
    "\n",
    "    # torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "    plt.figure()\n",
    "    plt.imshow(mix_all[0,:,:,0].abs())\n",
    "    plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(10000, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "\n",
    "#%% load data and model\n",
    "class DOA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mainnet = nn.Sequential(\n",
    "            nn.Linear(12,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64,1)\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        m = self.mainnet(x)\n",
    "        ang = self.hnet(m)\n",
    "        rx12 = self.rxnet(m)\n",
    "        ch = torch.pi*torch.arange(3, device=x.device)\n",
    "        hj = ((ang.tanh() @ ch[None,:])*1j).exp() # shape:[I, M]\n",
    "        return hj, rx12 \n",
    "\n",
    "def lower2matrix(rx12):\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[0,0], rx_inv_hat[1,1], rx_inv_hat[2,2] = \\\n",
    "        rx_inv_hat[0,0]/2, rx_inv_hat[1,1]/2, rx_inv_hat[2,2]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "model = DOA().cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "#%%\n",
    "M, btsize, n_tr = 3, 64, int(1e4)\n",
    "lamb = 10\n",
    "const_range = torch.arange(M).to(torch.cfloat)[None,:].cuda()\n",
    "data = Data.TensorDataset(mix_all[:n_tr], H_all[:n_tr])\n",
    "tr = Data.DataLoader(data, batch_size=btsize, drop_last=True, shuffle=True)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= 1e-4,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "\"Pre calc constants\"\n",
    "Is = torch.stack([torch.eye(3,3)]*btsize, dim=0).to(torch.cfloat).cuda()\n",
    "Is2 = torch.stack([torch.eye(3,3)]*200, dim=0).to(torch.cfloat).cuda()\n",
    "ind = torch.tril_indices(3,3)\n",
    "indx = ind[0]*3+ind[1]\n",
    "\n",
    "\"validation\"\n",
    "val0, h0 = mix_all[n_tr:n_tr+200], H_all[n_tr:n_tr+200].cuda()\n",
    "rx_val = (val0[...,None] @ val0[:,:,:,None,:].conj()).mean(dim=(1,2))\n",
    "rx_val_cuda = rx_val.cuda()\n",
    "\n",
    "loss_all, loss_val_all = [], []\n",
    "for epoch in range(2001):\n",
    "    for i, (mix, H) in enumerate(tr):\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        mix, H = mix.cuda(), H.cuda()\n",
    "        for j in range(3): # recursive for each source\n",
    "            if j == 0:\n",
    "                Rx = mix[...,None] @ mix[:,:,:,None,:].conj()\n",
    "                rx = Rx.mean(dim=(1,2))\n",
    "            else:\n",
    "                w = rx_inv_hat@hhat[...,None] / \\\n",
    "                        (hhat[:,None,:].conj()@rx_inv_hat@hhat[...,None])\n",
    "                p = Is - hhat[...,None]@w.permute(0,2,1).conj()\n",
    "                rx = p@rx@p.permute(0,2,1).conj()\n",
    "\n",
    "            inp = torch.stack((rx.real, rx.imag), dim=1)[:,:,ind[0], ind[1]].reshape(btsize, -1)\n",
    "            hhat, rx12 = model(inp)\n",
    "            rx_inv_hat = lower2matrix(rx12)\n",
    "            loss = loss + ((Is-rx_inv_hat@rx).abs()**2).mean() + \\\n",
    "                lamb*((H[:,:,j]-hhat).abs()**2).mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        if i % 30 == 0:\n",
    "            loss_all.append(loss.detach().cpu().item())\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_all, '-x')\n",
    "        plt.title(f'epoch {epoch}')\n",
    "        plt.show()\n",
    "\n",
    "        if epoch >50:\n",
    "            plt.figure()\n",
    "            plt.plot(loss_all[-100:], '-rx')\n",
    "            plt.title(f'epoch {epoch}')\n",
    "            plt.show()\n",
    "\n",
    "        \"Validation\"\n",
    "        with torch.no_grad():\n",
    "            loss_val = 0\n",
    "            for j in range(3): # recursive for each source\n",
    "                if j ==0 :\n",
    "                    rx = rx_val_cuda\n",
    "                else:\n",
    "                    w = rx_inv_hat_val@hhat_val[...,None] / \\\n",
    "                            (hhat_val[:,None,:].conj()@rx_inv_hat_val@hhat_val[...,None])\n",
    "                    p = Is2 - hhat_val[...,None]@w.permute(0,2,1).conj()\n",
    "                    rx = p@rx@p.permute(0,2,1).conj()\n",
    "\n",
    "                rl = rx.reshape(200, -1)[:,indx] # take lower triangle\n",
    "                inp = torch.stack((rl.real, rl.imag), dim=1).reshape(200, -1)\n",
    "                hhat_val, rx12 = model(inp)\n",
    "                rx_inv_hat_val = lower2matrix(rx12)\n",
    "\n",
    "                loss_val = loss_val + ((Is2-rx_inv_hat_val@rx).abs()**2).mean() + \\\n",
    "                    lamb*((h0[:,:,j]-hhat_val).abs()**2).mean()\n",
    "\n",
    "            loss_val_all.append(loss_val.cpu().item())\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all, '-x')\n",
    "            plt.title(f'val loss at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_validation_loss')\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all[-50:], '-rx')\n",
    "            plt.title(f'last 50 val loss epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_50val_at_epoch{epoch}')\n",
    "            plt.close('all')   \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v20\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "rid = 'v20'\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "\n",
    "#%% generate data\n",
    "if True:\n",
    "    M, n_data= 3, int(1.2e4)\n",
    "    dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "    v0 = dicts['v'][..., 0]\n",
    "    v1 = dicts['v'][..., 1]\n",
    "    v2 = dicts['v'][..., 2]\n",
    "    from skimage.transform import resize\n",
    "    v0 = torch.tensor(resize(v0, (100, 100), preserve_range=True))\n",
    "    v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "    plt.imshow(v0.abs())\n",
    "    plt.colorbar()\n",
    "    v1 = torch.tensor(resize(v1, (100, 100), preserve_range=True))\n",
    "    v1 = awgn(v1, snr=30, seed=1).abs().to(torch.cfloat)\n",
    "    v2 = torch.tensor(resize(v2, (100, 100), preserve_range=True))\n",
    "    v2 = awgn(v2, snr=30, seed=2).abs().to(torch.cfloat)\n",
    "\n",
    "    snr=0\n",
    "    delta = v0.mean()*10**(-snr/10)\n",
    "    angs = (torch.rand(n_data,1)*20 +10)/180*np.pi  # signal aoa [10, 30]\n",
    "    H = (1j*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    angs_n1 = (torch.rand(n_data,1)*20 -70)/180*np.pi  # noise aoa [-70, -50]\n",
    "    hs_n1 = (1j*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    angs_n2 = (torch.rand(n_data,1)*20 +120)/180*np.pi  # noise aoa [120, 140]\n",
    "    hs_n2 = (1j*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    signal = (H[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:])\n",
    "    n1 = (hs_n1[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:])\n",
    "    n2 = (hs_n2[..., None]@(torch.randn(v2.shape, dtype=torch.cfloat)*(v2**0.5)).flatten()[None,:])\n",
    "    mix =  (signal + n1 + n2).reshape(n_data, M, 100, 100)   \n",
    "    mix_all, sig_all = mix.permute(0,2,3,1), signal.reshape(n_data, M, 100, 100).permute(0,2,3,1)\n",
    "    mixn = awgn_batch(mix_all)\n",
    "    H_all = torch.stack((H, hs_n1, hs_n2), dim=-1)\n",
    "\n",
    "    # torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "    plt.figure()\n",
    "    plt.imshow(mix_all[0,:,:,0].abs())\n",
    "    plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(10000, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "\n",
    "#%% load data and model\n",
    "class DOA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mainnet = nn.Sequential(\n",
    "            nn.Linear(12,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64,1)\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        m = self.mainnet(x)\n",
    "        ang = self.hnet(m)\n",
    "        rx12 = self.rxnet(m)\n",
    "        ch = torch.pi*torch.arange(3, device=x.device)\n",
    "        hj = ((ang.tanh() @ ch[None,:])*1j).exp() # shape:[I, M]\n",
    "        return hj, rx12 \n",
    "\n",
    "def lower2matrix(rx12):\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[0,0], rx_inv_hat[1,1], rx_inv_hat[2,2] = \\\n",
    "        rx_inv_hat[0,0]/2, rx_inv_hat[1,1]/2, rx_inv_hat[2,2]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "model = DOA().cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "#%%\n",
    "M, btsize, n_tr = 3, 64, int(1e4)\n",
    "lamb = 1\n",
    "const_range = torch.arange(M).to(torch.cfloat)[None,:].cuda()\n",
    "data = Data.TensorDataset(mix_all[:n_tr], H_all[:n_tr])\n",
    "tr = Data.DataLoader(data, batch_size=btsize, drop_last=True, shuffle=True)\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= 1e-4,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "\"Pre calc constants\"\n",
    "Is = torch.stack([torch.eye(3,3)]*btsize, dim=0).to(torch.cfloat).cuda()\n",
    "Is2 = torch.stack([torch.eye(3,3)]*200, dim=0).to(torch.cfloat).cuda()\n",
    "ind = torch.tril_indices(3,3)\n",
    "indx = ind[0]*3+ind[1]\n",
    "\n",
    "\"validation\"\n",
    "val0, h0 = mix_all[n_tr:n_tr+200], H_all[n_tr:n_tr+200].cuda()\n",
    "rx_val = (val0[...,None] @ val0[:,:,:,None,:].conj()).mean(dim=(1,2))\n",
    "rx_val_cuda = rx_val.cuda()\n",
    "\n",
    "loss_all, loss_val_all = [], []\n",
    "for epoch in range(2001):\n",
    "    for i, (mix, H) in enumerate(tr):\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        mix, H = mix.cuda(), H.cuda()\n",
    "        for j in range(3): # recursive for each source\n",
    "            if j == 0:\n",
    "                Rx = mix[...,None] @ mix[:,:,:,None,:].conj()\n",
    "                rx = Rx.mean(dim=(1,2))\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    w = rx_inv_hat@hhat[...,None] / \\\n",
    "                            (hhat[:,None,:].conj()@rx_inv_hat@hhat[...,None])\n",
    "                    p = Is - hhat[...,None]@w.permute(0,2,1).conj()\n",
    "                    rx = p@rx@p.permute(0,2,1).conj()\n",
    "\n",
    "            inp = torch.stack((rx.real, rx.imag), dim=1)[:,:,ind[0], ind[1]].reshape(btsize, -1)\n",
    "            hhat, rx12 = model(inp)\n",
    "            rx_inv_hat = lower2matrix(rx12)\n",
    "            loss = loss + ((Is-rx_inv_hat@rx).abs()**2).mean() + \\\n",
    "                lamb*((H[:,:,j]-hhat).abs()**2).mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        if i % 30 == 0:\n",
    "            loss_all.append(loss.detach().cpu().item())\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_all, '-x')\n",
    "        plt.title(f'epoch {epoch}')\n",
    "        plt.show()\n",
    "\n",
    "        if epoch >50:\n",
    "            plt.figure()\n",
    "            plt.plot(loss_all[-100:], '-rx')\n",
    "            plt.title(f'epoch {epoch}')\n",
    "            plt.show()\n",
    "\n",
    "        \"Validation\"\n",
    "        with torch.no_grad():\n",
    "            loss_val = 0\n",
    "            for j in range(3): # recursive for each source\n",
    "                if j ==0 :\n",
    "                    rx = rx_val_cuda\n",
    "                else:\n",
    "                    w = rx_inv_hat_val@hhat_val[...,None] / \\\n",
    "                            (hhat_val[:,None,:].conj()@rx_inv_hat_val@hhat_val[...,None])\n",
    "                    p = Is2 - hhat_val[...,None]@w.permute(0,2,1).conj()\n",
    "                    rx = p@rx@p.permute(0,2,1).conj()\n",
    "\n",
    "                rl = rx.reshape(200, -1)[:,indx] # take lower triangle\n",
    "                inp = torch.stack((rl.real, rl.imag), dim=1).reshape(200, -1)\n",
    "                hhat_val, rx12 = model(inp)\n",
    "                rx_inv_hat_val = lower2matrix(rx12)\n",
    "\n",
    "                loss_val = loss_val + ((Is2-rx_inv_hat_val@rx).abs()**2).mean() + \\\n",
    "                    lamb*((h0[:,:,j]-hhat_val).abs()**2).mean()\n",
    "\n",
    "            loss_val_all.append(loss_val.cpu().item())\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all, '-x')\n",
    "            plt.title(f'val loss at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_validation_loss')\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_val_all[-50:], '-rx')\n",
    "            plt.title(f'last 50 val loss epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'last_50val_at_epoch{epoch}')\n",
    "            plt.close('all')   \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
