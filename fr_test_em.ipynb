{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 3-class EM -- without noise\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "#%%\n",
    "d, s, h = torch.load('../data/nem_ss/val1kM3FT64_xsh_data5.pt')\n",
    "N, F = s.shape[-1], s.shape[-2] # h is M*J matrix, here 6*6\n",
    "ratio = d.abs().amax(dim=(1,2,3))\n",
    "x_all = (d/ratio[:,None,None,None]).permute(0,2,3,1)\n",
    "s_all = s.abs().permute(0,2,3,1) \n",
    "\n",
    "class EM:\n",
    "    def calc_ll(self, x, vhat, Rj):\n",
    "        \"\"\" Rj shape of [J, M, M]\n",
    "            vhat shape of [N, F, J]\n",
    "            x shape of [N, F, M]\n",
    "        \"\"\"\n",
    "        Rcj = vhat[...,None, None]*Rj[:,None,None] #shape of[J,N,F,M,M]\n",
    "        Rx = Rcj.sum(0) #shape of[N,F,M,M]\n",
    "        l = -(np.pi*Rx.det()).log() - (x[..., None, :].conj()@Rx.inverse()@x[..., None]).squeeze()\n",
    "        return Rcj, Rx, l.sum()\n",
    "\n",
    "    def rand_init(self, x, J=6, Hscale=1, Rbscale=100, seed=0):\n",
    "        N, F, M = x.shape\n",
    "        torch.torch.manual_seed(seed)\n",
    "        dtype = x.dtype\n",
    "\n",
    "        vhat = torch.randn(N, F, J).abs().to(dtype)\n",
    "        Hhat = torch.randn(M, J, dtype=dtype)*Hscale\n",
    "        Rb = torch.eye(M).to(dtype)*Rbscale\n",
    "        Rj = torch.zeros(J, M, M).to(dtype)\n",
    "        return vhat, Hhat, Rb, Rj\n",
    "\n",
    "    def cluster_init(self, x, J=3, K=60, init=1, Rbscale=1e-3, showfig=False):\n",
    "        \"\"\"psudo code, https://www.saedsayad.com/clustering_hierarchical.htm\n",
    "        Given : A set X of obejects{x1,...,xn}\n",
    "                A cluster distance function dist(c1, c2)\n",
    "        for i=1 to n\n",
    "            ci = {xi}\n",
    "        end for\n",
    "        C = {c1, ..., cn}\n",
    "        I = n+1\n",
    "        While I>1 do\n",
    "            (cmin1, cmin2) = minimum dist(ci, cj) for all ci, cj in C\n",
    "            remove cmin1 and cmin2 from C\n",
    "            add {cmin1, cmin2} to C\n",
    "            I = I - 1\n",
    "        end while\n",
    "\n",
    "        However, this naive algorithm does not fit large samples. \n",
    "        Here we use scipy function for the linkage.\n",
    "        J is how many clusters\n",
    "        \"\"\"   \n",
    "        dtype = x.dtype\n",
    "        N, F, M = x.shape\n",
    "\n",
    "        \"get data and clusters ready\"\n",
    "        x_norm = ((x[:,:,None,:]@x[..., None].conj())**0.5)[:,:,0]\n",
    "        if init==1: x_ = x/x_norm * (-1j*x[...,0:1].angle()).exp() # shape of [N, F, M] x_bar\n",
    "        else: x_ = x * (-1j*x[...,0:1].angle()).exp() # the x_tilde in Duong's paper\n",
    "        data = x_.reshape(N*F, M)\n",
    "        I = data.shape[0]\n",
    "        C = [[i] for i in range(I)]  # initial cluster\n",
    "\n",
    "        \"calc. affinity matrix and linkage\"\n",
    "        perms = torch.combinations(torch.arange(len(C)))\n",
    "        d = data[perms]\n",
    "        table = ((d[:,0] - d[:,1]).abs()**2).sum(dim=-1)**0.5\n",
    "        from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "        z = linkage(table, method='average')\n",
    "        if showfig: dn = dendrogram(z, p=3, truncate_mode='level')\n",
    "\n",
    "        \"find the max J cluster and sample index\"\n",
    "        zind = torch.tensor(z).to(torch.int)\n",
    "        flag = torch.cat((torch.ones(I), torch.zeros(I)))\n",
    "        c = C + [[] for i in range(I)]\n",
    "        for i in range(z.shape[0]-K): # threshold of K level to stop\n",
    "            c[i+I] = c[zind[i][0]] + c[zind[i][1]]\n",
    "            flag[i+I], flag[zind[i][0]], flag[zind[i][1]] = 1, 0, 0\n",
    "        ind = (flag == 1).nonzero(as_tuple=True)[0]\n",
    "        dict_c = {}  # which_cluster: how_many_nodes\n",
    "        for i in range(ind.shape[0]):\n",
    "            dict_c[ind[i].item()] = len(c[ind[i]])\n",
    "        dict_c_sorted = {k:v for k,v in sorted(dict_c.items(), key=lambda x: -x[1])}\n",
    "        cs = []\n",
    "        for i, (k,v) in enumerate(dict_c_sorted.items()):\n",
    "            if i == J:\n",
    "                break\n",
    "            cs.append(c[k])\n",
    "\n",
    "        \"initil the EM variables\"\n",
    "        Hhat = torch.rand(M, J, dtype=dtype)\n",
    "        Rj = torch.rand(J, M, M, dtype=dtype)\n",
    "        for i in range(J):\n",
    "            d = data[torch.tensor(cs[i])] # shape of [I_cj, M]\n",
    "            Hhat[:,i] = d.mean(0)\n",
    "            Rj[i] = (d[..., None] @ d[:,None,:].conj()).mean(0)\n",
    "        vhat = torch.ones(J, N, F).abs().to(dtype)\n",
    "        Rb = torch.eye(M).to(dtype)*Rbscale\n",
    "\n",
    "        return vhat, Hhat, Rb, Rj\n",
    "\n",
    "    def em_func_(self, x, J=3, max_iter=501, lamb=0, thresh_K=60, init=1):\n",
    "        \"\"\"init=0: random\n",
    "            init=1: x_bar original\n",
    "            init=else: x_tilde duong's paper\n",
    "        \"\"\"\n",
    "        #  EM algorithm for one complex sample\n",
    "        N, F, M = x.shape\n",
    "        eye = torch.eye(M).to(x.dtype)\n",
    "        if init == 0: # random init\n",
    "            vhat, Hhat, Rb, Rj = self.rand_init(x, J=J)\n",
    "        elif init == 1: #hierarchical initialization -- x_bar\n",
    "            vhat, Hhat, Rb, Rj = self.cluster_init(x, J=J, K=thresh_K, init=init)\n",
    "        else:  #hierarchical initialization -- x_tilde\n",
    "            vhat, Hhat, Rb, Rj = self.cluster_init(x, J=J, K=thresh_K, init=init)\n",
    "        \n",
    "        Rcj, Rx, ll = self.calc_ll(x, vhat, Rj)\n",
    "        ll_traj = []\n",
    "        for i in range(max_iter):\n",
    "            \"E-step\"\n",
    "            W = Rcj @ Rx.inverse() #shape of[J,N,F,M,M]\n",
    "            chat = W @ x[...,None] #shape of[J,N,F,M,1]\n",
    "            Rcjhat = chat @ chat.transpose(-1,-2).conj() + (eye- W)@Rcj #shape of[J,N,F,M,M]\n",
    "\n",
    "            \"M-step\"\n",
    "            vhat = (Rj.inverse()[:,None,None]@Rcjhat).diagonal(dim1=-1, dim2=-2).mean(-1)\n",
    "            vhat.real = threshold(vhat.real, floor=1e-10, ceiling=10)\n",
    "            vhat.imag = vhat.imag - vhat.imag\n",
    "            Rj = (Rcjhat/vhat[...,None, None]).mean(dim=(1,2)) #shape of[J,M,M]\n",
    "\n",
    "            \"compute log-likelyhood\"\n",
    "            Rcj, Rx, ll = self.calc_ll(x, vhat, Rj)\n",
    "            ll_traj.append(ll.item())\n",
    "            if i > 30 and abs((ll_traj[i] - ll_traj[i-3])/ll_traj[i-3]) <1e-4:\n",
    "                print(f'EM early stop at iter {i}')\n",
    "                break\n",
    "\n",
    "        return chat, vhat, ll_traj, torch.linalg.matrix_rank(Rcj).double().mean()\n",
    "\n",
    "#%%\n",
    "EMs, EMh = [], []\n",
    "for snr in ['inf', 20, 10, 5, 0]:\n",
    "    ems, emh = [], []\n",
    "    for ind in range(1000):\n",
    "        if snr != 'inf':\n",
    "            data = awgn(x_all[ind], snr)\n",
    "        else:\n",
    "            data = x_all[ind]\n",
    "        try:\n",
    "            chat, vhat, ll_traj, rank = \\\n",
    "                    EM().em_func_(data, J=3, max_iter=301, thresh_K=10, init=1)\n",
    "            shat = chat.permute(3,4,1,2,0).abs()[0]\n",
    "            hhat = chat[...,0].permute(1,2,3,0).mean(dim=(0,1))\n",
    "            temp_s = s_corr_cuda(shat, s_all[ind:ind+1].abs()).item()\n",
    "            temp = h_corr(hhat, h[ind])\n",
    "            if ind %20 == 0 :\n",
    "                print(f'At epoch {ind}', ' h corr: ', temp, ' s corr:', temp_s)\n",
    "\n",
    "            ems.append(temp_s)\n",
    "            emh.append(temp)\n",
    "        except:\n",
    "            print(f\"An exception occurred {ind}\")\n",
    "    EMs.append(sum(ems)/len(ems))\n",
    "    EMh.append(sum(emh)/len(emh))\n",
    "\n",
    "    print(f'done with one snr {snr}')\n",
    "    print('EMs, EMh', EMs, EMh)\n",
    "\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 3-class EM\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "#%%\n",
    "d, s, h = torch.load('../data/nem_ss/val1kM6FT64_xsh_data5.pt')\n",
    "N, F = s.shape[-1], s.shape[-2] # h is M*J matrix, here 6*6\n",
    "ratio = d.abs().amax(dim=(1,2,3))\n",
    "x_all = (d/ratio[:,None,None,None]).permute(0,2,3,1)\n",
    "s_all = s.abs().permute(0,2,3,1) \n",
    "\n",
    "class EM:\n",
    "    def calc_ll(self, x, vhat, Rj, Rn):\n",
    "        \"\"\" Rj shape of [J, M, M]\n",
    "            vhat shape of [N, F, J]\n",
    "            x shape of [N, F, M]\n",
    "        \"\"\"\n",
    "        Rcj = vhat[...,None, None]*Rj[:,None,None]  #shape of[J,N,F,M,M]\n",
    "        Rx = Rcj.sum(0) + Rn #shape of[N,F,M,M]\n",
    "        l = -(np.pi*Rx.det()).log() - (x[..., None, :].conj()@Rx.inverse()@x[..., None]).squeeze()\n",
    "        return Rcj, Rx, l.sum()\n",
    "\n",
    "    def rand_init(self, x, J=6, Hscale=1, Rbscale=100, seed=0):\n",
    "        N, F, M = x.shape\n",
    "        torch.torch.manual_seed(seed)\n",
    "        dtype = x.dtype\n",
    "\n",
    "        vhat = torch.randn(N, F, J).abs().to(dtype)\n",
    "        Hhat = torch.randn(M, J, dtype=dtype)*Hscale\n",
    "        Rb = torch.eye(M).to(dtype)*Rbscale\n",
    "        Rj = torch.zeros(J, M, M).to(dtype)\n",
    "        return vhat, Hhat, Rb, Rj\n",
    "\n",
    "    def cluster_init(self, x, J=3, K=60, init=1, Rbscale=1e-3, showfig=False):\n",
    "        \"\"\"psudo code, https://www.saedsayad.com/clustering_hierarchical.htm\n",
    "        Given : A set X of obejects{x1,...,xn}\n",
    "                A cluster distance function dist(c1, c2)\n",
    "        for i=1 to n\n",
    "            ci = {xi}\n",
    "        end for\n",
    "        C = {c1, ..., cn}\n",
    "        I = n+1\n",
    "        While I>1 do\n",
    "            (cmin1, cmin2) = minimum dist(ci, cj) for all ci, cj in C\n",
    "            remove cmin1 and cmin2 from C\n",
    "            add {cmin1, cmin2} to C\n",
    "            I = I - 1\n",
    "        end while\n",
    "\n",
    "        However, this naive algorithm does not fit large samples. \n",
    "        Here we use scipy function for the linkage.\n",
    "        J is how many clusters\n",
    "        \"\"\"   \n",
    "        dtype = x.dtype\n",
    "        N, F, M = x.shape\n",
    "\n",
    "        \"get data and clusters ready\"\n",
    "        x_norm = ((x[:,:,None,:]@x[..., None].conj())**0.5)[:,:,0]\n",
    "        if init==1: x_ = x/x_norm * (-1j*x[...,0:1].angle()).exp() # shape of [N, F, M] x_bar\n",
    "        else: x_ = x * (-1j*x[...,0:1].angle()).exp() # the x_tilde in Duong's paper\n",
    "        data = x_.reshape(N*F, M)\n",
    "        I = data.shape[0]\n",
    "        C = [[i] for i in range(I)]  # initial cluster\n",
    "\n",
    "        \"calc. affinity matrix and linkage\"\n",
    "        perms = torch.combinations(torch.arange(len(C)))\n",
    "        d = data[perms]\n",
    "        table = ((d[:,0] - d[:,1]).abs()**2).sum(dim=-1)**0.5\n",
    "        from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "        z = linkage(table, method='average')\n",
    "        if showfig: dn = dendrogram(z, p=3, truncate_mode='level')\n",
    "\n",
    "        \"find the max J cluster and sample index\"\n",
    "        zind = torch.tensor(z).to(torch.int)\n",
    "        flag = torch.cat((torch.ones(I), torch.zeros(I)))\n",
    "        c = C + [[] for i in range(I)]\n",
    "        for i in range(z.shape[0]-K): # threshold of K level to stop\n",
    "            c[i+I] = c[zind[i][0]] + c[zind[i][1]]\n",
    "            flag[i+I], flag[zind[i][0]], flag[zind[i][1]] = 1, 0, 0\n",
    "        ind = (flag == 1).nonzero(as_tuple=True)[0]\n",
    "        dict_c = {}  # which_cluster: how_many_nodes\n",
    "        for i in range(ind.shape[0]):\n",
    "            dict_c[ind[i].item()] = len(c[ind[i]])\n",
    "        dict_c_sorted = {k:v for k,v in sorted(dict_c.items(), key=lambda x: -x[1])}\n",
    "        cs = []\n",
    "        for i, (k,v) in enumerate(dict_c_sorted.items()):\n",
    "            if i == J:\n",
    "                break\n",
    "            cs.append(c[k])\n",
    "\n",
    "        \"initil the EM variables\"\n",
    "        Hhat = torch.rand(M, J, dtype=dtype)\n",
    "        Rj = torch.rand(J, M, M, dtype=dtype)\n",
    "        for i in range(J):\n",
    "            d = data[torch.tensor(cs[i])] # shape of [I_cj, M]\n",
    "            Hhat[:,i] = d.mean(0)\n",
    "            Rj[i] = (d[..., None] @ d[:,None,:].conj()).mean(0)\n",
    "        vhat = torch.ones(J, N, F).abs().to(dtype)\n",
    "        Rb = torch.eye(M).to(dtype)*Rbscale\n",
    "\n",
    "        return vhat, Hhat, Rb, Rj\n",
    "\n",
    "    def em_func_(self, x, J=3, max_iter=501, lamb=0, thresh_K=60, init=1):\n",
    "        \"\"\"init=0: random\n",
    "            init=1: x_bar original\n",
    "            init=else: x_tilde duong's paper\n",
    "        \"\"\"\n",
    "        #  EM algorithm for one complex sample\n",
    "        N, F, M = x.shape\n",
    "        eye = torch.eye(M).to(x.dtype)\n",
    "        if init == 0: # random init\n",
    "            vhat, Hhat, Rb, Rj = self.rand_init(x, J=J)\n",
    "        elif init == 1: #hierarchical initialization -- x_bar\n",
    "            vhat, Hhat, Rb, Rj = self.cluster_init(x, J=J, K=thresh_K, init=init)\n",
    "        else:  #hierarchical initialization -- x_tilde\n",
    "            vhat, Hhat, Rb, Rj = self.cluster_init(x, J=J, K=thresh_K, init=init)\n",
    "        \n",
    "        Rcj, Rx, ll = self.calc_ll(x, vhat, Rj, Rb)\n",
    "        xxh = x[...,None]@x[:,:,None].conj() #shape of [N,F,M,M]\n",
    "        ll_traj = []\n",
    "        for i in range(max_iter):\n",
    "            \"E-step\"\n",
    "            Rx_inv = Rx.inverse()\n",
    "            W = Rcj @ Rx_inv #shape of[J,N,F,M,M]\n",
    "            chat = W @ x[...,None] #shape of[J,N,F,M,1]\n",
    "            Rcjhat = chat @ chat.transpose(-1,-2).conj() + (eye- W)@Rcj #shape of[J,N,F,M,M]\n",
    "            Rc = Rcj.sum(0, keepdim=True) #shape of[1,N,F,M,M]\n",
    "            ch = chat.sum(0, keepdim=True) #shape of[1,N,F,M,1]\n",
    "            RcRx_inv = Rc@Rx_inv\n",
    "            Rnhat = xxh - RcRx_inv@xxh - xxh@(RcRx_inv.transpose(-1,-2).conj()) \\\n",
    "               + (eye-RcRx_inv)@Rc + ch@ch.transpose(-1, -2).conj() #shape of [N,F,M,M]\n",
    "\n",
    "            \"M-step\"\n",
    "            vhat = (Rj.inverse()[:,None,None]@Rcjhat).diagonal(dim1=-1, dim2=-2).mean(-1)\n",
    "            vhat.real = threshold(vhat.real, floor=1e-10, ceiling=10)\n",
    "            vhat.imag = vhat.imag - vhat.imag\n",
    "            Rj = (Rcjhat/vhat[...,None, None]).mean(dim=(1,2)) #shape of[J,M,M]\n",
    "            delta = threshold(1/(N*F*M)*Rnhat.diagonal(0,-1,-2).sum().real) #shape of [1]\n",
    "\n",
    "            Rn = delta*torch.eye(M, device=Rj.device)\n",
    "            # Rn = threshold(Rnhat.diagonal(0,-1,-2).mean(dim=(0,1)).real).diag()\n",
    "\n",
    "            \"compute log-likelyhood\"\n",
    "            Rcj, Rx, ll = self.calc_ll(x, vhat, Rj, Rn)\n",
    "            ll_traj.append(ll.item())\n",
    "            if i > 30 and abs((ll_traj[i] - ll_traj[i-3])/ll_traj[i-3]) <1e-4:\n",
    "                print(f'EM early stop at iter {i}')\n",
    "                break\n",
    "\n",
    "        return chat, vhat, ll_traj, Rn\n",
    "\n",
    "#%%\n",
    "EMs, EMh = [], []\n",
    "for snr in ['inf',10, 0]:\n",
    "    ems, emh = [], []\n",
    "    for ind in range(5):\n",
    "        if snr != 'inf':\n",
    "            data = awgn(x_all[ind], snr)\n",
    "        else:\n",
    "            data = x_all[ind]\n",
    "        # try\n",
    "        chat, vhat, ll_traj, Rn = \\\n",
    "                EM().em_func_(data, J=3, max_iter=301, thresh_K=10, init=1)\n",
    "        print(Rn)\n",
    "        shat = chat.permute(3,4,1,2,0).abs()[0]\n",
    "        hhat = chat[...,0].permute(1,2,3,0).mean(dim=(0,1))\n",
    "        temp_s = s_corr_cuda(shat, s_all[ind:ind+1].abs()).item()\n",
    "        temp = h_corr(hhat, h[ind])\n",
    "        if ind %20 == 0 :\n",
    "            print(f'At epoch {ind}', ' h corr: ', temp, ' s corr:', temp_s)\n",
    "\n",
    "        ems.append(temp_s)\n",
    "        emh.append(temp)\n",
    "        # except:\n",
    "        #     print(f\"An exception occurred {ind}\")\n",
    "    EMs.append(sum(ems)/len(ems))\n",
    "    EMh.append(sum(emh)/len(emh))\n",
    "\n",
    "    print(f'done with one snr {snr}')\n",
    "    print('EMs, EMh', EMs, EMh)\n",
    "\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 6-class EM --without noise term\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "#%%\n",
    "d, s, h = torch.load('../data/nem_ss/val1kM6FT64_xsh_data5.pt')\n",
    "N, F = s.shape[-1], s.shape[-2] # h is M*J matrix, here 6*6\n",
    "ratio = d.abs().amax(dim=(1,2,3))\n",
    "x_all = (d/ratio[:,None,None,None]).permute(0,2,3,1)\n",
    "s_all = s.abs().permute(0,2,3,1) \n",
    "\n",
    "class EM:\n",
    "    def calc_ll(self, x, vhat, Rj):\n",
    "        \"\"\" Rj shape of [J, M, M]\n",
    "            vhat shape of [N, F, J]\n",
    "            x shape of [N, F, M]\n",
    "        \"\"\"\n",
    "        Rcj = vhat[...,None, None]*Rj[:,None,None] #shape of[J,N,F,M,M]\n",
    "        Rx = Rcj.sum(0) #shape of[N,F,M,M]\n",
    "        l = -(np.pi*Rx.det()).log() - (x[..., None, :].conj()@Rx.inverse()@x[..., None]).squeeze()\n",
    "        return Rcj, Rx, l.sum()\n",
    "\n",
    "    def rand_init(self, x, J=6, Hscale=1, Rbscale=100, seed=0):\n",
    "        N, F, M = x.shape\n",
    "        torch.torch.manual_seed(seed)\n",
    "        dtype = x.dtype\n",
    "\n",
    "        vhat = torch.randn(N, F, J).abs().to(dtype)\n",
    "        Hhat = torch.randn(M, J, dtype=dtype)*Hscale\n",
    "        Rb = torch.eye(M).to(dtype)*Rbscale\n",
    "        Rj = torch.zeros(J, M, M).to(dtype)\n",
    "        return vhat, Hhat, Rb, Rj\n",
    "\n",
    "    def cluster_init(self, x, J=3, K=60, init=1, Rbscale=1e-3, showfig=False):\n",
    "        \"\"\"psudo code, https://www.saedsayad.com/clustering_hierarchical.htm\n",
    "        Given : A set X of obejects{x1,...,xn}\n",
    "                A cluster distance function dist(c1, c2)\n",
    "        for i=1 to n\n",
    "            ci = {xi}\n",
    "        end for\n",
    "        C = {c1, ..., cn}\n",
    "        I = n+1\n",
    "        While I>1 do\n",
    "            (cmin1, cmin2) = minimum dist(ci, cj) for all ci, cj in C\n",
    "            remove cmin1 and cmin2 from C\n",
    "            add {cmin1, cmin2} to C\n",
    "            I = I - 1\n",
    "        end while\n",
    "\n",
    "        However, this naive algorithm does not fit large samples. \n",
    "        Here we use scipy function for the linkage.\n",
    "        J is how many clusters\n",
    "        \"\"\"   \n",
    "        dtype = x.dtype\n",
    "        N, F, M = x.shape\n",
    "\n",
    "        \"get data and clusters ready\"\n",
    "        x_norm = ((x[:,:,None,:]@x[..., None].conj())**0.5)[:,:,0]\n",
    "        if init==1: x_ = x/x_norm * (-1j*x[...,0:1].angle()).exp() # shape of [N, F, M] x_bar\n",
    "        else: x_ = x * (-1j*x[...,0:1].angle()).exp() # the x_tilde in Duong's paper\n",
    "        data = x_.reshape(N*F, M)\n",
    "        I = data.shape[0]\n",
    "        C = [[i] for i in range(I)]  # initial cluster\n",
    "\n",
    "        \"calc. affinity matrix and linkage\"\n",
    "        perms = torch.combinations(torch.arange(len(C)))\n",
    "        d = data[perms]\n",
    "        table = ((d[:,0] - d[:,1]).abs()**2).sum(dim=-1)**0.5\n",
    "        from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "        z = linkage(table, method='average')\n",
    "        if showfig: dn = dendrogram(z, p=3, truncate_mode='level')\n",
    "\n",
    "        \"find the max J cluster and sample index\"\n",
    "        zind = torch.tensor(z).to(torch.int)\n",
    "        flag = torch.cat((torch.ones(I), torch.zeros(I)))\n",
    "        c = C + [[] for i in range(I)]\n",
    "        for i in range(z.shape[0]-K): # threshold of K level to stop\n",
    "            c[i+I] = c[zind[i][0]] + c[zind[i][1]]\n",
    "            flag[i+I], flag[zind[i][0]], flag[zind[i][1]] = 1, 0, 0\n",
    "        ind = (flag == 1).nonzero(as_tuple=True)[0]\n",
    "        dict_c = {}  # which_cluster: how_many_nodes\n",
    "        for i in range(ind.shape[0]):\n",
    "            dict_c[ind[i].item()] = len(c[ind[i]])\n",
    "        dict_c_sorted = {k:v for k,v in sorted(dict_c.items(), key=lambda x: -x[1])}\n",
    "        cs = []\n",
    "        for i, (k,v) in enumerate(dict_c_sorted.items()):\n",
    "            if i == J:\n",
    "                break\n",
    "            cs.append(c[k])\n",
    "\n",
    "        \"initil the EM variables\"\n",
    "        Hhat = torch.rand(M, J, dtype=dtype)\n",
    "        Rj = torch.rand(J, M, M, dtype=dtype)\n",
    "        for i in range(J):\n",
    "            d = data[torch.tensor(cs[i])] # shape of [I_cj, M]\n",
    "            Hhat[:,i] = d.mean(0)\n",
    "            Rj[i] = (d[..., None] @ d[:,None,:].conj()).mean(0)\n",
    "        vhat = torch.ones(J, N, F).abs().to(dtype)\n",
    "        Rb = torch.eye(M).to(dtype)*Rbscale\n",
    "\n",
    "        return vhat, Hhat, Rb, Rj\n",
    "\n",
    "    def em_func_(self, x, J=3, max_iter=501, lamb=0, thresh_K=60, init=1):\n",
    "        \"\"\"init=0: random\n",
    "            init=1: x_bar original\n",
    "            init=else: x_tilde duong's paper\n",
    "        \"\"\"\n",
    "        #  EM algorithm for one complex sample\n",
    "        N, F, M = x.shape\n",
    "        eye = torch.eye(M).to(x.dtype)\n",
    "        if init == 0: # random init\n",
    "            vhat, Hhat, Rb, Rj = self.rand_init(x, J=J)\n",
    "        elif init == 1: #hierarchical initialization -- x_bar\n",
    "            vhat, Hhat, Rb, Rj = self.cluster_init(x, J=J, K=thresh_K, init=init)\n",
    "        else:  #hierarchical initialization -- x_tilde\n",
    "            vhat, Hhat, Rb, Rj = self.cluster_init(x, J=J, K=thresh_K, init=init)\n",
    "        \n",
    "        Rcj, Rx, ll = self.calc_ll(x, vhat, Rj)\n",
    "        ll_traj = []\n",
    "        for i in range(max_iter):\n",
    "            \"E-step\"\n",
    "            W = Rcj @ Rx.inverse() #shape of[J,N,F,M,M]\n",
    "            chat = W @ x[...,None] #shape of[J,N,F,M,1]\n",
    "            Rcjhat = chat @ chat.transpose(-1,-2).conj() + (eye- W)@Rcj #shape of[J,N,F,M,M]\n",
    "\n",
    "            \"M-step\"\n",
    "            vhat = (Rj.inverse()[:,None,None]@Rcjhat).diagonal(dim1=-1, dim2=-2).mean(-1)\n",
    "            vhat.real = threshold(vhat.real, floor=1e-10, ceiling=10)\n",
    "            vhat.imag = vhat.imag - vhat.imag\n",
    "            Rj = (Rcjhat/vhat[...,None, None]).mean(dim=(1,2)) #shape of[J,M,M]\n",
    "\n",
    "            \"compute log-likelyhood\"\n",
    "            Rcj, Rx, ll = self.calc_ll(x, vhat, Rj)\n",
    "            ll_traj.append(ll.item())\n",
    "            if i > 30 and abs((ll_traj[i] - ll_traj[i-3])/ll_traj[i-3]) <1e-4:\n",
    "                print(f'EM early stop at iter {i}')\n",
    "                break\n",
    "\n",
    "        return chat, vhat, ll_traj, torch.linalg.matrix_rank(Rcj).double().mean()\n",
    "\n",
    "#%%\n",
    "EMs, EMh = [], []\n",
    "for snr in ['inf', 20, 10, 5, 0]:\n",
    "    ems, emh = [], []\n",
    "    for ind in range(1000):\n",
    "        if snr != 'inf':\n",
    "            data = awgn(x_all[ind], snr)\n",
    "        else:\n",
    "            data = x_all[ind]\n",
    "        try:\n",
    "            chat, vhat, ll_traj, rank = \\\n",
    "                    EM().em_func_(data, J=6, max_iter=301, thresh_K=10, init=1)\n",
    "            shat = chat.permute(3,4,1,2,0).abs()[0]\n",
    "            hhat = chat[...,0].permute(1,2,3,0).mean(dim=(0,1))\n",
    "            temp_s = s_corr_cuda(shat, s_all[ind:ind+1].abs()).item()\n",
    "            temp = h_corr(hhat, h[ind])\n",
    "            if ind %20 == 0 :\n",
    "                print(f'At epoch {ind}', ' h corr: ', temp, ' s corr:', temp_s)\n",
    "\n",
    "            ems.append(temp_s)\n",
    "            emh.append(temp)\n",
    "        except:\n",
    "            print(f\"An exception occurred {ind}\")\n",
    "    EMs.append(sum(ems)/len(ems))\n",
    "    EMh.append(sum(emh)/len(emh))\n",
    "\n",
    "    print(f'done with one snr {snr}')\n",
    "    print('EMs, EMh', EMs, EMh)\n",
    "\n",
    "print('End date time ', datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dcab5b4c93e5d350732afa47b682ac0d9d883fbc947c4d4de760a957920f983"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
