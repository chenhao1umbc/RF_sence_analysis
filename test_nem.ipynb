{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 3-class nem\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "#%%\n",
    "d, s, h = torch.load('../data/nem_ss/test1kM3FT64_xsh_data3.pt')\n",
    "N, F = s.shape[-1], s.shape[-2] # h is M*J matrix, here 6*6\n",
    "ratio = d.abs().amax(dim=(1,2,3))\n",
    "x_all = (d/ratio[:,None,None,None]).permute(0,2,3,1)\n",
    "s_all = s.abs().permute(0,2,3,1) \n",
    "glr = 0.01\n",
    "\n",
    "from unet.unet_model import *\n",
    "class UNetHalf(nn.Module):\n",
    "    \"16 layers here\"\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        \"\"\"Only the up part of the unet\n",
    "        Args:\n",
    "            n_channels ([type]): [how many input channels=n_sources]\n",
    "            n_classes ([type]): [how many output classes=n_sources]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_ch = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.n_ch = 128\n",
    "\n",
    "        self.up = nn.Sequential(DoubleConv(n_channels, self.n_ch),\n",
    "                    MyUp(self.n_ch, self.n_ch//2),\n",
    "                    MyUp(self.n_ch//2, self.n_ch//4),\n",
    "                    MyUp(self.n_ch//4, self.n_ch//4))\n",
    "        self.reshape = nn.Sequential(\n",
    "                    nn.Conv2d(self.n_ch//4, self.n_ch//4, kernel_size=3, padding=(1,2)),\n",
    "                    nn.BatchNorm2d(self.n_ch//4),\n",
    "                    nn.LeakyReLU(inplace=True),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4))\n",
    "        self.outc = OutConv(32, n_classes)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.reshape(x) \n",
    "        x = self.outc(x)\n",
    "        x = self.sig(x)\n",
    "        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n",
    "        return out\n",
    "\n",
    "def cluster_init(x, J=3, K=10, init=1, Rbscale=1e-3, showfig=False):\n",
    "    \"\"\"psudo code, https://www.saedsayad.com/clustering_hierarchical.htm\n",
    "    Given : A set X of obejects{x1,...,xn}\n",
    "            A cluster distance function dist(c1, c2)\n",
    "    for i=1 to n\n",
    "        ci = {xi}\n",
    "    end for\n",
    "    C = {c1, ..., cn}\n",
    "    I = n+1\n",
    "    While I>1 do\n",
    "        (cmin1, cmin2) = minimum dist(ci, cj) for all ci, cj in C\n",
    "        remove cmin1 and cmin2 from C\n",
    "        add {cmin1, cmin2} to C\n",
    "        I = I - 1\n",
    "    end while\n",
    "\n",
    "    However, this naive algorithm does not fit large samples. \n",
    "    Here we use scipy function for the linkage.\n",
    "    J is how many clusters\n",
    "    \"\"\"   \n",
    "    dtype = x.dtype\n",
    "    N, F, M = x.shape\n",
    "\n",
    "    \"get data and clusters ready\"\n",
    "    x_norm = ((x[:,:,None,:]@x[..., None].conj())**0.5)[:,:,0]\n",
    "    if init==1: x_ = x/x_norm * (-1j*x[...,0:1].angle()).exp() # shape of [N, F, M] x_bar\n",
    "    else: x_ = x * (-1j*x[...,0:1].angle()).exp() # the x_tilde in Duong's paper\n",
    "    data = x_.reshape(N*F, M)\n",
    "    I = data.shape[0]\n",
    "    C = [[i] for i in range(I)]  # initial cluster\n",
    "\n",
    "    \"calc. affinity matrix and linkage\"\n",
    "    perms = torch.combinations(torch.arange(len(C)))\n",
    "    d = data[perms]\n",
    "    table = ((d[:,0] - d[:,1]).abs()**2).sum(dim=-1)**0.5\n",
    "    from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "    z = linkage(table, method='average')\n",
    "    if showfig: dn = dendrogram(z, p=3, truncate_mode='level')\n",
    "\n",
    "    \"find the max J cluster and sample index\"\n",
    "    zind = torch.tensor(z).to(torch.int)\n",
    "    flag = torch.cat((torch.ones(I), torch.zeros(I)))\n",
    "    c = C + [[] for i in range(I)]\n",
    "    for i in range(z.shape[0]-K): # threshold of K level to stop\n",
    "        c[i+I] = c[zind[i][0]] + c[zind[i][1]]\n",
    "        flag[i+I], flag[zind[i][0]], flag[zind[i][1]] = 1, 0, 0\n",
    "    ind = (flag == 1).nonzero(as_tuple=True)[0]\n",
    "    dict_c = {}  # which_cluster: how_many_nodes\n",
    "    for i in range(ind.shape[0]):\n",
    "        dict_c[ind[i].item()] = len(c[ind[i]])\n",
    "    dict_c_sorted = {k:v for k,v in sorted(dict_c.items(), key=lambda x: -x[1])}\n",
    "    cs = []\n",
    "    for i, (k,v) in enumerate(dict_c_sorted.items()):\n",
    "        if i == J:\n",
    "            break\n",
    "        cs.append(c[k])\n",
    "\n",
    "    \"initil the EM variables\"\n",
    "    Hhat = torch.rand(M, J, dtype=dtype)\n",
    "    for i in range(J):\n",
    "        d = data[torch.tensor(cs[i])] # shape of [I_cj, M]\n",
    "        Hhat[:,i] = d.mean(0)\n",
    "\n",
    "    return Hhat\n",
    "\n",
    "def nem_hci(x, J=6, Hscale=1, Rbscale=1, max_iter=501, seed=1, model=''):\n",
    "\n",
    "    def log_likelihood(x, vhat, Hhat, Rb, ):\n",
    "        \"\"\" Hhat shape of [I, M, J] # I is NO. of samples, M is NO. of antennas, J is NO. of sources\n",
    "            vhat shape of [I, N, F, J]\n",
    "            Rb shape of [I, M, M]\n",
    "            x shape of [I, N, F, M]\n",
    "        \"\"\"\n",
    "        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n",
    "        Rcj = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj()\n",
    "        Rxperm = Rcj + Rb \n",
    "        Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "        l = -(np.pi*torch.linalg.det(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "        return l.sum().real, Rs, Rxperm, Rcj\n",
    "\n",
    "    torch.manual_seed(seed) \n",
    "    if model == '':\n",
    "        print('A model is needed')\n",
    "\n",
    "    model = torch.load(model)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(False)\n",
    "    model.eval()\n",
    "\n",
    "    # EM part\n",
    "    \"initial\"        \n",
    "    N, F, M = x.shape\n",
    "    NF= N*F\n",
    "    gamma = torch.rand(1,J,1,8,8).cuda()\n",
    "    x = x.cuda()\n",
    "\n",
    "    vhat = torch.randn(1, N, F, J).abs().to(torch.cdouble).cuda()\n",
    "    outs = []\n",
    "    for j in range(J):\n",
    "        outs.append(model(gamma[:,j]))\n",
    "    out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "    vhat.real = threshold(out)\n",
    "    Hhat = cluster_init(x.cpu(), J=J).cuda().to(torch.cdouble)\n",
    "    Rb = torch.ones(1, M).diag_embed().cuda().to(torch.cdouble)*Rbscale\n",
    "    Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((0,1))/NF\n",
    "    Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n",
    "    Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n",
    "    gamma.requires_grad_()\n",
    "    optim_gamma = torch.optim.RAdam([gamma],\n",
    "            lr= glr,\n",
    "            betas=(0.9, 0.999), \n",
    "            eps=1e-8,\n",
    "            weight_decay=0)\n",
    "    ll_traj = []\n",
    "\n",
    "    for ii in range(max_iter): # EM iterations\n",
    "        \"E-step\"\n",
    "        W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n",
    "        shat = W.permute(2,0,1,3,4) @ x[...,None]\n",
    "        Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n",
    "        Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n",
    "        Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n",
    "\n",
    "        \"M-step\"\n",
    "        Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n",
    "        Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n",
    "            Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n",
    "        Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n",
    "        Rb.imag = Rb.imag - Rb.imag\n",
    "\n",
    "        # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n",
    "        # vj.imag = vj.imag - vj.imag\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(gamma[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat.real = threshold(out, ceiling=1)\n",
    "        loss = loss_func(vhat, Rsshatnf.cuda())\n",
    "        optim_gamma.zero_grad()   \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_([gamma], max_norm=1)\n",
    "        optim_gamma.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        \"compute log-likelyhood\"\n",
    "        vhat = vhat.detach()\n",
    "        ll, Rs, Rx, Rcj = log_likelihood(x, vhat, Hhat, Rb)\n",
    "        ll_traj.append(ll.item())\n",
    "        if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n",
    "        if ii > 20 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <5e-4:\n",
    "            # print(f'EM early stop at iter {ii}')\n",
    "            break\n",
    "\n",
    "    return (shat.cpu(), Hhat.cpu(), vhat.cpu().squeeze()), gamma.detach().cpu(), Rb.cpu(), ll_traj\n",
    "\n",
    "#%%\n",
    "rid = 'n4'\n",
    "model = f'../data/nem_ss/models/{rid}/model_rid{rid}_39.pt'\n",
    "\n",
    "EMs, EMh = [], []\n",
    "for snr in ['inf', 20, 10, 5, 0]:\n",
    "    ems, emh = [], []\n",
    "    for ind in range(1000):\n",
    "        if snr != 'inf':\n",
    "            data = awgn(x_all[ind], snr).to(torch.cdouble)\n",
    "        else:\n",
    "            data = x_all[ind].to(torch.cdouble)\n",
    "\n",
    "        shv, g, Rb, loss = nem_hci(data, J=3, seed=10, model=model, max_iter=301)\n",
    "        shat, Hhat, vhat = shv\n",
    "        temp_s = s_corr_cuda(shat.squeeze().abs()[None], s_all[ind:ind+1].abs()).item()\n",
    "        temp = h_corr(Hhat.squeeze(), h[ind])\n",
    "        if ind %20 == 0 :\n",
    "            print(f'At epoch {ind}', ' h corr: ', temp, ' s corr:', temp_s)\n",
    "        ems.append(temp_s)\n",
    "        emh.append(temp)\n",
    "\n",
    "    EMs.append(sum(ems)/len(ems))\n",
    "    EMh.append(sum(emh)/len(emh))\n",
    "\n",
    "    print(f'snr{snr}, EMs, EMh', EMs, EMh)\n",
    "print('End date time ', datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 6-class nem\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "#%%\n",
    "d, s, h = torch.load('../data/nem_ss/test1kM6FT64_xsh_data3.pt')\n",
    "N, F = s.shape[-1], s.shape[-2] # h is M*J matrix, here 6*6\n",
    "ratio = d.abs().amax(dim=(1,2,3))\n",
    "x_all = (d/ratio[:,None,None,None]).permute(0,2,3,1)\n",
    "s_all = s.abs().permute(0,2,3,1) \n",
    "glr = 0.01\n",
    "\n",
    "from unet.unet_model import *\n",
    "class UNetHalf(nn.Module):\n",
    "    \"16 layers here\"\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        \"\"\"Only the up part of the unet\n",
    "        Args:\n",
    "            n_channels ([type]): [how many input channels=n_sources]\n",
    "            n_classes ([type]): [how many output classes=n_sources]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_ch = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.n_ch = 128\n",
    "\n",
    "        self.up = nn.Sequential(DoubleConv(n_channels, self.n_ch),\n",
    "                    MyUp(self.n_ch, self.n_ch//2),\n",
    "                    MyUp(self.n_ch//2, self.n_ch//4),\n",
    "                    MyUp(self.n_ch//4, self.n_ch//4))\n",
    "        self.reshape = nn.Sequential(\n",
    "                    nn.Conv2d(self.n_ch//4, self.n_ch//4, kernel_size=3, padding=(1,2)),\n",
    "                    nn.BatchNorm2d(self.n_ch//4),\n",
    "                    nn.LeakyReLU(inplace=True),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4))\n",
    "        self.outc = OutConv(32, n_classes)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.reshape(x) \n",
    "        x = self.outc(x)\n",
    "        x = self.sig(x)\n",
    "        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n",
    "        return out\n",
    "\n",
    "def cluster_init(x, J=3, K=7, init=1, Rbscale=1e-3, showfig=False):\n",
    "    \"\"\"psudo code, https://www.saedsayad.com/clustering_hierarchical.htm\n",
    "    Given : A set X of obejects{x1,...,xn}\n",
    "            A cluster distance function dist(c1, c2)\n",
    "    for i=1 to n\n",
    "        ci = {xi}\n",
    "    end for\n",
    "    C = {c1, ..., cn}\n",
    "    I = n+1\n",
    "    While I>1 do\n",
    "        (cmin1, cmin2) = minimum dist(ci, cj) for all ci, cj in C\n",
    "        remove cmin1 and cmin2 from C\n",
    "        add {cmin1, cmin2} to C\n",
    "        I = I - 1\n",
    "    end while\n",
    "\n",
    "    However, this naive algorithm does not fit large samples. \n",
    "    Here we use scipy function for the linkage.\n",
    "    J is how many clusters\n",
    "    \"\"\"   \n",
    "    dtype = x.dtype\n",
    "    N, F, M = x.shape\n",
    "\n",
    "    \"get data and clusters ready\"\n",
    "    x_norm = ((x[:,:,None,:]@x[..., None].conj())**0.5)[:,:,0]\n",
    "    if init==1: x_ = x/x_norm * (-1j*x[...,0:1].angle()).exp() # shape of [N, F, M] x_bar\n",
    "    else: x_ = x * (-1j*x[...,0:1].angle()).exp() # the x_tilde in Duong's paper\n",
    "    data = x_.reshape(N*F, M)\n",
    "    I = data.shape[0]\n",
    "    C = [[i] for i in range(I)]  # initial cluster\n",
    "\n",
    "    \"calc. affinity matrix and linkage\"\n",
    "    perms = torch.combinations(torch.arange(len(C)))\n",
    "    d = data[perms]\n",
    "    table = ((d[:,0] - d[:,1]).abs()**2).sum(dim=-1)**0.5\n",
    "    from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "    z = linkage(table, method='average')\n",
    "    if showfig: dn = dendrogram(z, p=3, truncate_mode='level')\n",
    "\n",
    "    \"find the max J cluster and sample index\"\n",
    "    zind = torch.tensor(z).to(torch.int)\n",
    "    flag = torch.cat((torch.ones(I), torch.zeros(I)))\n",
    "    c = C + [[] for i in range(I)]\n",
    "    for i in range(z.shape[0]-K): # threshold of K level to stop\n",
    "        c[i+I] = c[zind[i][0]] + c[zind[i][1]]\n",
    "        flag[i+I], flag[zind[i][0]], flag[zind[i][1]] = 1, 0, 0\n",
    "    ind = (flag == 1).nonzero(as_tuple=True)[0]\n",
    "    dict_c = {}  # which_cluster: how_many_nodes\n",
    "    for i in range(ind.shape[0]):\n",
    "        dict_c[ind[i].item()] = len(c[ind[i]])\n",
    "    dict_c_sorted = {k:v for k,v in sorted(dict_c.items(), key=lambda x: -x[1])}\n",
    "    cs = []\n",
    "    for i, (k,v) in enumerate(dict_c_sorted.items()):\n",
    "        if i == J:\n",
    "            break\n",
    "        cs.append(c[k])\n",
    "\n",
    "    \"initil the EM variables\"\n",
    "    Hhat = torch.rand(M, J, dtype=dtype)\n",
    "    for i in range(J):\n",
    "        d = data[torch.tensor(cs[i])] # shape of [I_cj, M]\n",
    "        Hhat[:,i] = d.mean(0)\n",
    "\n",
    "    return Hhat\n",
    "\n",
    "def nem_hci(x, J=6, Hscale=1, Rbscale=1, max_iter=501, seed=1, model=''):\n",
    "\n",
    "    def log_likelihood(x, vhat, Hhat, Rb, ):\n",
    "        \"\"\" Hhat shape of [I, M, J] # I is NO. of samples, M is NO. of antennas, J is NO. of sources\n",
    "            vhat shape of [I, N, F, J]\n",
    "            Rb shape of [I, M, M]\n",
    "            x shape of [I, N, F, M]\n",
    "        \"\"\"\n",
    "        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n",
    "        Rcj = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj()\n",
    "        Rxperm = Rcj + Rb \n",
    "        Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "        l = -(np.pi*torch.linalg.det(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "        return l.sum().real, Rs, Rxperm, Rcj\n",
    "\n",
    "    torch.manual_seed(seed) \n",
    "    if model == '':\n",
    "        print('A model is needed')\n",
    "\n",
    "    model = torch.load(model)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(False)\n",
    "    model.eval()\n",
    "\n",
    "    # EM part\n",
    "    \"initial\"        \n",
    "    N, F, M = x.shape\n",
    "    NF= N*F\n",
    "    gamma = torch.rand(1,J,1,8,8).cuda()\n",
    "    x = x.cuda()\n",
    "\n",
    "    vhat = torch.randn(1, N, F, J).abs().to(torch.cdouble).cuda()\n",
    "    outs = []\n",
    "    for j in range(J):\n",
    "        outs.append(model(gamma[:,j]))\n",
    "    out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "    vhat.real = threshold(out)\n",
    "    Hhat = cluster_init(x.cpu(), J=J).cuda().to(torch.cdouble)\n",
    "    Rb = torch.ones(1, M).diag_embed().cuda().to(torch.cdouble)*Rbscale\n",
    "    Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((0,1))/NF\n",
    "    Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n",
    "    Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n",
    "    gamma.requires_grad_()\n",
    "    optim_gamma = torch.optim.RAdam([gamma],\n",
    "            lr= glr,\n",
    "            betas=(0.9, 0.999), \n",
    "            eps=1e-8,\n",
    "            weight_decay=0)\n",
    "    ll_traj = []\n",
    "\n",
    "    for ii in range(max_iter): # EM iterations\n",
    "        \"E-step\"\n",
    "        W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n",
    "        shat = W.permute(2,0,1,3,4) @ x[...,None]\n",
    "        Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n",
    "        Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n",
    "        Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n",
    "\n",
    "        \"M-step\"\n",
    "        Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n",
    "        Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n",
    "            Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n",
    "        Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n",
    "        Rb.imag = Rb.imag - Rb.imag\n",
    "\n",
    "        # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n",
    "        # vj.imag = vj.imag - vj.imag\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(gamma[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat.real = threshold(out, ceiling=1)\n",
    "        loss = loss_func(vhat, Rsshatnf.cuda())\n",
    "        optim_gamma.zero_grad()   \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_([gamma], max_norm=1)\n",
    "        optim_gamma.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        \"compute log-likelyhood\"\n",
    "        vhat = vhat.detach()\n",
    "        ll, Rs, Rx, Rcj = log_likelihood(x, vhat, Hhat, Rb)\n",
    "        ll_traj.append(ll.item())\n",
    "        if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n",
    "        if ii > 20 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <5e-4:\n",
    "            # print(f'EM early stop at iter {ii}')\n",
    "            break\n",
    "\n",
    "    return (shat.cpu(), Hhat.cpu(), vhat.cpu().squeeze()), gamma.detach().cpu(), Rb.cpu(), ll_traj\n",
    "\n",
    "#%%\n",
    "print('End date time ', datetime.now())\n",
    "rid = 'n8'\n",
    "EMs, EMh = [], []\n",
    "model = f'../data/nem_ss/models/{rid}/model_rid{rid}_54.pt'\n",
    "for snr in ['inf', 20, 10, 5, 0]:\n",
    "    ems, emh = [], []\n",
    "    for ind in range(0, 1000, 1):\n",
    "        if snr != 'inf':\n",
    "            data = awgn(x_all[ind], snr).to(torch.cdouble)\n",
    "        else:\n",
    "            data = x_all[ind].to(torch.cdouble)\n",
    "\n",
    "        shv, g, Rb, loss = nem_hci(data, J=6, seed=10, model=model, max_iter=301)\n",
    "        shat, Hhat, vhat = shv\n",
    "        temp_s = s_corr_cuda(shat.squeeze().abs()[None], s_all[ind:ind+1].abs()).item()\n",
    "        temp = h_corr(Hhat.squeeze(), h[ind])\n",
    "        if ind %20 == 0 :\n",
    "            print(f'At epoch {ind}', ' h corr: ', temp, ' s corr:', temp_s)\n",
    "        ems.append(temp_s)\n",
    "        emh.append(temp)\n",
    "\n",
    "    EMs.append(sum(ems)/len(ems))\n",
    "    EMh.append(sum(emh)/len(emh))\n",
    "\n",
    "    print(f'{snr}, EMs, EMh', EMs, EMh)\n",
    "print('End date time ', datetime.now())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c728c5ad72b5fd3c6e083c4badca00ca04470578383c0e9d983163c40aa43e1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
