{"cells":[{"cell_type":"markdown","metadata":{"id":"NVnL-nCeBA3t"},"source":["## Real data running history\n","3 neural networks for 3 components"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"wtuWghQF4NBH","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=5000 the same initialization, warm start,warm shared Hhat, lr_gamma=0.01, 3000tr samples, trim=1\n","\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 5000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.stack([gtr[:,None] for j in range(J)], dim=1)\n","\n","model, optimizer = {}, {}\n","loss_iter, loss_tr = [], []\n","for j in range(J):\n","    model[j] = UNetHalf(opts['n_ch'], 1).cuda()\n","    optimizer[j] = optim.RAdam(model[j].parameters(),\n","                    lr= opts['lr'],\n","                    betas=(0.9, 0.999),\n","                    eps=1e-8,\n","                    weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for j in range(J):\n","        for param in model[j].parameters():\n","            param.requires_grad_(False)\n","        model[j].eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            # Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","            for j in range(J):\n","                out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update neural network\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","        for j in range(J):\n","            model[j].train()\n","            for param in model[j].parameters():\n","                param.requires_grad_(True)\n","            out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            optimizer[j].zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        for j in range(J):\n","            torch.nn.utils.clip_grad_norm_(model[j].parameters(), max_norm=1)\n","            optimizer[j].step()\n","            torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"nrIREPPmhvbC","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=5100 the same initialization, cold start,warm shared Hhat, lr_gamma=0.01, 3000tr samples, trim=1\n","\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 5100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.stack([gtr[:,None] for j in range(J)], dim=1)\n","\n","model, optimizer = {}, {}\n","loss_iter, loss_tr = [], []\n","for j in range(J):\n","    model[j] = UNetHalf(opts['n_ch'], 1).cuda()\n","    optimizer[j] = optim.RAdam(model[j].parameters(),\n","                    lr= opts['lr'],\n","                    betas=(0.9, 0.999),\n","                    eps=1e-8,\n","                    weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for j in range(J):\n","        for param in model[j].parameters():\n","            param.requires_grad_(False)\n","        model[j].eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            # Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","            for j in range(J):\n","                out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        # #%% update neural network\n","        # with torch.no_grad():\n","        #     gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","        #     vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","        #     Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","        for j in range(J):\n","            model[j].train()\n","            for param in model[j].parameters():\n","                param.requires_grad_(True)\n","            out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            optimizer[j].zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        for j in range(J):\n","            torch.nn.utils.clip_grad_norm_(model[j].parameters(), max_norm=1)\n","            optimizer[j].step()\n","            torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"EJQuTsL0TNfU","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=5200 the same initialization, cold start, cold not shared Hhat(due to mistak, it is cold shared Hhat)\n","\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 5200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.stack([gtr[:,None] for j in range(J)], dim=1)\n","\n","model, optimizer = {}, {}\n","loss_iter, loss_tr = [], []\n","for j in range(J):\n","    model[j] = UNetHalf(opts['n_ch'], 1).cuda()\n","    optimizer[j] = optim.RAdam(model[j].parameters(),\n","                    lr= opts['lr'],\n","                    betas=(0.9, 0.999),\n","                    eps=1e-8,\n","                    weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for j in range(J):\n","        for param in model[j].parameters():\n","            param.requires_grad_(False)\n","        model[j].eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            # Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","            for j in range(J):\n","                out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        # #%% update neural network\n","        # with torch.no_grad():\n","        #     gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","        #     vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","        #     Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","        #     Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","        for j in range(J):\n","            model[j].train()\n","            for param in model[j].parameters():\n","                param.requires_grad_(True)\n","            out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            optimizer[j].zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        for j in range(J):\n","            torch.nn.utils.clip_grad_norm_(model[j].parameters(), max_norm=1)\n","            optimizer[j].step()\n","            torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"MyCXm7Y5gEFk","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=5201 the same initialization, cold start, cold not shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 5201 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.stack([gtr[:,None] for j in range(J)], dim=1)\n","\n","model, optimizer = {}, {}\n","loss_iter, loss_tr = [], []\n","for j in range(J):\n","    model[j] = UNetHalf(opts['n_ch'], 1).cuda()\n","    optimizer[j] = optim.RAdam(model[j].parameters(),\n","                    lr= opts['lr'],\n","                    betas=(0.9, 0.999),\n","                    eps=1e-8,\n","                    weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for j in range(J):\n","        for param in model[j].parameters():\n","            param.requires_grad_(False)\n","        model[j].eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","            for j in range(J):\n","                out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        # #%% update neural network\n","        # with torch.no_grad():\n","        #     gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","        #     vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","        #     Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","        #     Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","        for j in range(J):\n","            model[j].train()\n","            for param in model[j].parameters():\n","                param.requires_grad_(True)\n","            out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            optimizer[j].zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        for j in range(J):\n","            torch.nn.utils.clip_grad_norm_(model[j].parameters(), max_norm=1)\n","            optimizer[j].step()\n","            torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"bMDmPtpF_2m0","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=5300 the same initialization, cold start,cold shared Hhat, lr_gamma=0.01, 3000tr samples, trim=1\n","\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 5300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.stack([gtr[:,None] for j in range(J)], dim=1)\n","\n","model, optimizer = {}, {}\n","loss_iter, loss_tr = [], []\n","for j in range(J):\n","    model[j] = UNetHalf(opts['n_ch'], 1).cuda()\n","    optimizer[j] = optim.RAdam(model[j].parameters(),\n","                    lr= opts['lr'],\n","                    betas=(0.9, 0.999),\n","                    eps=1e-8,\n","                    weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for j in range(J):\n","        for param in model[j].parameters():\n","            param.requires_grad_(False)\n","        model[j].eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr.cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            # Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","            for j in range(J):\n","                out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        # #%% update neural network\n","        # with torch.no_grad():\n","        #     gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","        #     vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","        #     Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","        for j in range(J):\n","            model[j].train()\n","            for param in model[j].parameters():\n","                param.requires_grad_(True)\n","            out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            optimizer[j].zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        for j in range(J):\n","            torch.nn.utils.clip_grad_norm_(model[j].parameters(), max_norm=1)\n","            optimizer[j].step()\n","            torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"az3WS8VeJ3ks","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=5400 warm start,warm not shared Hhat(due to mistak, it is warm not shared Hhat)\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 5400 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.stack([gtr[:,None] for j in range(J)], dim=1)\n","\n","model, optimizer = {}, {}\n","loss_iter, loss_tr = [], []\n","for j in range(J):\n","    model[j] = UNetHalf(opts['n_ch'], 1).cuda()\n","    optimizer[j] = optim.RAdam(model[j].parameters(),\n","                    lr= opts['lr'],\n","                    betas=(0.9, 0.999),\n","                    eps=1e-8,\n","                    weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for j in range(J):\n","        for param in model[j].parameters():\n","            param.requires_grad_(False)\n","        model[j].eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            # Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","            for j in range(J):\n","                out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update neural network\n","        with torch.no_grad():\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","        for j in range(J):\n","            model[j].train()\n","            for param in model[j].parameters():\n","                param.requires_grad_(True)\n","            out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            optimizer[j].zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        for j in range(J):\n","            torch.nn.utils.clip_grad_norm_(model[j].parameters(), max_norm=1)\n","            optimizer[j].step()\n","            torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ezDYPNg_VZV5","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=5401 the same initialization, warm start,warm not shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 5401 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.stack([gtr[:,None] for j in range(J)], dim=1)\n","\n","model, optimizer = {}, {}\n","loss_iter, loss_tr = [], []\n","for j in range(J):\n","    model[j] = UNetHalf(opts['n_ch'], 1).cuda()\n","    optimizer[j] = optim.RAdam(model[j].parameters(),\n","                    lr= opts['lr'],\n","                    betas=(0.9, 0.999),\n","                    eps=1e-8,\n","                    weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for j in range(J):\n","        for param in model[j].parameters():\n","            param.requires_grad_(False)\n","        model[j].eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","            for j in range(J):\n","                out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update neural network\n","        with torch.no_grad():\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","        for j in range(J):\n","            model[j].train()\n","            for param in model[j].parameters():\n","                param.requires_grad_(True)\n","            out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            optimizer[j].zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        for j in range(J):\n","            torch.nn.utils.clip_grad_norm_(model[j].parameters(), max_norm=1)\n","            optimizer[j].step()\n","            torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n"]},{"cell_type":"markdown","metadata":{"id":"3jW2Qw0aymjk"},"source":["## single model\n","11- means one neural network with 3 channels ; 12- means one neural network with 1 channel\n","\n","The best result is 125240, similar one is 125243"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"_xrsikAz2IB0","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=115200 cold start, cold not shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 115200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 3  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1) # shape of \n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.sigmoid(model(g)).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        # #%% update variable\n","        # with torch.no_grad():\n","        #     gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","        #     vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","        #     Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","        #     Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","        out = torch.sigmoid(model(g)).permute(0,2,3,1)\n","        optimizer.zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"87IWUIeX15DD","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=115300 cold start, cold shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 115300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 3  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1) # shape of \n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            # Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.sigmoid(model(g)).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        # #%% update variable\n","        # with torch.no_grad():\n","        #     gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","        #     vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","        #     Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","        #     Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","        out = torch.sigmoid(model(g)).permute(0,2,3,1)\n","        optimizer.zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rwz9o349M-I8","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125000 cold start, cold not shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        # #%% update variable\n","        # with torch.no_grad():\n","        #     gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","        #     vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","        #     Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","        #     Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WkjG2xQJlRYQ","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125100 warm start, warm not shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"UYMou3qsDCPe","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125110 warm start, warm not shared Hhat, awgn20db\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=20, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"moLXl2evJCQh","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid125111 warm start, warm not shared Hhat, awgn20db, 100iter\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125111 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 101\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=20, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"abn0pzz4ErlA","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125120 warm start, warm not shared Hhat, awgn15db\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125120 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"BmBJFFqKJTgC","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid125121 warm start, warm not shared Hhat, awgn15db, 100 iter\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125121 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 101\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"dhta5U5WJ_ra","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid125130 warm start, warm not shared Hhat, awgn10db, 100iter\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125130 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 101\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Ouc0IlPeQRU0","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125200 warm start, warm shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"xtUNj7E1dtsj","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125210 warm start, warm shared Hhat, added_noise*3\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125210 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.randn(J, 1, opts['d_gamma'], opts['d_gamma'])*3\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"XLvtfOfBdxDm","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125220 warm start, warm shared Hhat, added_noise\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125220 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.randn(J, 1, opts['d_gamma'], opts['d_gamma'])\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"1O_oAwmId7vO","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125230 warm start, warm shared Hhat, awgn_snr10\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125230 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"iW5STi0u4ceR","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125240 code is missing"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"29PDQV7EALnK","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125241 warm start, warm shared Hhat, awgn_snr0\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125241 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=0, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"YoUhXuzGAR94","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125242 warm start, warm shared Hhat, awgn_snr5\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125242 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=5, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"pC5d4B6SAXpb","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125243 warm start, warm shared Hhat, awgn_snr15\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125243 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"gCc7S-8KAg6U","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125244 warm start, warm shared Hhat, awgn_snr20\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125244 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=20, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"9cjHdElWwbts","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125250 warm start, warm shared Hhat, awgn_snr10, gamma learning rate 0.01\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125250 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.01)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"rkAAOE84jG8Y","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125260 warm start, warm shared Hhat,awgn_snr10, gamma learning rate 0.01, changed model rate 0.005, em eps as 5e-4\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125260 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.005\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.01)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 8 and abs((ll_traj[ii] - ll_traj[ii-5])/ll_traj[ii-5]) <5e-4:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Ahzy5wWXYsE5","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125265 warm start, warm shared Hhat,awgn_snr10, EM iter 201, em eps as 5e-4\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125265 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 8 and abs((ll_traj[ii] - ll_traj[ii-5])/ll_traj[ii-5]) <5e-4:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"4oTLEz-A13-V","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125270 warm start, warm shared Hhat,awgn_snr10, gamma learning rate 0.005\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125270 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.005)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"markdown","metadata":{"id":"uPI2xhUkPW6M"},"source":["## 13 series\n","1 channel, eps is 5e-4, EM iter is 201, overall iter is 71\n","\n","139- loading previous model as the starting point"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"D2zaI_O7Pmuw","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid130000 setting as rid=125243 warm start, warm shared Hhat, awgn_snr15\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 130000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mnvvt9uNm_c0","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid130001 setting as rid=125243 warm start, warm shared Hhat, awgn_snr15\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 130001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"_SAOIZSSe2Gh","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid132000  16to100, warm start, warm shared Hhat, awgn_snr15\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf16to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 132000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 16 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"U05rQhMfswma","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid132100 , 16to100, warm start, warm shared Hhat, awgn_snr20\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf16to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 132100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 16 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=20, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"p5jDiXNLs5Q-","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid132200 , 16to100, warm start, warm shared Hhat, awgn_snr10\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf16to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 132200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 16 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"KS5r4s-Ds-Uc","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid132300 , 16to100, warm start, warm shared Hhat, awgn_snr5\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf16to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 132300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 16 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=5, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"652DnBhwtExs","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid133000 , stack half unet, warm start, warm shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_stack\n","torch.manual_seed(1)\n","\n","rid = 133000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.randn(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","from skimage.transform import resize\n","xx_all = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))[:,None]\n","loss_iter, loss_tr = [], []\n","model = UNetHalf8to100_stack(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        xx = xx_all[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"APOMrT69rGLs","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid133010 , stack half unet, warm start, warm shared Hhat, 256 inner channels\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_stack_256 as HUnet\n","torch.manual_seed(1)\n","\n","rid = 133010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.randn(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","from skimage.transform import resize\n","xx_all = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))[:,None]\n","loss_iter, loss_tr = [], []\n","model = HUnet(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        xx = xx_all[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"o3KUa7E3zFi3","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid133100 , stack half unet structure2, warm start, warm shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_stack2\n","torch.manual_seed(1)\n","\n","rid = 133100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.randn(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","from skimage.transform import resize\n","xx_all = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))[:,None]\n","loss_iter, loss_tr = [], []\n","model = UNetHalf8to100_stack2(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        xx = xx_all[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"RARDxOuvrTQP","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid133110 , stack half unet structure2, warm start, warm shared Hhat, 256 inner channels\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_stack2_256 as HUnet\n","torch.manual_seed(1)\n","\n","rid = 133110 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.randn(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","from skimage.transform import resize\n","xx_all = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))[:,None]\n","loss_iter, loss_tr = [], []\n","model = HUnet(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        xx = xx_all[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"HwL3UT1JOC5n","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid133200, interpolate, half unet, warm start, warm shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_stack3\n","torch.manual_seed(1)\n","\n","rid = 133200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.randn(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","from skimage.transform import resize\n","xx_all = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))[:,None, None]\n","loss_iter, loss_tr = [], []\n","model = UNetHalf8to100_stack3(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        inp = torch.rand(opts['batch_size'], J, 1, opts['d_gamma'],opts['d_gamma']*2).cuda()\n","        inp[...,0::2] = xx_all[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            inp = inp.detach()\n","            for j in range(J):\n","                inp[:,j,...,1::2] = g[:, j]\n","                outs.append(torch.sigmoid(model(inp[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        inp = inp.detach()\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(inp[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WJWsiQBXM_jx","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid133201 , interpolate, half unet, warm start, warm shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_stack3\n","torch.manual_seed(1)\n","\n","rid = 133201 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.randn(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","from skimage.transform import resize\n","# xx_all = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    # order=1, preserve_range=True ))[:,None, None]\n","xx_all = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","loss_iter, loss_tr = [], []\n","model = UNetHalf8to100_stack3(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        inp = torch.rand(opts['batch_size'], J, 1, opts['d_gamma'],opts['d_gamma']*2).cuda()\n","        inp[...,0::2] = xx_all[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            inp = inp.detach()\n","            for j in range(J):\n","                inp[:,j,...,1::2] = g[:, j]\n","                outs.append(torch.sigmoid(model(inp[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        inp = inp.detach()\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(inp[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"7TpSr7fzTxnf","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid133210 ,interpolate, half unet, warm start, warm shared Hhat, 256 inner channels\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_stack3_256 as HUnet\n","torch.manual_seed(1)\n","\n","rid = 133210 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.randn(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","from skimage.transform import resize\n","# xx_all = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    # order=1, preserve_range=True ))[:,None, None]\n","xx_all = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","loss_iter, loss_tr = [], []\n","model = HUnet(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        inp = torch.rand(opts['batch_size'], J, 1, opts['d_gamma'],opts['d_gamma']*2).cuda()\n","        inp[...,0::2] = xx_all[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            inp = inp.detach()\n","            for j in range(J):\n","                inp[:,j,...,1::2] = g[:, j]\n","                outs.append(torch.sigmoid(model(inp[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        inp = inp.detach()\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(inp[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Fq4z20Nmcc8U","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid135000 warm start, warm shared Hhat, awgn_snr15, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 130000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"XFb-ZHCGSm5r","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid135100 warm start, warm shared Hhat, awgn_snr10, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 135100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"5YSTb70VFQ7r","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid135110 warm start, warm shared Hhat, awgn_snr10, 2 more layers, 256 inner channels\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 135110 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WqL4MAdjMzHk","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid135111 warm start, warm shared Hhat, awgn_snr10, 2 more layers, 256 inner channels\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 135111 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"hDINoHcjNJhs","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid135120 warm start, warm shared Hhat, awgn_snr15, 2 more layers, 256 inner channels\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 135120 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"lEpdOxt7NMtP","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid135130 warm start, warm shared Hhat, awgn_snr5, 2 more layers, 256 inner channels\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 135130 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=5, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mmhjldaOSuC-","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid135200 warm start, warm shared Hhat, awgn_snr5, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 135200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=5, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"yZyYOEEzSxDj","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid135300 warm start, warm shared Hhat, awgn_snr20, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 135300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=20, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"1tsojMUvTIbz","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid136000  partially covered with noise, awgn_snr10, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 136000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 10\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","#%%\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"qMxTfNpFTOMe","vscode":{"languageId":"python"}},"outputs":[],"source":["\n","#@title rid136100 awgn_snr10, gtr partially covered with noise\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 136100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 10\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","#%%\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"0c--DxdzzDZC","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid136200 partially covered with noise, awgn_snr15, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 136200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 15\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"YHcUJXhKzEy8","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid136300 partially covered with noise, awgn_snr5, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 136300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 5\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"bPgsZ8HczJpy","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid136400 partially covered with noise, awgn_snr20, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 136400 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 20\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Cx7EOtCXBzuE","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid137000  partially covered with noise, awgn_snr10, 19 layers (5 more)\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_19 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 137000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 10\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","#%%\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"9Jpo11dtooxs","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid137100  partially covered with noise, awgn_snr15, 19 layers (5 more)\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_19 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 137100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 15\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","#%%\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"_H9CY9x1oqLz","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid137200  partially covered with noise, awgn_snr5, 19 layers (5 more)\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_19 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 137200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 5\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","#%%\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"FfEDIViYSq6B","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid139000 warm start, warm shared Hhat, awgn_snr15, loaded125240\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 139000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","# model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","model = torch.load('../data/nem_ss/models/model_rid125240.pt')\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","Hhat = torch.load('../data/nem_ss/models/Hhat_rid125240.pt')\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"fd8iT5ThUa3c","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid139100 warm start, warm shared Hhat, awgn_snr15, loaded125240, g step 5e-3\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 139100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","# model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","model = torch.load('../data/nem_ss/models/model_rid125240.pt')\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","Hhat = torch.load('../data/nem_ss/models/Hhat_rid125240.pt')\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.005)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"markdown","metadata":{"id":"BqCh4vrkSS2C"},"source":["## 14 series using new structures\n","140-2channel, $\\gamma$ as resized mixture; <br/> \n","141-2channel, $\\gamma$ as random noise; similar as 140<br/> \n","142-update Hhat individually; -- too slow cannot finish <br/> \n","143-update Hhat as one, which was I supposed to do before; -- similar result as before <br/>\n","144-added batchnorm before sigmoid/ normalize Hhat; -- too slow, barely finish<br/>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ma_I7AomJgtq","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140000 , unet8to100 warm start, warm shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNet8to100\n","torch.manual_seed(1)\n","\n","rid = 140000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNet8to100(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        xx = x[...,0].abs()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(xx[:,None], g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(xx[:,None], g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ikHO0O2CNz7T","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140100 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"GG-lPXAF5hVO","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140110 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, 128 inner channel\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140110 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"lx6KhQH83w8k","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140120 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140120 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Th87MSoE36Os","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140121 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside, save the temp results\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140121 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","        print('Hhat.norm()', Hhat.cpu().norm())\n","        print('Rb.norm.max and Rb.norm()', Rb.cpu().norm(dim=(-1,-2)).mean(), Rb.cpu().norm())\n","        print('v.max.mean for all sources, vhat.norm()',vhat.detach().cpu().real.amax(dim=(1,2)).mean(dim=0), vhat.detach().cpu().norm())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"8RytjcyaOsz9","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140122 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside, save the temp results\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140122 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","        print('Hhat.norm()', Hhat.cpu().norm())\n","        print('Rb.norm.max and Rb.norm()', Rb.cpu().norm(dim=(-1,-2)).mean(), Rb.cpu().norm())\n","        print('v.max.mean for all sources, vhat.norm()',vhat.detach().cpu().real.amax(dim=(1,2)).mean(dim=0), vhat.detach().cpu().norm())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"eN9QO3hx3YNX","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140130 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside and vj/vj.max\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140130 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"RtbJ-9McLQsW","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140131 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside and vj/vj.max\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140131 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print('Hhat.norm()', Hhat.cpu().norm())\n","        print(' vhat.norm()', vhat.detach().cpu().norm())\n","    \n","        print(f'batch {i} is done \\n')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"jc1kRTNzPu2Q","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140132 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside and vj/vj.max, another run, saving temp results\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140132 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print('Hhat.norm()', Hhat.cpu().norm())\n","        print(' vhat.norm()', vhat.detach().cpu().norm())\n","    \n","        print(f'batch {i} is done \\n')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"fxTow9_6MAQm","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140133 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside and vj/vj.max, another run\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140133 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print('Hhat.norm()', Hhat.cpu().norm())\n","        print(' vhat.norm()', vhat.detach().cpu().norm())\n","    \n","        print(f'batch {i} is done \\n')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"0slb_cpCs7BF","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140140 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside and vj/vj.max, another run, changed step size 2times for the model -- did not run\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140140 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.002\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print('Hhat.norm()', Hhat.cpu().norm())\n","        print(' vhat.norm()', vhat.detach().cpu().norm())\n","    \n","        print(f'batch {i} is done \\n')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"c0G5_8MN3IkS","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140200 warm start, warm shared Hhat, 16 layers, 2 channel input,  label as basis\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    lb[j,0] = basis[:, j]\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"tVrFGAvp5n7T","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140210 warm start, warm shared Hhat, 16 layers, 2 channel input,  label as basis, 128 inner channel\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140210 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    lb[j,0] = basis[:, j]\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"RROdQz1k3I-z","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140300 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis with FC 9*8\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_16_FC as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]]).cuda()\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = g[:,j]\n","                ins[:,j, :,:,8] = basis[:,j]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = g[:,j]\n","            ins[:,j, :,:,8] = basis[:,j]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Ie9k1XEz3N-F","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140301 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis with FC 9*8\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_16_FC as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140301 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]]).cuda()\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = g[:,j]\n","                ins[:,j, :,:,8] = basis[:,j]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = g[:,j]\n","            ins[:,j, :,:,8] = basis[:,j]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"wuHiNaW_4zxx","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140310 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis with FC 9*8, 128 inner channel\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_16_FC_128 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140310 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]]).cuda()\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = g[:,j]\n","                ins[:,j, :,:,8] = basis[:,j]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = g[:,j]\n","            ins[:,j, :,:,8] = basis[:,j]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"g3ZPWOJG1tbQ","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140400 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis using conv\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_stack1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140400 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]]).cuda()\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = g[:,j]\n","                ins[:,j, :,:,8] = basis[:,j]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = g[:,j]\n","            ins[:,j, :,:,8] = basis[:,j]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ypOyVhJoRpuU","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid141100 warm start, warm shared Hhat, 16 layers, 2 channel input, gamma=label as random noise\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 141100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"MuH2cQhrFbDU","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid141101 warm start, warm shared Hhat, 16 layers, 2 channel input, gamma=label as random noise\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 141101 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"e4Ou25wxo-5n","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid141102 warm start, warm shared Hhat, 16 layers, 2 channel input, gamma=label as random noise -- just 11 epochs because stopping critierion is wrong\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 141102 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[epoch-5:epoch-2])/3, sum(loss_tr[epoch-2:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        if (s1-s2)/s1 < 5e-4 :\n","            print('break-2')\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ywv0UoJ4q0x7","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid141103 warm start, warm shared Hhat, 16 layers, 2 channel input, gamma=label as random noise\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 141103 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')    \n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[epoch-5:epoch-2])/3, sum(loss_tr[epoch-2:])/3\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     if (s1-s2)/s1 < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Mxvid5X_SDRf","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid141200 warm start, warm shared Hhat, 16 layers, 2 channel input,  gamma=label as basis\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 141200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    gtr[j,0] = basis[:, j]\n","gtr = gtr.repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"OMo_td8jVs-U","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid141300 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis with FC 9*8\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_16_FC as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 141300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,1,opts['d_gamma'])\n","for j in range(J):\n","    gtr[j,0] = basis[:, j]\n","gtr = gtr.repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = lb0\n","                ins[:,j, :,:,8] = g[:,j,0]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = lb0\n","            ins[:,j, :,:,8] = g[:,j,0]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"PKxm-7e0V-FI","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid141301 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis with FC 9*8\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_16_FC as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 141301 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,1,opts['d_gamma'])\n","for j in range(J):\n","    gtr[j,0] = basis[:, j]\n","gtr = gtr.repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = lb0\n","                ins[:,j, :,:,8] = g[:,j,0]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = lb0\n","            ins[:,j, :,:,8] = g[:,j,0]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"BJ1goPwDWKpu","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid141400 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis using conv\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_stack1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 141400 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,1,opts['d_gamma'])\n","for j in range(J):\n","    gtr[j,0] = basis[:, j]\n","gtr = gtr.repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = lb0\n","                ins[:,j, :,:,8] = g[:,j,0]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = lb0\n","            ins[:,j, :,:,8] = g[:,j,0]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"n2NZFUIsV_qb","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid141401 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis using conv\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_stack1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 141401 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,1,opts['d_gamma'])\n","for j in range(J):\n","    gtr[j,0] = basis[:, j]\n","gtr = gtr.repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = lb0\n","                ins[:,j, :,:,8] = g[:,j,0]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = lb0\n","            ins[:,j, :,:,8] = g[:,j,0]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"X3h1cXAkGrMM","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid142000 based on rid140130, changed Hhat to seperate one -- could not finished within max time\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 142000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Htr, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"AC7NiGgLGt-g","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid142100 based on 140120, changed Hhat to seperate ones -- could not finished within max time\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 142100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Htr, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"HuyTL5vv2kKN","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid142200 based on rid141100, changed Hhat as Htr -- could not finished within max time\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 142200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Htr, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"3Z8y54ba2sAh","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid 142300 based on rid141200 changed Hhat to Htr -- could not finished within max time\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 142300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    gtr[j,0] = basis[:, j]\n","gtr = gtr.repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Htr, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"rDhkyNLtOpCz","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid143000 based on 140120, changed Hhat to M*J\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 143000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Hhat = Hhat.mean(0)\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"rVLqLif-hI5g","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid143100 based on 140120, changed Hhat to M*J before NN\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 143100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Hhat = Hhat.mean(0)\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"KBGUPf3khL5e","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144000 similar to r140120 with batch norm before sigmoid inside -- much slower, 70 epoches is not enough\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140120 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"XnwsA-GGn_HP","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144001 similar to r140120 with batch norm before sigmoid inside, 100 epoch\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 100\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"HhPCL3AzklZA","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144002 similar to r140120 with batch norm before sigmoid inside, 150 epoch\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 144002 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 150\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"s7pdVpD2rop6","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144003, load the result of 144001 as initial\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144003 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 70\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","# model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","model = torch.load('../data/nem_ss/models/model_rid144001.pt')\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('Hhat_rid144001.pt')\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"KGjxHE2YoK5-","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144010 similar to r140120 with batch norm before sigmoid inside, double learning rate\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.002\n","opts['n_epochs'] = 100\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"xjQQ16SMr5ee","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144011, load the result of 144010 as initial\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144011 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.002\n","opts['n_epochs'] = 70\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","# model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","model = torch.load('../data/nem_ss/models/model_rid144010.pt')\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/models/Hhat_rid144010.pt')\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"SMRJqMVX2zgE","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144020 similar to r140120 with batch norm before sigmoid inside, triple learning rate\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144020 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.003\n","opts['n_epochs'] = 70\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"IArHbiAj26iK","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144030 similar to r140120 with batch norm before sigmoid inside, *5 learning rate\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144030 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.005\n","opts['n_epochs'] = 70\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"F_CPQUlK9Agj","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144040 similar to r140120 with batch norm before sigmoid inside, *7 learning rate\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144040 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.007\n","opts['n_epochs'] = 70\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"-o9eCmap9A-9","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144050 similar to r140120 with batch norm before sigmoid inside, *10 learning rate\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144050 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.01\n","opts['n_epochs'] = 70\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"1Tfh9KrQhOWR","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144100 based on 140120, changed Hhat to M*J before NN with batch-norm before sigmoid -- result is very similar to 144000\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 143000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Hhat = Hhat.mean(0)\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Nck9V_kmvS3B","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144200 similar to r140120 with batch norm before sigmoid inside, with Hj normalized\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = Hhat/((Hhat.abs()**2).sum(1)**0.5)[:,None,:]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"kKSSnCblvYdl","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144300 just as rid140120 with Hj normalized\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = Hhat/((Hhat.abs()**2).sum(1)**0.5)[:,None,:]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"1JGaDZoMonZ9","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144400 based on 140120, changed Hhat to M*J AFTER NN -- should be marked as 143-...\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144400 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        if not (epoch==0 and i==0): Hhat = Hhat.mean(0)\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Hhat = Hhat.mean(0)\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"HUa7cOJ9g1kG","vscode":{"languageId":"python"}},"outputs":[],"source":["#%%\n","#@title rid144500 relu as last layer with Hj normalized\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_relu as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144500 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = Hhat/((Hhat.abs()**2).sum(1)**0.5)[:,None,:]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')\n","#%%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"RhhnONHRg6S9","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144520 relu as last layer with Hj normalized, gradiant clip 0.5\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_relu as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144520 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.002\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = Hhat/((Hhat.abs()**2).sum(1)**0.5)[:,None,:]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"7asrenmVHxwA","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title r145000 based on rid140130, 3 fewer batch norm to make it faster\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_lsbn as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 145000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"5CgNnL30QEUg","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title r145001 based on rid140130, 3 fewer batch norm to make it faster\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_lsbn as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 145001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"o1FZghn9JDcF","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title r145002 based on rid140130, 3 fewer batch norm to make it faster\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_lsbn as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 145002 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-6:-3])/3, sum(loss_tr[-3:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ZII0tMY6JXHE","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title r145100 based on rid140120, 3 fewer batch norm to make it faster -- not really working well\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_lsbn as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 145100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"RFuBLusSQNiP","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title r145101 based on rid140120, 3 fewer batch norm to make it faster\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_lsbn as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 145101 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"IL2C-7QgSA_m","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid146000 less batch norm with bn before sigmoid, with stopping criteria 5e-4 -- learning rate too small, stopped\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_lsbn2 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 146000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[epoch-5:epoch-2])/3, sum(loss_tr[epoch-2:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        if (s1-s2)/s1 < 5e-4 :\n","            print('break-2')\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"VM8tmI3wSEnj","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid146010 less batch norm with bn before sigmoid, without stopping criteria 5e-4-- learning rate too small, stopped\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_lsbn2 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 146010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')    \n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[epoch-5:epoch-2])/3, sum(loss_tr[epoch-2:])/3\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     if (s1-s2)/s1 < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"kxsYP4l2TOKh","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid146110 warm start, warm shared Hhat, 16 layers, 2 channel input, gamma=label as random noise-- learning rate too small, stopped\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_lsbn2 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 146110 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')  "]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"XxomcurZT2Jn","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid146111 warm start, warm shared Hhat, 16 layers, 2 channel input, gamma=label as random noise -- learning rate too small, stopped\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_lsbn2 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 146111 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')  "]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"jwCeIVAfoX45","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid147000 similar to r140120 with batch norm before sigmoid inside  -- last layer with batchnorm or batchnor+relu are very slow\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig2 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 147000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[epoch-5:epoch-2])/3, sum(loss_tr[epoch-2:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"n0bMRtzfoepU","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid148000 similar to r140120 with batch norm before sigmoid inside -- last layer with batchnorm or batchnor+relu are very slow\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig3 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 148000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[epoch-5:epoch-2])/3, sum(loss_tr[epoch-2:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"zPjjM6pldFaJ","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid149000 similar to r140120 with batch norm before sigmoid inside --last layer 1 conv changed to 3 conv\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig4 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 149000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-6:-3])/3, sum(loss_tr[-3:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"rTB0hXG9H2-m","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid149100 sigmoid changed to -- 1-conv relu+vj/vj.max \n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_2 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 149100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-6:-3])/3, sum(loss_tr[-3:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"4o7usneqH6ce","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid149200 sigmoid changed to 3-conv relu+vj/vj.max  -- shows up nan error\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_3 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 149200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-6:-3])/3, sum(loss_tr[-3:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break"]},{"cell_type":"markdown","metadata":{"id":"w_il05ERz3u0"},"source":["## 15 series using new structures\n","basically, record all the models and loss, so far 150000_35 is chosen for the 3 class final data. Hope to defeat overfitting(after too many epoches vj is too small) and simple stopping criteria.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"VyJGL4ru4Nb0","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid150000 similar to 140100, but vj/vjmax_detach ------------*star*\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 150000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"QUQvzApz6efF","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid150100 similar to 149000, but vj/vjmax_detach\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 150100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"AYau35bEZsW0","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid151000, same as rid140120, but record all the models\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 151000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    # if epoch >20 :\n","    #     s1, s2 = sum(loss_tr[((-2*n-10):(-n-10)])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Caz1899ObdVo","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid151001, same as rid140120, but with stopping criteria and recording\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 151001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >20 :\n","        s1, s2 = sum(loss_tr[(-2*n-10):(-n-10)])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"nigmG0MhF3Rq","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid152000, same as rid140120, with validation\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 152000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","d, *_ = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xval[:200])\n","val = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gval = torch.tensor(resize(xval[...,0].abs(), [xval.shape[0],opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","g = torch.load('../data/nem_ss/gval_500.pt')\n","gval = g[:200]/g[:200].amax(dim=[1,2])[...,None,None]  #standardization \n","gval = torch.cat([gval[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I_val,J,1,8,8]\n","\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr, loss_val = [], [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","l = torch.load('../data/nem_ss/140100_lb.pt')\n","lb = l.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","    \n","    if epoch > 20:\n","        loss_val.append(val_run(val, gval, model, lb))\n","        torch.cuda.empty_cache()\n","        plt.figure()\n","        plt.plot(loss_val, '-or')\n","        plt.title(f'Val loss fuction up to epoch{epoch}')\n","        plt.savefig(fig_loc + f'id{rid}_ValLoss_epoch{epoch}')\n","        torch.save(loss_val, mod_loc +f'loss_val_rid{rid}.pt')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')\n","    \n","\n","    # if epoch >20 :\n","    #     s1, s2 = sum(loss_tr[(-2*n-10):(-n-10)])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"markdown","metadata":{"id":"hyhzT1CIcERI"},"source":["## 16 series -- working on the 6 classes data"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"SU5mjShWcO0F","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid160000 based on rid150000 for 6 classes------------*star*\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 160000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"56FAuehPEraW","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid160001 based on rid150000 for 6 classes, batch size 64 is too big to run on colab\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 160001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jP_zaqohpWuK","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid160100 based on rid150000 for 6 classes\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 160100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WrDopGHuvk4l","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid160200 based on rid150000 for 6 classes\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_7 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 160200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"7WNqA_lGvit3","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid160201 based on rid150000 for 6 classes\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_7 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 160201 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"]},{"cell_type":"markdown","metadata":{"id":"RsvZxNctBe2D"},"source":["## 17 series\n","170 Based on vj/vj.detach().max(), only train existing classes, really weakly setting\n","171 Unsupervised setting, trying to explore vj could be all zero for J<M, for the future weakly setting\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"XoZ7E4bDME0I","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid170000 based on rid160100 for 6 classes\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","def batch_process(model, g, lb, idx):\n","    \"\"\"This function try to process forward pass in a batch way\n","\n","    Args:\n","        model (object): [Neural network]\n","        g (gamma): [shape of [I,J,1,8,8]]\n","        lb (regularizer): [shape of[I,J,1,8,8]]\n","        idx (which g,lb to use): [shape of [?,2]]\n","    \"\"\"\n","    G = g[idx[:,0], idx[:,1]]  # shape of [?<I,?<J,1, 8, 8]\n","    L = lb[idx[:,0], idx[:,1]]\n","    inputs = torch.cat((G, L), dim=2).reshape(-1, 2, L.shape[-2], L.shape[-1])\n","    outs = []\n","    bs, i = 30, 0  # batch size\n","    while i*bs <= inputs.shape[0]:\n","        outs.append(model(inputs[i*bs:i*bs+bs]).squeeze())\n","        i += 1 \n","    res = torch.cat(outs).reshape(G.shape[0], G.shape[1],100,100)\n","    return res\n","\n","rid = 170000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 2850 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 50\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # how many samples to average for stopping \n","\n","d, lb = torch.load('../data/nem_ss/weakly50percomb_tr3kM6FT100_xlb.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","lbs = torch.zeros(I, J)\n","for i, v in enumerate(lb):\n","    lbs[i*50:(i+1)*50, v] = 1\n","\"shuffle data\"\n","ind = torch.randperm(I)\n","xtr, lbs = xtr[ind], lbs[ind]\n","data = Data.TensorDataset(xtr, lbs)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.ones(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.ones(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x, y) in enumerate(tr): # x is data, y is label\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat@Rs.permute(1,2,0,3,4)@Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj, s = [], y.sum() # s means how many components in one batch\n","        idx = torch.nonzero(y)\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4)@Hhat.transpose(-1,-2).conj()@Rx.inverse()  #shape of [N,F,I,J,M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - \\\n","                (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF #shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            \"calculate vj\"\n","            out = torch.ones(vhat.shape, device=vhat.device)*1e-20\n","            out[idx[:,0],:,:,idx[:,1]] = batch_process(model, g, lb, idx)[:,0]\n","            vhat.real = threshold(out)\n","            \n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_lh(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.detach().item()/s)\n","            if torch.isnan(ll_traj[-1]) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        out = torch.ones(vhat.shape, device=vhat.device)*1e-20\n","        out[idx[:,0],:,:,idx[:,1]] = batch_process(model, g, lb, idx)[:,0]\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        loss = loss_func(vhat, Rsshatnf.cuda())\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item()/s)\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item()/s)\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"uDswEqGw3iYG","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid170100 based on rid170000 for 6 classes, not shared H\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","def batch_process(model, g, lb, idx):\n","    \"\"\"This function try to process forward pass in a batch way\n","\n","    Args:\n","        model (object): [Neural network]\n","        g (gamma): [shape of [I,J,1,8,8]]\n","        lb (regularizer): [shape of[I,J,1,8,8]]\n","        idx (which g,lb to use): [shape of [?,2]]\n","    \"\"\"\n","    G = g[idx[:,0], idx[:,1]]  # shape of [?<I,?<J,1, 8, 8]\n","    L = lb[idx[:,0], idx[:,1]]\n","    inputs = torch.cat((G, L), dim=2).reshape(-1, 2, L.shape[-2], L.shape[-1])\n","    outs = []\n","    bs, i = 30, 0  # batch size\n","    while i*bs <= inputs.shape[0]:\n","        outs.append(model(inputs[i*bs:i*bs+bs]).squeeze())\n","        i += 1 \n","    res = torch.cat(outs).reshape(G.shape[0], G.shape[1],100,100)\n","    return res\n","\n","rid = 170100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 2850 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 50\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # how many samples to average for stopping \n","\n","d, lb = torch.load('../data/nem_ss/weakly50percomb_tr3kM6FT100_xlb.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","lbs = torch.zeros(I, J)\n","for i, v in enumerate(lb):\n","    lbs[i*50:(i+1)*50, v] = 1\n","\"shuffle data\"\n","ind = torch.randperm(I)\n","xtr, lbs = xtr[ind], lbs[ind]\n","data = Data.TensorDataset(xtr, lbs)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.ones(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","# Hhat = torch.ones(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x, y) in enumerate(tr): # x is data, y is label\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat@Rs.permute(1,2,0,3,4)@Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj, s = [], y.sum() # s means how many components in one batch\n","        idx = torch.nonzero(y)\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4)@Hhat.transpose(-1,-2).conj()@Rx.inverse()  #shape of [N,F,I,J,M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - \\\n","                (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF #shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            \"calculate vj\"\n","            out = torch.ones(vhat.shape, device=vhat.device)*1e-20\n","            out[idx[:,0],:,:,idx[:,1]] = batch_process(model, g, lb, idx)[:,0]\n","            vhat.real = threshold(out)\n","            \n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_lh(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.detach().item()/s)\n","            if torch.isnan(ll_traj[-1]) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        out = torch.ones(vhat.shape, device=vhat.device)*1e-20\n","        out[idx[:,0],:,:,idx[:,1]] = batch_process(model, g, lb, idx)[:,0]\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        loss = loss_func(vhat, Rsshatnf.cuda())\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item()/s)\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item()/s)\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"0A9dyp6B3mmU","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid171000, relu, ceiling=1\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import Model171000 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 171000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","# d, *_ = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","# xval = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","# data = Data.TensorDataset(xval[:200])\n","# val = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# # from skimage.transform import resize\n","# # gval = torch.tensor(resize(xval[...,0].abs(), [xval.shape[0],opts['d_gamma'],opts['d_gamma']],\\\n","# #     order=1, preserve_range=True ))\n","# g = torch.load('../data/nem_ss/gval_500.pt')\n","# gval = g[:200]/g[:200].amax(dim=[1,2])[...,None,None]  #standardization \n","# gval = torch.cat([gval[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I_val,J,1,8,8]\n","\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr, loss_val = [], [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","l = torch.load('../data/nem_ss/140100_lb.pt')\n","lb = l.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out, ceiling=1)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out,ceiling=1)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","    \n","    # if epoch > 20:\n","    #     loss_val.append(val_run(val, gval, model, lb))\n","    #     torch.cuda.empty_cache()\n","    #     plt.figure()\n","    #     plt.plot(loss_val, '-or')\n","    #     plt.title(f'Val loss fuction up to epoch{epoch}')\n","    #     plt.savefig(fig_loc + f'id{rid}_ValLoss_epoch{epoch}')\n","    #     torch.save(loss_val, mod_loc +f'loss_val_rid{rid}.pt')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')\n","    \n","\n","    # if epoch >20 :\n","    #     s1, s2 = sum(loss_tr[(-2*n-10):(-n-10)])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Bfxp7NsO9qNg","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid171001, relu, ceiling=1, not*3\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import Model171000 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 171001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","# d, *_ = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","# xval = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","# data = Data.TensorDataset(xval[:200])\n","# val = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# # from skimage.transform import resize\n","# # gval = torch.tensor(resize(xval[...,0].abs(), [xval.shape[0],opts['d_gamma'],opts['d_gamma']],\\\n","# #     order=1, preserve_range=True ))\n","# g = torch.load('../data/nem_ss/gval_500.pt')\n","# gval = g[:200]/g[:200].amax(dim=[1,2])[...,None,None]  #standardization \n","# gval = torch.cat([gval[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I_val,J,1,8,8]\n","\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr, loss_val = [], [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","l = torch.load('../data/nem_ss/140100_lb.pt')\n","lb = l.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out, ceiling=1)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out,ceiling=1)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","    \n","    # if epoch > 20:\n","    #     loss_val.append(val_run(val, gval, model, lb))\n","    #     torch.cuda.empty_cache()\n","    #     plt.figure()\n","    #     plt.plot(loss_val, '-or')\n","    #     plt.title(f'Val loss fuction up to epoch{epoch}')\n","    #     plt.savefig(fig_loc + f'id{rid}_ValLoss_epoch{epoch}')\n","    #     torch.save(loss_val, mod_loc +f'loss_val_rid{rid}.pt')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')\n","    \n","\n","    # if epoch >20 :\n","    #     s1, s2 = sum(loss_tr[(-2*n-10):(-n-10)])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"xAnhPbwk9rZc","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid171010, e^x, ceiling=1\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import Model171010 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 171010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","# d, *_ = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","# xval = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","# data = Data.TensorDataset(xval[:200])\n","# val = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# # from skimage.transform import resize\n","# # gval = torch.tensor(resize(xval[...,0].abs(), [xval.shape[0],opts['d_gamma'],opts['d_gamma']],\\\n","# #     order=1, preserve_range=True ))\n","# g = torch.load('../data/nem_ss/gval_500.pt')\n","# gval = g[:200]/g[:200].amax(dim=[1,2])[...,None,None]  #standardization \n","# gval = torch.cat([gval[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I_val,J,1,8,8]\n","\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr, loss_val = [], [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","l = torch.load('../data/nem_ss/140100_lb.pt')\n","lb = l.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out, ceiling=1)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out,ceiling=1)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","    \n","    # if epoch > 20:\n","    #     loss_val.append(val_run(val, gval, model, lb))\n","    #     torch.cuda.empty_cache()\n","    #     plt.figure()\n","    #     plt.plot(loss_val, '-or')\n","    #     plt.title(f'Val loss fuction up to epoch{epoch}')\n","    #     plt.savefig(fig_loc + f'id{rid}_ValLoss_epoch{epoch}')\n","    #     torch.save(loss_val, mod_loc +f'loss_val_rid{rid}.pt')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')\n","    \n","\n","    # if epoch >20 :\n","    #     s1, s2 = sum(loss_tr[(-2*n-10):(-n-10)])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mlESL3Qj-5ZM","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid171011, e^x, ceiling=1, gamma rate=0.01\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import Model171010 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 171011 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","# d, *_ = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","# xval = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","# data = Data.TensorDataset(xval[:200])\n","# val = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# # from skimage.transform import resize\n","# # gval = torch.tensor(resize(xval[...,0].abs(), [xval.shape[0],opts['d_gamma'],opts['d_gamma']],\\\n","# #     order=1, preserve_range=True ))\n","# g = torch.load('../data/nem_ss/gval_500.pt')\n","# gval = g[:200]/g[:200].amax(dim=[1,2])[...,None,None]  #standardization \n","# gval = torch.cat([gval[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I_val,J,1,8,8]\n","\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr, loss_val = [], [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","l = torch.load('../data/nem_ss/140100_lb.pt')\n","lb = l.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.01)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out, ceiling=1)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out,ceiling=1)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","    \n","    # if epoch > 20:\n","    #     loss_val.append(val_run(val, gval, model, lb))\n","    #     torch.cuda.empty_cache()\n","    #     plt.figure()\n","    #     plt.plot(loss_val, '-or')\n","    #     plt.title(f'Val loss fuction up to epoch{epoch}')\n","    #     plt.savefig(fig_loc + f'id{rid}_ValLoss_epoch{epoch}')\n","    #     torch.save(loss_val, mod_loc +f'loss_val_rid{rid}.pt')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')\n","    \n","\n","    # if epoch >20 :\n","    #     s1, s2 = sum(loss_tr[(-2*n-10):(-n-10)])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"markdown","metadata":{"id":"RsvZxNctBe2D"},"source":["## 18 series\n","Using the Hierarchiecal Clustering Initialization to see if [gamma1, gamma2] structure can be replaced by only gamma2\\\n","gamma1 is resized mixture\\\n","gamma2 is random noise served as label\\"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mlESL3Qj-5ZM","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid180000, M=6, totally random\n","#%% this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 180000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.ones(I, N, F, J).abs().to(torch.cdouble)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid180010, M=6, same random for all samples\n","#%% this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 180010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.ones(I, N, F, J).abs().to(torch.cdouble)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","gtr = torch.rand(J,1,opts['d_gamma'], opts['d_gamma']).repeat(I,1,1,1)\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid180100, M=3, totally random\n","#%% this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 180100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.ones(I, N, F, J).abs().to(torch.cdouble)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid180101, M=3, totally random\n","#%% this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 180101 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.ones(I, N, F, J).abs().to(torch.cdouble)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid180110, M=3, same random for all samples\n","#%% this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 180110 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.ones(I, N, F, J).abs().to(torch.cdouble)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","gtr = torch.rand(J,1,opts['d_gamma'], opts['d_gamma']).repeat(I,1,1,1,1)\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid180120, M=3, totally random, gamma with inner loop\n","#%% this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%%\n","rid = 180120 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.ones(I, N, F, J).abs().to(torch.cdouble)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.1) #### \n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            l0, lpre = 0, 0\n","            rec = []\n","            for ig in range(100):\n","                outs = []\n","                for j in range(J):\n","                    outs.append(model(g[:,j]))\n","                out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","                vhat = vhat.detach()\n","                vhat.real = threshold(out)\n","                loss = loss_func(vhat, Rsshatnf.cuda())\n","                optim_gamma.zero_grad()   \n","                loss.backward()\n","                rec.append(loss.detach().item())\n","                if ig == 0:\n","                    l0 = loss.detach().item()\n","                    lpre = l0\n","                else:\n","                    curr = loss.detach().item()\n","                    if (curr/l0).real < 5e-4 or g.grad.norm() < 5e-4:\n","                        # print(f'gamma break at {ig}, due to loss decreasing slow')\n","                        break\n","                    elif curr.real>lpre.real:\n","                        print(f'gamma break at {ig}, , due to loss increasing')\n","                        break\n","                    lpre = curr\n","                torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n","                optim_gamma.step()\n","                torch.cuda.empty_cache()\n","               \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid180130, M=3, totally random, gamma with inner loop, gammalr=0.01\n","#%% this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%%\n","rid = 180130 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.ones(I, N, F, J).abs().to(torch.cdouble)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.01) # gamma learning rate \n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            l0, lpre = 0, 0\n","            rec = []\n","            for ig in range(100):\n","                outs = []\n","                for j in range(J):\n","                    outs.append(model(g[:,j]))\n","                out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","                vhat = vhat.detach()\n","                vhat.real = threshold(out)\n","                loss = loss_func(vhat, Rsshatnf.cuda())\n","                optim_gamma.zero_grad()   \n","                loss.backward()\n","                rec.append(loss.detach().item())\n","                if ig == 0:\n","                    l0 = loss.detach().item()\n","                    lpre = l0\n","                else:\n","                    curr = loss.detach().item()\n","                    if (curr/l0).real < 5e-4 or g.grad.norm() < 5e-4:\n","                        # print(f'gamma break at {ig}, due to loss decreasing slow')\n","                        break\n","                    elif curr.real>lpre.real:\n","                        print(f'gamma break at {ig}, , due to loss increasing')\n","                        break\n","                    lpre = curr\n","                torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n","                optim_gamma.step()\n","                torch.cuda.empty_cache()\n","               \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid180140, M=3, totally random, gamma with inner loop, gammalr=0.001\n","#%% this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%%\n","rid = 180140 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.ones(I, N, F, J).abs().to(torch.cdouble)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001) # gamma learning rate \n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            l0, lpre = 0, 0\n","            rec = []\n","            for ig in range(100):\n","                outs = []\n","                for j in range(J):\n","                    outs.append(model(g[:,j]))\n","                out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","                vhat = vhat.detach()\n","                vhat.real = threshold(out)\n","                loss = loss_func(vhat, Rsshatnf.cuda())\n","                optim_gamma.zero_grad()   \n","                loss.backward()\n","                rec.append(loss.detach().item())\n","                if ig == 0:\n","                    l0 = loss.detach().item()\n","                    lpre = l0\n","                else:\n","                    curr = loss.detach().item()\n","                    if (curr/l0).real < 5e-4 or g.grad.norm() < 5e-4:\n","                        # print(f'gamma break at {ig}, due to loss decreasing slow')\n","                        break\n","                    elif curr.real>lpre.real:\n","                        print(f'gamma break at {ig}, , due to loss increasing')\n","                        break\n","                    lpre = curr\n","                torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n","                optim_gamma.step()\n","                torch.cuda.empty_cache()\n","               \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid180200, M=3, resized mixture as gamma\n","#%% this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 180200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.ones(I, N, F, J).abs().to(torch.cdouble)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","# gtr = torch.rand(J,1,opts['d_gamma'], opts['d_gamma']).repeat(I,1,1,1)\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1).to(torch.float)\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid180210, M=3, resized mixture as gamma, batch=64\n","# this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 180210 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.ones(I, N, F, J).abs().to(torch.cdouble)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","# gtr = torch.rand(J,1,opts['d_gamma'], opts['d_gamma']).repeat(I,1,1,1)\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1).to(torch.float)\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid180220, M=3, resized mixture as gamma, batch=64, FCN\n","# this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.fcn_model import FCN1 \n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 180220 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = FCN1().cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.ones(I, N, F, J).abs().to(torch.cdouble)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","gtr = torch.rand(I,J,opts['d_gamma'],opts['d_gamma'])\n","# gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","# gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","# gtr = torch.cat([gtr for j in range(J)], dim=1).to(torch.float)\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.01)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            l0, lpre = 0, 0\n","            # rec = []\n","            for ig in range(100):\n","                outs = model(g)\n","                out = outs.permute(0,2,3,1).to(torch.double)\n","                vhat = vhat.detach()\n","                vhat.real = threshold(out)\n","                loss = loss_func(vhat, Rsshatnf.cuda())\n","                optim_gamma.zero_grad()   \n","                loss.backward()\n","                # rec.append(loss.detach().item())\n","                if ig == 0:\n","                    l0 = loss.detach().item()\n","                    lpre = l0\n","                else:\n","                    curr = loss.detach().item()\n","                    if (curr/l0).real < 5e-4 or g.grad.norm() < 5e-4:\n","                        # print(f'gamma break at {ig}, due to loss decreasing slow')\n","                        break\n","                    elif curr.real>lpre.real:\n","                        print(f'gamma break at {ig}, , due to loss increasing')\n","                        break\n","                    lpre = curr\n","                # torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n","                optim_gamma.step()\n","                torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid181000, based on 150000, using HCI\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 181000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid181010, based on 150000, using HCI with hybrid precision -- works well\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","# torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 181010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr.to(torch.cdouble))\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1).to(torch.float32)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.float64)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid181020, based on 150000, using HCI with hybrid precision, Rb=1e-3\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","# torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 181020 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr.to(torch.cdouble))\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1).to(torch.float32)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.float64)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182000, delta=100, H is ground truth, gamma iter=1, glr=0.001\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 100, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            # torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182010, delta=1, H is ground truth, gamma iter=1, glr=0.001\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            # torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182020, delta=0.01, H is ground truth, gamma iter=1, glr=0.001\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182020 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1e-2, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            # torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182100, delta=100, H is ground truth, gamma iter=1, glr=0.01\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 100, 1e-2 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            # torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182110, delta=1, H is ground truth, gamma iter=1, glr=0.01\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182110 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            # torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182120, delta=0.01, H is ground truth, gamma iter=1, glr=0.01\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182120 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1e-2, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            # torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182200, delta=100, H is ground truth, gamma iter=1, glr=0.01, gnorm clip to 10\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 100, 1e-2 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182210, delta=1, H is ground truth, gamma iter=1, glr=0.01, gnorm clip to 10\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182210 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            # torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182220, delta=0.01, H is ground truth, gamma iter=1, glr=0.01, gnorm clip to 10\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182220 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1e-2, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            # torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182230, delta=100, H is ground truth, gamma iter=1, glr=0.001, gnorm clip to 1\n","# to compare with 182240, just gamma1, gamma2 vs gamma\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182230 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 100, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182240, delta=100, H is ground truth, gamma iter=1, glr=0.001, gnorm clip to 1, run gamma1 gamma2 \n","# to see if I could reproduce previouse results. It should work better, only diff is H is ground truth\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182240 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 100, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1).to(torch.float)\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182241\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182241 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 100, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","# _, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","# Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1).to(torch.float)\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182242\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182242 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 100, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1).to(torch.float)\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182250, delta=0.001\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182250 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.001, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182260, delta=1\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182260 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182300, delta=0.001\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.001, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182301, delta=0.001\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.001, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182310, delta=1\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182310 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182320, delta=100\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182320 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 100, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182330, delta=0.001, glr=0.01\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182330 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.001, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182331, delta=0.001, glr=0.01\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182331 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.001, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182340/r182341, delta=1, glr=0.01\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182340 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182342, delta=1, glr=0.01\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182342 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 128\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182343, delta=1, glr=0.01\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182343 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 128\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.randn(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182344, delta=1, glr=0.01\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182344 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.randn(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182345, delta=1, glr=0.01\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182345 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 128\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 281\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.randn(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182346, delta=1, glr=0.01\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182346 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 281\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.randn(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182350, delta=100, glr=0.01\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182350 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 100, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182360\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182360 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.001, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 51\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182370\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182370 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 51\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182380\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182380 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 100, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 51\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"markdown","metadata":{},"source":["## 19 series\n","Using spatial broadcast decoder(SBD), only $\\gamma$ and HCI to get good results as before"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r190000\n","# bases on r182340, sbd\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD1 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 190000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 128  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N,opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","# _, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r190010\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD1 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 190010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 64  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N,opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","# _, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = h.to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r190020\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD1 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 190020 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 64  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N,opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191000\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD2 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 32  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191001\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD2 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 32  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 181\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191002\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD2 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191002 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 32  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 181\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = torch.load('../data/nem_ss/models/rid191001/'+f'model_rid191001_180.pt')\n","# model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191003\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD2 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191003 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 32  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 361\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191010\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD2 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 32  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191100\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD3 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191101\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD3 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191101 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 181\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191110\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD3 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191110 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191200\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD4 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191210\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD4 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191210 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191300\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD4 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.001 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _, h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            l0, lpre = 0, 0\n","            for ig in range(50):\n","                outs = []\n","                for j in range(J):\n","                    outs.append(model(g[:,j]))\n","                out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","                vhat = vhat.detach()\n","                vhat.real = threshold(out)\n","                loss = loss_func(vhat, Rsshatnf.cuda())\n","                optim_gamma.zero_grad()   \n","                loss.backward()\n","                if ig == 0:\n","                    l0 = loss.detach().item()\n","                    lpre = l0\n","                else:\n","                    curr = loss.detach().item()\n","                    l1, l2 = abs(curr-l0), abs(lpre-l0)\n","                    if abs((l2-l1)/l1) < 5e-4 or g.grad.norm() < 5e-4 :\n","                        print(f'gamma break at {ig}, due to loss decreasing slow')\n","                        break\n","                    elif curr.real>lpre.real:\n","                        print(f'gamma break at {ig}, , due to loss increasing')\n","                        break\n","                    lpre = curr\n","                torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n","                optim_gamma.step()\n","                torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191310\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD4 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191310 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.001 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _, h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            l0, lpre = 0, 0\n","            for ig in range(50):\n","                outs = []\n","                for j in range(J):\n","                    outs.append(model(g[:,j]))\n","                out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","                vhat = vhat.detach()\n","                vhat.real = threshold(out)\n","                loss = loss_func(vhat, Rsshatnf.cuda())\n","                optim_gamma.zero_grad()   \n","                loss.backward()\n","                if ig == 0:\n","                    l0 = loss.detach().item()\n","                    lpre = l0\n","                else:\n","                    curr = loss.detach().item()\n","                    l1, l2 = abs(curr-l0), abs(lpre-l0)\n","                    if abs((l2-l1)/l1) < 5e-4 or g.grad.norm() < 5e-4 :\n","                        print(f'gamma break at {ig}, due to loss decreasing slow')\n","                        break\n","                    elif curr.real>lpre.real:\n","                        print(f'gamma break at {ig}, , due to loss increasing')\n","                        break\n","                    lpre = curr\n","                torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n","                optim_gamma.step()\n","                torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r192000\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD3 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 192000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.01\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r192001\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD3 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 192001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.01\n","opts['n_epochs'] = 181\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r192010\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD3 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 192010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.005\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r192020\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD3 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 192020 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.002\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r192100\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD4 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 192100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.001 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.005\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _, h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            l0, lpre = 0, 0\n","            for ig in range(10):\n","                outs = []\n","                for j in range(J):\n","                    outs.append(model(g[:,j]))\n","                out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","                vhat = vhat.detach()\n","                vhat.real = threshold(out)\n","                loss = loss_func(vhat, Rsshatnf.cuda())\n","                optim_gamma.zero_grad()   \n","                loss.backward()\n","                if ig == 0:\n","                    l0 = loss.detach().item()\n","                    lpre = l0\n","                else:\n","                    curr = loss.detach().item()\n","                    l1, l2 = abs(curr-l0), abs(lpre-l0)\n","                    if abs((l2-l1)/l1) < 5e-4 or g.grad.norm() < 5e-4 :\n","                        print(f'gamma break at {ig}, due to loss decreasing slow')\n","                        break\n","                    elif curr.real>lpre.real:\n","                        print(f'gamma break at {ig}, , due to loss increasing')\n","                        break\n","                    lpre = curr\n","                torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n","                optim_gamma.step()\n","                torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r192110\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD4 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 192110 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.001 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.005\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _, h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            l0, lpre = 0, 0\n","            for ig in range(5):\n","                outs = []\n","                for j in range(J):\n","                    outs.append(model(g[:,j]))\n","                out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","                vhat = vhat.detach()\n","                vhat.real = threshold(out)\n","                loss = loss_func(vhat, Rsshatnf.cuda())\n","                optim_gamma.zero_grad()   \n","                loss.backward()\n","                if ig == 0:\n","                    l0 = loss.detach().item()\n","                    lpre = l0\n","                else:\n","                    curr = loss.detach().item()\n","                    l1, l2 = abs(curr-l0), abs(lpre-l0)\n","                    if abs((l2-l1)/l1) < 5e-4 or g.grad.norm() < 5e-4 :\n","                        print(f'gamma break at {ig}, due to loss decreasing slow')\n","                        break\n","                    elif curr.real>lpre.real:\n","                        print(f'gamma break at {ig}, , due to loss increasing')\n","                        break\n","                    lpre = curr\n","                torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n","                optim_gamma.step()\n","                torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r193000\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD5 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 193000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 32  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"markdown","metadata":{},"source":["## Fully neural network \n","the running id starts with v1000"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% reproduce VAE-SS, but not using laplasian, using Gauassian\n","from cmath import rect\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","import pandas as pd\n","from vae_model import LinearBlock\n","\n","\n","rid = \"_vae_repr0\"# running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","class VAE2(nn.Module):\n","    \"\"\"This is MLP version  -- ref VAESS\n","    Input shape [I,MNF], e.g.[32, 3*100*100]\"\"\"\n","    def __init__(self, dimx=784, K=2):\n","        super(VAE2, self).__init__()\n","\n","        self.K = K\n","        self.dz = 20\n","        chans = (700, 600, 500, 400, 300)\n","        # chans = (2560, 2048, 1536, 1024, 512)\n","        self.encoder = nn.Sequential(\n","            LinearBlock(dimx, chans[0]),\n","            LinearBlock(chans[0],chans[1]),\n","            LinearBlock(chans[1],chans[2]),\n","            LinearBlock(chans[2],chans[3]),\n","            LinearBlock(chans[3],chans[4]),\n","            nn.Linear(chans[4], 2*self.dz*K)\n","            )\n","        self.decoder = nn.Sequential(\n","            LinearBlock(self.dz, chans[4]),\n","            LinearBlock(chans[4],chans[3]),\n","            LinearBlock(chans[3],chans[2]),\n","            LinearBlock(chans[2],chans[1]),\n","            LinearBlock(chans[1],chans[0]),\n","            LinearBlock(chans[0],dimx,activation=False),\n","            )\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        \"Encoder and Get latent variable\"\n","        zz = self.encoder(x)\n","        mu = zz[:,::2]\n","        logvar = zz[:,1::2]\n","        z = self.reparameterize(mu, logvar)\n","        \"Decoder\"\n","        sources = self.decoder(z.view(-1,self.dz))\n","        s = sources.view(-1,self.K, x.shape[-1])\n","        x_hat = s.sum(1)\n","\n","        return x_hat, z, mu, logvar, s\n","\n","\n","def pre_mix(lb, d):\n","    d = d.to(torch.float)\n","    ind1 = torch.randperm(d.shape[0])\n","    ind2 = torch.randperm(d.shape[0])\n","    dd = d[ind1] + d[ind2]\n","    lbs = torch.stack([lb[ind1],lb[ind2]], dim=1)\n","    return lbs, dd\n","\n","#%%\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 128\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 5000\n","K = 2\n","\n","dd = pd.read_csv(\"../data/mnist_train.csv\", delimiter=\",\", header=None).values\n","lb, d = torch.from_numpy(dd[:,0]), torch.from_numpy(dd[:,1:])\n","lbs, d = pre_mix(lb, d)\n","xtr = (d/d.abs().amax(dim=1, keepdim=True).to(torch.float32)).cuda() # [sample, D)\n","data = Data.TensorDataset(xtr, lbs)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = VAE2(784, K).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","rec = []\n","for epoch in range(opts['n_epochs']):\n","    for i, (x, y) in enumerate(tr): \n","        optimizer.zero_grad()         \n","        x_hat, z, mu, logvar, s = model(x)\n","        loss = loss_vae(x, x_hat, mu, logvar, 0.5)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        rec.append(loss.detach().cpu().item())\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        if loss.isnan() : print(nan)\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%50 == 0:\n","        print('labels', y[0])\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'id{rid}Loss fuction at epoch{epoch}')\n","        plt.show()\n","\n","        plt.figure()\n","        plt.plot(rec, '-ob')\n","        plt.savefig(fig_loc + f'id{rid}All loss at epoch{epoch}')\n","        plt.show()\n","\n","        plt.figure()\n","        plt.imshow(x[0].cpu().reshape(28,28))\n","        plt.title('first sample GT')\n","        plt.savefig(fig_loc + f'id{rid}first sample GT at epoch{epoch}')\n","        plt.show()\n","\n","        plt.figure()\n","        plt.imshow(x_hat[0].detach().cpu().reshape(28,28))\n","        plt.title('first sample reconstruction')\n","        plt.savefig(fig_loc + f'id{rid}first sample reconstruction at epoch{epoch}')\n","        plt.show()\n","\n","        plt.figure()\n","        plt.imshow(s[0,0].detach().cpu().reshape(28,28))\n","        plt.title('first sample of estimated channel 1')\n","        plt.savefig(fig_loc + f'id{rid}first sample of estimated channel 1 at epoch{epoch}')\n","        plt.show()\n","\n","        plt.figure()\n","        plt.imshow(s[0,1].detach().cpu().reshape(28,28))\n","        plt.title('first sample of estimated channel 2')\n","        plt.savefig(fig_loc + f'id{rid}first sample of estimated channel 2 at epoch{epoch}')\n","        plt.show()\n","\n","        plt.close('all')\n","\n","print('Done at ', datetime.now())\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% VAE-SS only mix two classes 0and 5\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","import pandas as pd\n","from vae_model import LinearBlock\n","\n","\n","rid = \"_vae_repr1\"# running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","class VAE2(nn.Module):\n","    \"\"\"This is MLP version  -- ref VAESS\n","    Input shape [I,MNF], e.g.[32, 3*100*100]\"\"\"\n","    def __init__(self, dimx=784, K=2):\n","        super(VAE2, self).__init__()\n","\n","        self.K = K\n","        self.dz = 20\n","        chans = (700, 600, 500, 400, 300)\n","        # chans = (2560, 2048, 1536, 1024, 512)\n","        self.encoder = nn.Sequential(\n","            LinearBlock(dimx, chans[0]),\n","            LinearBlock(chans[0],chans[1]),\n","            LinearBlock(chans[1],chans[2]),\n","            LinearBlock(chans[2],chans[3]),\n","            LinearBlock(chans[3],chans[4]),\n","            nn.Linear(chans[4], 2*self.dz*K)\n","            )\n","        self.decoder = nn.Sequential(\n","            LinearBlock(self.dz, chans[4]),\n","            LinearBlock(chans[4],chans[3]),\n","            LinearBlock(chans[3],chans[2]),\n","            LinearBlock(chans[2],chans[1]),\n","            LinearBlock(chans[1],chans[0]),\n","            LinearBlock(chans[0],dimx,activation=False),\n","            )\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        \"Encoder and Get latent variable\"\n","        zz = self.encoder(x)\n","        mu = zz[:,::2]\n","        logvar = zz[:,1::2]\n","        z = self.reparameterize(mu, logvar)\n","        \"Decoder\"\n","        sources = self.decoder(z.view(-1,self.dz))\n","        s = sources.view(-1,self.K, x.shape[-1])\n","        x_hat = s.sum(1)\n","\n","        return x_hat, z, mu, logvar, s\n","\n","def pre_mix(d1, d2):\n","    d1 = d1.to(torch.float)\n","    d2 = d2.to(torch.float)\n","    ind1 = torch.randperm(d1.shape[0])\n","    ind2 = torch.randperm(d2.shape[0])\n","    dd = d1[ind1] + d2[ind2]\n","    return dd\n","\n","#%%\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 128\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 2000\n","K = 2\n","\n","dd = pd.read_csv(\"../data/mnist_train.csv\", delimiter=\",\", header=None).values\n","lb, d = torch.from_numpy(dd[:,0]), torch.from_numpy(dd[:,1:])\n","ind0 = (lb == 0).nonzero(as_tuple=True)[0]\n","ind5 = (lb == 5).nonzero(as_tuple=True)[0]\n","d1, d2 = d[ind0][:5000], d[ind5][:5000]\n","dd = []\n","for i in range(6):\n","    dd.append(pre_mix(d1, d2))\n","d = torch.cat(dd, dim=0)\n","xtr = (d/d.abs().amax(dim=1, keepdim=True).to(torch.float32)).cuda() # [sample, D)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = VAE2(784, K).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","rec = []\n","for epoch in range(opts['n_epochs']):\n","    for i, (x, ) in enumerate(tr): \n","        optimizer.zero_grad()         \n","        x_hat, z, mu, logvar, s = model(x)\n","        loss = loss_vae(x, x_hat, mu, logvar, 0.5)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        rec.append(loss.detach().cpu().item())\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        if loss.isnan() : print(nan)\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%50 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'epoch{epoch}_Loss fuction')\n","        plt.show()\n","\n","        plt.figure()\n","        plt.plot(rec, '-ob')\n","        plt.savefig(fig_loc + f'epoch{epoch}_All loss')\n","        plt.show()\n","\n","        plt.figure()\n","        plt.imshow(x[0].cpu().reshape(28,28))\n","        plt.title('first sample GT')\n","        plt.savefig(fig_loc + f'epoch{epoch}_first sample GT')\n","        plt.show()\n","\n","        plt.figure()\n","        plt.imshow(x_hat[0].detach().cpu().reshape(28,28))\n","        plt.title('first sample reconstruction')\n","        plt.savefig(fig_loc + f'epoch{epoch}_first sample reconstruction')\n","        plt.show()\n","\n","        plt.figure()\n","        plt.imshow(s[0,0].detach().cpu().reshape(28,28))\n","        plt.title('first sample of estimated channel 1')\n","        plt.savefig(fig_loc + f'epoch{epoch}_first sample of estimated channel 1')\n","        plt.show()\n","\n","        plt.figure()\n","        plt.imshow(s[0,1].detach().cpu().reshape(28,28))\n","        plt.title('first sample of estimated channel 2')\n","        plt.savefig(fig_loc + f'epoch{epoch}_first sample of estimated channel 2')\n","        plt.show()\n","\n","        plt.close('all')\n","\n","print('Done at ', datetime.now())\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% only mix 1and 4\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","import pandas as pd\n","from vae_model import LinearBlock\n","\n","\n","rid = \"_vae_repr2\"# running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","class VAE2(nn.Module):\n","    \"\"\"This is MLP version  -- ref VAESS\n","    Input shape [I,MNF], e.g.[32, 3*100*100]\"\"\"\n","    def __init__(self, dimx=784, K=2):\n","        super(VAE2, self).__init__()\n","\n","        self.K = K\n","        self.dz = 20\n","        chans = (700, 600, 500, 400, 300)\n","        # chans = (2560, 2048, 1536, 1024, 512)\n","        self.encoder = nn.Sequential(\n","            LinearBlock(dimx, chans[0]),\n","            LinearBlock(chans[0],chans[1]),\n","            LinearBlock(chans[1],chans[2]),\n","            LinearBlock(chans[2],chans[3]),\n","            LinearBlock(chans[3],chans[4]),\n","            nn.Linear(chans[4], 2*self.dz*K)\n","            )\n","        self.decoder = nn.Sequential(\n","            LinearBlock(self.dz, chans[4]),\n","            LinearBlock(chans[4],chans[3]),\n","            LinearBlock(chans[3],chans[2]),\n","            LinearBlock(chans[2],chans[1]),\n","            LinearBlock(chans[1],chans[0]),\n","            LinearBlock(chans[0],dimx,activation=False),\n","            )\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        \"Encoder and Get latent variable\"\n","        zz = self.encoder(x)\n","        mu = zz[:,::2]\n","        logvar = zz[:,1::2]\n","        z = self.reparameterize(mu, logvar)\n","        \"Decoder\"\n","        sources = self.decoder(z.view(-1,self.dz))\n","        s = sources.view(-1,self.K, x.shape[-1])\n","        x_hat = s.sum(1)\n","\n","        return x_hat, z, mu, logvar, s\n","\n","def pre_mix(d1, d2):\n","    d1 = d1.to(torch.float)\n","    d2 = d2.to(torch.float)\n","    ind1 = torch.randperm(d1.shape[0])\n","    ind2 = torch.randperm(d2.shape[0])\n","    dd = d1[ind1] + d2[ind2]\n","    return dd\n","\n","#%%\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 128\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 2000\n","K = 2\n","\n","dd = pd.read_csv(\"../data/mnist_train.csv\", delimiter=\",\", header=None).values\n","lb, d = torch.from_numpy(dd[:,0]), torch.from_numpy(dd[:,1:])\n","ind0 = (lb == 1).nonzero(as_tuple=True)[0]\n","ind5 = (lb == 4).nonzero(as_tuple=True)[0]\n","d1, d2 = d[ind0][:5000], d[ind5][:5000]\n","dd = []\n","for i in range(6):\n","    dd.append(pre_mix(d1, d2))\n","d = torch.cat(dd, dim=0)\n","xtr = (d/d.abs().amax(dim=1, keepdim=True).to(torch.float32)).cuda() # [sample, D)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = VAE2(784, K).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","rec = []\n","for epoch in range(opts['n_epochs']):\n","    for i, (x, ) in enumerate(tr): \n","        optimizer.zero_grad()         \n","        x_hat, z, mu, logvar, s = model(x)\n","        loss = loss_vae(x, x_hat, mu, logvar, 0.5)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        rec.append(loss.detach().cpu().item())\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        if loss.isnan() : print(nan)\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%50 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'epoch{epoch}_Loss fuction')\n","        plt.show()\n","\n","        plt.figure()\n","        plt.plot(rec, '-ob')\n","        plt.savefig(fig_loc + f'epoch{epoch}_All loss')\n","        plt.show()\n","\n","        plt.figure()\n","        plt.imshow(x[0].cpu().reshape(28,28))\n","        plt.title('first sample GT')\n","        plt.savefig(fig_loc + f'epoch{epoch}_first sample GT')\n","        plt.show()\n","\n","        plt.figure()\n","        plt.imshow(x_hat[0].detach().cpu().reshape(28,28))\n","        plt.title('first sample reconstruction')\n","        plt.savefig(fig_loc + f'epoch{epoch}_first sample reconstruction')\n","        plt.show()\n","\n","        plt.figure()\n","        plt.imshow(s[0,0].detach().cpu().reshape(28,28))\n","        plt.title('first sample of estimated channel 1')\n","        plt.savefig(fig_loc + f'epoch{epoch}_first sample of estimated channel 1')\n","        plt.show()\n","\n","        plt.figure()\n","        plt.imshow(s[0,1].detach().cpu().reshape(28,28))\n","        plt.title('first sample of estimated channel 2')\n","        plt.savefig(fig_loc + f'epoch{epoch}_first sample of estimated channel 2')\n","        plt.show()\n","\n","        plt.close('all')\n","\n","print('Done at ', datetime.now())\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10000\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN0 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb.to(torch.cfloat)\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10000' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(hgt).to(torch.cfloat).repeat(I,1,1)\n","Rstr = torch.ones(I,N,F,M).diag_embed().to(torch.cfloat)\n","Rbtr = torch.zeros(I, M, M).to(torch.cfloat)\n","\n","loss_iter, loss_tr = [], []\n","model = NN(3,3,100).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        h = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        rs = Rstr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","\n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x, h, rs, rb)\n","        loss = loss_fun(x, Rs.to(torch.cfloat), Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","\n","        with torch.no_grad():\n","            Rstr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rs.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","            \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'id{rid}_last50_epoch{epoch}')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach().to(torch.cfloat) \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach().to(torch.cfloat)[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(J):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii].abs())\n","            plt.title(f'estimated sources-{ii} at {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_estimated sources-{ii} at {epoch}')\n","            plt.show()\n","            plt.close()\n","        print(h_corr(hh.cpu(), torch.tensor(hgt)))\n","\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10100\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN1 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10100' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(torch.cat((x.real, x.imag), dim=1))\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'id{rid}_last50_epoch{epoch}')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii].abs())\n","            plt.title(f'estimated sources-{ii} at {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_estimated sources-{ii} at {epoch}')\n","            plt.show()\n","            plt.close()\n","        print(h_corr(hh.cpu(), torch.tensor(hgt)))\n","\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10101\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN1 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10101' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 5000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(torch.cat((x.real, x.imag), dim=1))\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","            plt.close()\n","        print(h_corr(hh.cpu(), torch.tensor(hgt)))\n","\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10102\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN1 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10102' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 5000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(torch.cat((x.real, x.imag), dim=1))\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","            plt.close()\n","        print(h_corr(hh.cpu(), torch.tensor(hgt)))\n","\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10110\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN1 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10110' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 6000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')# [sample,M,N,F]\n","xtr0 = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).to(torch.cfloat) \n","d = torch.load('../data/nem_ss/tr3kM3J02FT100.pt')\n","xtr1 = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).to(torch.cfloat)\n","d = torch.load('../data/nem_ss/tr3kM3J05FT100.pt')\n","xtr2 = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).to(torch.cfloat)\n","d = torch.load('../data/nem_ss/tr3kM3J25FT100.pt')\n","xtr3 = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).to(torch.cfloat) \n","xtr = torch.cat((xtr0, xtr1[:1000], xtr2[:1000], xtr3[:1000]), dim=0)\n","\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(torch.cat((x.real, x.imag), dim=1))\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'id{rid}_last50_epoch{epoch}')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii].abs())\n","            plt.title(f'estimated sources-{ii} at {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_estimated sources-{ii} at {epoch}')\n","            plt.show()\n","            plt.close('all')\n","        print(h_corr(hh.cpu(), torch.tensor(hgt)))\n","\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10120\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN1 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10120' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 6000 # how many samples\n","M, N, F, K = 3, 100, 100, 2\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 500\n","\n","# d = torch.load('../data/nem_ss/tr3kM3FT100.pt')# [sample,M,N,F]\n","# xtr0 = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).to(torch.cfloat) \n","d = torch.load('../data/nem_ss/tr3kM3J02FT100.pt')\n","xtr1 = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).to(torch.cfloat)\n","d = torch.load('../data/nem_ss/tr3kM3J05FT100.pt')\n","xtr2 = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).to(torch.cfloat)\n","d = torch.load('../data/nem_ss/tr3kM3J25FT100.pt')\n","xtr3 = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).to(torch.cfloat) \n","xtr = torch.cat((xtr1[:1000], xtr2[:1000], xtr3[:1000]), dim=0)\n","\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(torch.cat((x.real, x.imag), dim=1))\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'id{rid}_last50_epoch{epoch}')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii].abs())\n","            plt.title(f'estimated sources-{ii} at {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_estimated sources-{ii} at {epoch}')\n","            plt.show()\n","            plt.close('all')\n","        print(h_corr(hh.cpu(), torch.tensor(hgt)))\n","\n","print('done')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10130\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN1 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10130' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 6000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 500\n","\n","# d = torch.load('../data/nem_ss/tr3kM3FT100.pt')# [sample,M,N,F]\n","# xtr0 = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).to(torch.cfloat) \n","d = torch.load('../data/nem_ss/tr3kM3J02FT100.pt')\n","xtr1 = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).to(torch.cfloat)\n","d = torch.load('../data/nem_ss/tr3kM3J05FT100.pt')\n","xtr2 = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).to(torch.cfloat)\n","d = torch.load('../data/nem_ss/tr3kM3J25FT100.pt')\n","xtr3 = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).to(torch.cfloat) \n","xtr = torch.cat((xtr1[:1000], xtr2[:1000], xtr3[:1000]), dim=0)\n","\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(torch.cat((x.real, x.imag), dim=1))\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'id{rid}_last50_epoch{epoch}')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii].abs())\n","            plt.title(f'estimated sources-{ii} at {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_estimated sources-{ii} at {epoch}')\n","            plt.show()\n","            plt.close('all')\n","        print(h_corr(hh.cpu(), torch.tensor(hgt)))\n","\n","print('done')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10140\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN1 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10140' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 5000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(torch.cat((x.real, x.imag), dim=1))\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","            plt.close()\n","        print(h_corr(hh.cpu(), torch.tensor(hgt)))\n","\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10150\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN1 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.1):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10150' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 5000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(torch.cat((x.real, x.imag), dim=1))\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","            plt.close()\n","        print(h_corr(hh.cpu(), torch.tensor(hgt)))\n","\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10160\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN1 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=10):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10160' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 5000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(torch.cat((x.real, x.imag), dim=1))\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","            plt.close()\n","        print(h_corr(hh.cpu(), torch.tensor(hgt)))\n","\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10170\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN1 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.01):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10170' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 5000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(torch.cat((x.real, x.imag), dim=1))\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","            plt.close()\n","        print(h_corr(hh.cpu(), torch.tensor(hgt)))\n","\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10200\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN1 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10200' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 3000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hgt = torch.tensor(hgt).to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, _, Rb, mu, logvar= model(torch.cat((x.real, x.imag), dim=1))\n","        loss = loss_fun(x, Rs, Hgt, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hgt\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        # print(h_corr(hh.cpu(), torch.tensor(hgt)))\n","\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10201\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN1 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10201' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 3000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hgt = torch.tensor(hgt).to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, _, Rb, mu, logvar= model(torch.cat((x.real, x.imag), dim=1))\n","        loss = loss_fun(x, Rs, Hgt, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%20 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hgt\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","\n","        # print(h_corr(hh.cpu(), torch.tensor(hgt)))\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10210\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN2 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10210' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 3000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hgt = torch.tensor(hgt).to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x, Hgt)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(h_corr(hh.cpu(), torch.tensor(hgt)))\n","\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10211 and v10212(NN2 adde threshold on Rb)\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN2 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10211' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 3000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hgt = torch.tensor(hgt).to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x, Hgt)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(h_corr(hh.cpu(), torch.tensor(hgt)))\n","\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10220\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","\n","from vae_model import *\n","class NN2_(nn.Module):\n","    \"\"\"This is spatial broadcast decoder (SBD) version\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","\n","        # Estimate V\n","        self.dz = 32\n","        self.K, self.M = K, M\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=32),\n","            Down(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            DoubleConv(in_channels=self.dz+2, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=32),\n","            DoubleConv(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=1),\n","            ) \n","\n","        self.im_size = im_size\n","        x = torch.linspace(-1, 1, im_size)\n","        y = torch.linspace(-1, 1, im_size)\n","        x_grid, y_grid = torch.meshgrid(x, y)\n","        # Add as constant, with extra dims for N and C\n","        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n","        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n","\n","        # Estimate H\n","        self.fc_h = nn.Sequential(\n","            LinearBlock(self.dz, 64),\n","            nn.Linear(64, 1),\n","            nn.Tanh()\n","            )   \n","        \n","        # Estimate Rb\n","        self.fc_b = nn.Sequential(\n","            LinearBlock(self.dz*self.K, 64),\n","            nn.Linear(64, 1),\n","            )   \n","    \n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x, h_int):\n","        batch_size = x.shape[0]\n","        z_all, v_all, h_all = [], [], [] \n","        for i in range(self.K):\n","            \"Encoder\"\n","            inp = h_int[:,i:i+1].t().conj()@x.permute(0,2,3,1).unsqueeze(-1)\n","            inp = inp.squeeze().abs()**2\n","            xx = self.encoder(inp[:,None,:,:])\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder1 get V\"\n","            # View z as 4D tensor to be tiled across new N and F dimensions            \n","            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n","            # Tile across to match image size\n","            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n","            # Expand grids to batches and concatenate on the channel dimension\n","            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n","                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n","            v = self.decoder(zbd).exp()\n","            v_all.append(threshold(v, ceiling=1e4)) # 1e-3 to 1e4\n","            \"Decoder2 get H\"\n","            ang = self.fc_h(z)\n","            h_all.append((ang*torch.pi*1j*torch.arange(self.M, device=ang.device)).exp())\n","        \"Decoder3 get sig_b\"\n","        sig_b = self.fc_b(torch.cat(z_all, dim=-1)).exp()\n","\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K], cfloat\n","        Rb = sig_b[:,:,None]**2 * torch.ones(batch_size, \\\n","            self.M, device=sig_b.device).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 'v10220' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 3000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hgt = torch.tensor(hgt).to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr = [], []\n","model = NN2_(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x, Hgt)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(h_corr(hh.cpu(), torch.tensor(hgt)))\n","\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10300\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN3 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10300' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 3000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","Hgt = torch.tensor(hgt).to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(h_corr(hh.cpu(), torch.tensor(hgt)))\n","torch.save(model, 'mod_loc'+f'{rid}_3000epoch.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10301\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN3 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10301' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","Hgt = torch.tensor(hgt).to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(h_corr(hh.cpu(), torch.tensor(hgt)))\n","torch.save(model, 'mod_loc'+f'{rid}_3000epoch.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10302\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN3 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10302' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","Hgt = torch.tensor(hgt).to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(h_corr(hh.cpu(), torch.tensor(hgt)))\n","torch.save(model, 'mod_loc'+f'{rid}_3000epoch.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10303\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN3 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10303' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10310\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN3 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10310' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","Hgt = torch.tensor(hgt).to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(f'done with epoch{epoch}', h_corr(hh.cpu(), torch.tensor(hgt)))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10311\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","from vae_model import NN3 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10311' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10400\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10400' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 9000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(f'done with epoch{epoch}')\n","        torch.save(model, 'mod_loc'+f'{rid}_epoch{epoch}.pt')\n","print('End date time ', datetime.now())\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10402\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10402' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 9000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","Hgt = torch.tensor(hgt).to(torch.cfloat).cuda()\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(f'done with epoch{epoch}', h_corr(hh.cpu(), torch.tensor(hgt)))\n","        torch.save(model, mod_loc+f'{rid}_epoch{epoch}.pt')\n","print('End date time ', datetime.now())\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10405\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10405' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 9000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","Hgt = torch.tensor(hgt).to(torch.cfloat).cuda()\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(f'done with epoch{epoch}', h_corr(hh.cpu(), torch.tensor(hgt)))\n","        torch.save(model, mod_loc+f'{rid}_epoch{epoch}.pt')\n","print('End date time ', datetime.now())\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10415\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10415' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 9000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","Hgt = torch.tensor(hgt).to(torch.cfloat).cuda()\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(f'done with epoch{epoch}', h_corr(hh.cpu(), torch.tensor(hgt)))\n","        torch.save(model, mod_loc+f'{rid}_epoch{epoch}.pt')\n","print('End date time ', datetime.now())\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10500\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN5 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10500' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","Hgt = torch.tensor(hgt).to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(f'done with epoch{epoch}', h_corr(hh.cpu(), torch.tensor(hgt)))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10600\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","from vae_model import NN3_1 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10600' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 100\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)[0]))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10610\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","from vae_model import NN6 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v10610' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 100\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10700\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","from vae_model import NN6_5 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((Rx, Rs, Hhat, Rb), f'rid{rid}_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","\n","rid = 'v10700' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 100\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v10710\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","from vae_model import NN6_5 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((Rx, Rs, Hhat, Rb), f'rid{rid}_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","\n","rid = 'v10710' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 100\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","for w in model.parameters():\n","    nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v11300\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN3 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    # ll, _, _ = log_lh(x.permute(0,2,3,1), \\\n","    #     vhat.to(torch.cfloat), Hhat, Rb.to(torch.cfloat))\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v11300' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 9000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","Hgt = torch.tensor(hgt).to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(h_corr(hh.cpu(), torch.tensor(hgt)))\n","torch.save(model, 'mod_loc'+f'{rid}_3000epoch.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v11301\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v11301' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        torch.save(model, mod_loc+f'model{rid}_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v11302\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v11302' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(f'done with epoch{epoch}')\n","        torch.save(model, mod_loc+f'model{rid}_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v11303\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.1):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v11303' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(f'done with epoch{epoch}')\n","        torch.save(model, mod_loc+f'model{rid}_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v11305\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v11305' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(f'done with epoch{epoch}')\n","        torch.save(model, mod_loc+f'model{rid}_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v11306\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v11306' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 5e-3\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(f'done with epoch{epoch}')\n","        torch.save(model, mod_loc+f'model{rid}_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v11307\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v11307' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-2\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(f'done with epoch{epoch}')\n","        torch.save(model, mod_loc+f'model{rid}_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v11308\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v11308' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 32\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(f'done with epoch{epoch}')\n","        torch.save(model, mod_loc+f'model{rid}_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v11311\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v11311' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(f'done with epoch{epoch}')\n","        torch.save(model, mod_loc+f'model{rid}_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v11312\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v11312' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(f'done with epoch{epoch}')\n","        torch.save(model, mod_loc+f'model{rid}_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v11313\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.1):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v11313' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(f'done with epoch{epoch}')\n","        torch.save(model, mod_loc+f'model{rid}_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v11400\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v11400' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 9000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(f'done with epoch{epoch}')\n","        torch.save(model, 'mod_loc'+f'{rid}_epoch{epoch}.pt')\n","print('End date time ', datetime.now())\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v11401\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v11401' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(f'done with epoch{epoch}')\n","        torch.save(model, mod_loc+f'model{rid}_epoch{epoch}.pt')\n","print('End date time ', datetime.now())\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v11402\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v11402' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(f'done with epoch{epoch}')\n","        torch.save(model, mod_loc+f'model{rid}_epoch{epoch}.pt')\n","print('End date time ', datetime.now())\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v11403\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.1):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v11403' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(f'done with epoch{epoch}')\n","        torch.save(model, mod_loc+f'model{rid}_epoch{epoch}.pt')\n","print('End date time ', datetime.now())\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v11411\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v11411' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(f'done with epoch{epoch}')\n","        torch.save(model, mod_loc+f'model{rid}_epoch{epoch}.pt')\n","print('End date time ', datetime.now())\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v11412\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v11412' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(f'done with epoch{epoch}')\n","        torch.save(model, mod_loc+f'model{rid}_epoch{epoch}.pt')\n","print('End date time ', datetime.now())\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v11413\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.1):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v11413' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","loss_iter, loss_tr = [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item())\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        hh = Hhat[0].detach()\n","        rs0 = Rs[0].detach() \n","        Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n","        shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","        for ii in range(K):\n","            plt.figure()\n","            plt.imshow(shat[:,:,ii,0].abs())\n","            plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","            plt.show()\n","\n","            plt.figure()\n","            plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","            plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","            plt.show()\n","            plt.close('all')\n","        print(f'done with epoch{epoch}')\n","        torch.save(model, mod_loc+f'model{rid}_epoch{epoch}.pt')\n","print('End date time ', datetime.now())\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v12000\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN5 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v12000' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh_ang6915-30.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)[0]))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v12100\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN3 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v12100' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh_ang6915-30.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)[0]))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v12200\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v12200' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh_ang6915-30.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)[0]))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v12201\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(10)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","\n","from vae_model import NN4 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v12201' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh_ang6915-30.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = torch.optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)[0]))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v12300\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","from vae_model import NN6 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v12300' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh_ang6915-30.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)[0]))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v12301\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","from vae_model import NN6 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v12301' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 100\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh_ang6915-30.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)[0]))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v12310\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","from vae_model import NN6 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.1):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v12310' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 100\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh_ang6915-30.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)[0]))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v12320\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","Adam = torch.optim.Adam\n","\n","from vae_model import NN6 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v12320' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 100\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh_ang6915-30.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)[0]))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v12330\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","Adam = torch.optim.Adam\n","\n","from vae_model import NN6 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v12330' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 100\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh_ang6915-30.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)[0]))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v12340\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","AdamP = optim.AdamP\n","\n","from vae_model import NN6 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v12340' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 100\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh_ang6915-30.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = AdamP(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)[0]))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v12350\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","AdamP = optim.AdamP\n","\n","from vae_model import NN6 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v12350' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 100\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh_ang6915-30.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = AdamP(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)[0]))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v12400\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","from vae_model import NN3_1 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v12400' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 100\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh_ang6915-30.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)[0]))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v12500\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(10)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","from vae_model import NN3_2 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v12500' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 100\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh_ang6915-30.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)[0]))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v12600\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","from vae_model import NN6_1 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v12600' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 100\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh_ang6915-30.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)[0]))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v12610\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","from vae_model import NN6_1 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.1):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v12610' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 100\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh_ang6915-30.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)[0]))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v12620\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","Adam = torch.optim.Adam\n","\n","from vae_model import NN6_1 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v12620' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 100\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh_ang6915-30.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)[0]))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v12630\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(10)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","Adam = torch.optim.Adam\n","\n","from vae_model import NN6_1 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v12630' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 100\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh_ang6915-30.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)[0]))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v12640\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","AdamP = optim.AdamP\n","\n","from vae_model import NN6_1 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v12640' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 100\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh_ang6915-30.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = AdamP(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)[0]))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v12650\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","AdamP = optim.AdamP\n","\n","from vae_model import NN6_1 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v12650' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 100\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh_ang6915-30.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","optimizer = AdamP(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)[0]))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v20000\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","#%%\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import NN7 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((Rx, Rs, Hhat, Rb), f'rid{rid}_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v20000' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","for w in model.parameters():\n","    nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(epoch, Rb[0], Rb.sum()/3/128)\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v20010\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","#%%\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import NN7 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((Rx, Rs, Hhat, Rb), f'rid{rid}_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v20010' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","for w in model.parameters():\n","    nn.init.normal_(w, mean=0., std=0.1)\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v20100\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","from vae_model import NN7 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((Rx, Rs, Hhat, Rb), f'rid{rid}_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v20100' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","for w in model.parameters():\n","    nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v20110\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","from vae_model import NN7 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((Rx, Rs, Hhat, Rb), f'rid{rid}_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v20110' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","for w in model.parameters():\n","    nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = torch.optim.Adam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v20200\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","#%%\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import NN7 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((Rx, Rs, Hhat, Rb), f'rid{rid}_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v20200' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","for w in model.parameters():\n","    nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = torch.optim.RMSprop(model.parameters(),\n","                lr= opts['lr'])\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v20210\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","#%%\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import NN7 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((Rx, Rs, Hhat, Rb), f'rid{rid}_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v20210' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","for w in model.parameters():\n","    nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = torch.optim.RMSprop(model.parameters(),\n","                lr= opts['lr'])\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v20220\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","#%%\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import NN7 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((Rx, Rs, Hhat, Rb), f'rid{rid}_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v20220' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","for w in model.parameters():\n","    nn.init.normal_(w, mean=0., std=0.1)\n","optimizer = torch.optim.RMSprop(model.parameters(),\n","                lr= opts['lr'])\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v20230\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","#%%\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import NN7 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((Rx, Rs, Hhat, Rb), f'rid{rid}_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","rid = 'v20230' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","for w in model.parameters():\n","    nn.init.normal_(w, mean=0., std=0.1)\n","optimizer = torch.optim.RMSprop(model.parameters(),\n","                lr= opts['lr'])\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n","        torch.save(model, mod_loc+f'modle_epoch{epoch}.pt')\n","print('done')\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v21000\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import NN7 as NN\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 'v21000' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d, snr=30, seed=1)\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","model = NN(M,K,N).cuda()\n","for w in model.parameters():\n","    nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(epoch, Rb[0], Rb.sum()/3/128)\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')\n","\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v22000\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","class NN8(nn.Module):\n","    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        # Estimate H and coarse V\n","        self.v_net = nn.Sequential(\n","            DoubleConv(in_channels=M*2, out_channels=1),\n","            ) \n","        self.v_out = OutConv(in_channels=1, out_channels=1)\n","        self.hb_net = nn.Sequential(\n","            Down(in_channels=1, out_channels=1),\n","            Down(in_channels=1, out_channels=1),\n","            Down(in_channels=1, out_channels=1),\n","            Reshape(-1, 12*12),\n","            )\n","        # Estimate H\n","        self.h_net = nn.Sequential(\n","            LinearBlock(12*12, 64),\n","            LinearBlock(64, 32),\n","            nn.Linear(32, 1),\n","            nn.Tanh()\n","            )   \n","        # Estimate Rb\n","        self.b_net = nn.Sequential(\n","            LinearBlock(12*12, 64),\n","            LinearBlock(64, 32),\n","            nn.Linear(32, 1),\n","            )   \n","        # Estimate V using auto encoder\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=32),\n","            Down(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            DoubleConv(in_channels=self.dz+2, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=32),\n","            DoubleConv(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=4),\n","            OutConv(in_channels=4, out_channels=1),\n","            ) \n","        self.im_size = im_size\n","        x = torch.linspace(-1, 1, im_size)\n","        y = torch.linspace(-1, 1, im_size)\n","        x_grid, y_grid = torch.meshgrid(x, y)\n","        # Add as constant, with extra dims for N and C\n","        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n","        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                temp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - temp.squeeze().permute(2,3,0,1)\n","            temp = self.v_net(torch.cat((inp.real, inp.imag), dim=1)).exp() \n","            vj = self.v_out(temp).exp() #sigma_s**2 >=0\n","            vj = threshold(vj, floor=1e-3, ceiling=1e3)  # shape of [I, 1, N, F]\n","            hb = self.hb_net(vj)\n","            ang = self.h_net(hb)  # shape of [I,1]\n","            sig_b_squared = self.b_net(hb).exp() # shape of [I,1]\n","            \"Get H\"\n","            ch = torch.pi*torch.arange(self.M, device=ang.device)\n","            hj = ((ang @ ch[None,:])*1j).exp() # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Get Rb, the energy of the rest\"\n","            Rb = threshold(sig_b_squared[:,:,None]**2, 1e-3, 1e2)*torch.ones(batch_size, \\\n","                self.M, device=ch.device).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rs = vj.permute(2,3,0,1)[..., None].to(torch.cfloat)  #shape of [N,F,I,1,1]\n","            Rx = hj[...,None] @ Rs @ hj[:,None].conj() + Rb # shape of [N,F,I,M,M]\n","            W = Rs @ hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","        \n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            # View z as 4D tensor to be tiled across new N and F dimensions            \n","            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n","            # Tile across to match image size\n","            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n","            # Expand grids to batches and concatenate on the channel dimension\n","            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n","                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n","            v = self.decoder(zbd).exp()\n","            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","\n","#%%\n","rid = 'v22000' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 100\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d, snr=30, seed=1)\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","NN = NN8\n","model = NN(M,K,N).cuda()\n","for w in model.parameters():\n","    nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(f'epoch{epoch}', Rb[0], Rb.sum()/3/128)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v22100\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","class NN8(nn.Module):\n","    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        # Estimate H and coarse V\n","        self.v_net = nn.Sequential(\n","            DoubleConv(in_channels=M*2, out_channels=1),\n","            ) \n","        self.v_out = OutConv(in_channels=1, out_channels=1)\n","        self.hb_net = nn.Sequential(\n","            Down(in_channels=1, out_channels=1),\n","            Down(in_channels=1, out_channels=1),\n","            Down(in_channels=1, out_channels=1),\n","            Reshape(-1, 12*12),\n","            )\n","        # Estimate H\n","        self.h_net = nn.Sequential(\n","            LinearBlock(12*12, 64),\n","            LinearBlock(64, 32),\n","            nn.Linear(32, 1),\n","            nn.Tanh()\n","            )   \n","        # Estimate Rb\n","        self.b_net = nn.Sequential(\n","            LinearBlock(12*12, 64),\n","            LinearBlock(64, 32),\n","            nn.Linear(32, 1),\n","            )   \n","        # Estimate V using auto encoder\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=32),\n","            Down(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            DoubleConv(in_channels=self.dz+2, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=32),\n","            DoubleConv(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=4),\n","            OutConv(in_channels=4, out_channels=1),\n","            ) \n","        self.im_size = im_size\n","        x = torch.linspace(-1, 1, im_size)\n","        y = torch.linspace(-1, 1, im_size)\n","        x_grid, y_grid = torch.meshgrid(x, y)\n","        # Add as constant, with extra dims for N and C\n","        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n","        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                temp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - temp.squeeze().permute(2,3,0,1)\n","            temp = self.v_net(torch.cat((inp.real, inp.imag), dim=1)).exp() \n","            vj = self.v_out(temp).exp() #sigma_s**2 >=0\n","            vj = threshold(vj, floor=1e-3, ceiling=1e3)  # shape of [I, 1, N, F]\n","            hb = self.hb_net(vj)\n","            ang = self.h_net(hb)  # shape of [I,1]\n","            sig_b_squared = self.b_net(hb).exp() # shape of [I,1]\n","            \"Get H\"\n","            ch = torch.pi*torch.arange(self.M, device=ang.device)\n","            hj = ((ang @ ch[None,:])*1j).exp() # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Get Rb, the energy of the rest\"\n","            Rb = threshold(sig_b_squared[:,:,None]**2, 1e-3, 1e2)*torch.ones(batch_size, \\\n","                self.M, device=ch.device).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rs = vj.permute(2,3,0,1)[..., None].to(torch.cfloat)  #shape of [N,F,I,1,1]\n","            Rx = hj[...,None] @ Rs @ hj[:,None].conj() + Rb # shape of [N,F,I,M,M]\n","            W = Rs @ hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","        \n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            # View z as 4D tensor to be tiled across new N and F dimensions            \n","            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n","            # Tile across to match image size\n","            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n","            # Expand grids to batches and concatenate on the channel dimension\n","            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n","                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n","            v = self.decoder(zbd).exp()\n","            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","\n","#%%\n","rid = 'v22100' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 100\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d, snr=30, seed=1)\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","NN = NN8\n","model = NN(M,K,N).cuda()\n","for w in model.parameters():\n","    nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(f'epoch{epoch}', Rb[0], Rb.sum()/3/128)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v23000\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","\n","class NN9(nn.Module):\n","    \"\"\"This is recursive Wiener filter version\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        # Estimate H and coarse V\n","        self.v_net = nn.Sequential(\n","            DoubleConv(in_channels=M*2, out_channels=32),\n","            DoubleConv(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=4),\n","            ) \n","        self.v_out = OutConv(in_channels=4, out_channels=1)\n","        self.hb_net = nn.Sequential(\n","            Down(in_channels=1, out_channels=32),\n","            Down(in_channels=32, out_channels=16),\n","            Down(in_channels=16, out_channels=8),\n","            Reshape(-1, 8*12*12),\n","            )\n","        # Estimate H\n","        self.h_net = nn.Sequential(\n","            LinearBlock(8*12*12, 64),\n","            nn.Linear(64, 32),\n","            nn.Linear(32, 1),\n","            nn.Tanh()\n","            )   \n","        # Estimate Rb\n","        self.b_net = nn.Sequential(\n","            LinearBlock(8*12*12, 64),\n","            nn.Linear(64, 32),\n","            nn.Linear(32, 1),\n","            )   \n","        # Estimate V using auto encoder\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=32),\n","            Down(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            DoubleConv(in_channels=self.dz+2, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=32),\n","            DoubleConv(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=4),\n","            OutConv(in_channels=4, out_channels=1),\n","            ) \n","        self.im_size = im_size\n","        x = torch.linspace(-1, 1, im_size)\n","        y = torch.linspace(-1, 1, im_size)\n","        x_grid, y_grid = torch.meshgrid(x, y)\n","        # Add as constant, with extra dims for N and C\n","        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n","        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - tmp.squeeze().permute(2,3,0,1)\n","            temp = self.v_net(torch.cat((inp.real, inp.imag), dim=1)).exp() \n","            vj = self.v_out(temp).exp() #sigma_s**2 >=0\n","            vj = threshold(vj, floor=1e-3, ceiling=1e2)  # shape of [I, 1, N, F]\n","            hb = self.hb_net(vj)\n","            ang = self.h_net(hb)  # shape of [I,1]\n","            sig_b_squared = self.b_net(hb).exp() # shape of [I,1]\n","            \"Get H\"\n","            ch = torch.pi*torch.arange(self.M, device=ang.device)\n","            hj = ((ang @ ch[None,:])*1j).exp() # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Get Rb, the energy of the rest\"\n","            Rb = sig_b_squared[:,:,None]*torch.ones(batch_size, \\\n","                self.M, device=ch.device).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rs = vj.mean(dim=(2,3)).to(torch.cfloat)[..., None]  #shape of [I,1,1]\n","            Rx = hj[...,None] @ Rs @ hj[:,None].conj() + Rb # shape of [I,M,M]\n","            W = Rs @ hj[:, None,].conj() @ Rx.inverse()  # shape of [I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","        \n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            # View z as 4D tensor to be tiled across new N and F dimensions            \n","            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n","            # Tile across to match image size\n","            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n","            # Expand grids to batches and concatenate on the channel dimension\n","            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n","                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n","            v = self.decoder(zbd).exp()\n","            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 'v23000' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d, snr=30, seed=1)\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","NN = NN9\n","model = NN(M,K,N).cuda()\n","for w in model.parameters():\n","    nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(epoch, Rb[0], Rb.sum()/3/128)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            \n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')\n","\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v23100\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","\n","class NN10(nn.Module):\n","    \"\"\"This is recursive Wiener filter version\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        # Estimate H and coarse V\n","        self.v_net = nn.Sequential(\n","            DoubleConv(in_channels=M*2, out_channels=1),\n","            ) \n","        self.v_out = OutConv(in_channels=1, out_channels=1)\n","        self.hb_net = nn.Sequential(\n","            Down(in_channels=1, out_channels=1),\n","            Down(in_channels=1, out_channels=1),\n","            Down(in_channels=1, out_channels=1),\n","            Reshape(-1, 12*12),\n","            )\n","        # Estimate H\n","        self.h_net = nn.Sequential(\n","            LinearBlock(12*12, 64),\n","            LinearBlock(64, 32),\n","            nn.Linear(32, 1),\n","            nn.Tanh()\n","            )   \n","        # Estimate Rb\n","        self.b_net = nn.Sequential(\n","            LinearBlock(12*12, 64),\n","            LinearBlock(64, 32),\n","            nn.Linear(32, 1),\n","            )   \n","        # Estimate V using auto encoder\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=32),\n","            Down(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            DoubleConv(in_channels=self.dz+2, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=32),\n","            DoubleConv(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=4),\n","            OutConv(in_channels=4, out_channels=1),\n","            ) \n","        self.im_size = im_size\n","        x = torch.linspace(-1, 1, im_size)\n","        y = torch.linspace(-1, 1, im_size)\n","        x_grid, y_grid = torch.meshgrid(x, y)\n","        # Add as constant, with extra dims for N and C\n","        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n","        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - tmp.squeeze().permute(2,3,0,1)\n","            temp = self.v_net(torch.cat((inp.real, inp.imag), dim=1)).exp() \n","            vj = self.v_out(temp).exp() #sigma_s**2 >=0\n","            vj = threshold(vj, floor=1e-3, ceiling=1e2)  # shape of [I, 1, N, F]\n","            hb = self.hb_net(vj)\n","            ang = self.h_net(hb)  # shape of [I,1]\n","            sig_b_squared = self.b_net(hb).exp() # shape of [I,1]\n","            \"Get H\"\n","            ch = torch.pi*torch.arange(self.M, device=ang.device)\n","            hj = ((ang @ ch[None,:])*1j).exp() # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Get Rb, the energy of the rest\"\n","            Rb = sig_b_squared[:,:,None]*torch.ones(batch_size, \\\n","                self.M, device=ch.device).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rs = vj.mean(dim=(2,3)).to(torch.cfloat)[..., None]  #shape of [I,1,1]\n","            Rx = hj[...,None] @ Rs @ hj[:,None].conj() + Rb # shape of [I,M,M]\n","            W = Rs @ hj[:, None,].conj() @ Rx.inverse()  # shape of [I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","        \n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            # View z as 4D tensor to be tiled across new N and F dimensions            \n","            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n","            # Tile across to match image size\n","            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n","            # Expand grids to batches and concatenate on the channel dimension\n","            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n","                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n","            v = self.decoder(zbd).exp()\n","            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 'v23100' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d, snr=30, seed=1)\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","NN = NN10\n","model = NN(M,K,N).cuda()\n","for w in model.parameters():\n","    nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(epoch, Rb[0], Rb.sum()/3/128)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            \n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')\n","\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v23200\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","\n","class NN11(nn.Module):\n","    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        # Estimate H and coarse V\n","        self.est = nn.Sequential(\n","            Down(in_channels=M*2, out_channels=64),\n","            Down(in_channels=64, out_channels=32),\n","            Down(in_channels=32, out_channels=8),\n","            Reshape(-1, 8*12*12),\n","            LinearBlock(8*12*12, 64),\n","            nn.Linear(64, 3),\n","            )   \n","        # Estimate V using auto encoder\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=32),\n","            Down(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            DoubleConv(in_channels=self.dz+2, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=32),\n","            DoubleConv(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=4),\n","            OutConv(in_channels=4, out_channels=1),\n","            ) \n","        self.im_size = im_size\n","        x = torch.linspace(-1, 1, im_size)\n","        y = torch.linspace(-1, 1, im_size)\n","        x_grid, y_grid = torch.meshgrid(x, y)\n","        # Add as constant, with extra dims for N and C\n","        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n","        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - tmp.squeeze().permute(2,3,0,1)\n","            res = self.est(torch.cat((inp.real, inp.imag), dim=1)) #vj,Rb,ang\n","            vj = threshold(res[:, 0:1].exp(), floor=1e-3, ceiling=1e2)\n","            sb = threshold(res[:, 1:2].exp(), floor=1e-3, ceiling=1e2)\n","            Rb = (sb*torch.ones(batch_size, self.M, \\\n","                device=sb.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","\n","            ch = torch.pi*torch.arange(self.M, device=res.device)\n","            hj = ((res[:, 2:].tanh() @ ch[None,:])*1j).exp() # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rs = vj[..., None].to(torch.cfloat)  #shape of [I,1,1]\n","            Rx = hj[...,None] @ Rs @ hj[:,None].conj() + Rb # shape of [I,M,M]\n","            W = Rs @ hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","        \n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            # View z as 4D tensor to be tiled across new N and F dimensions            \n","            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n","            # Tile across to match image size\n","            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n","            # Expand grids to batches and concatenate on the channel dimension\n","            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n","                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n","            v = self.decoder(zbd).exp()\n","            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 'v23200' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d, snr=30, seed=1)\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = d = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","NN = NN11\n","model = NN(M,K,N).cuda()\n","for w in model.parameters():\n","    nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(epoch, Rb[0], Rb.sum()/3/128)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)))\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            \n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')\n","\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v23300\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","class NN12(nn.Module):\n","    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        # Estimate H and coarse V\n","        self.est = nn.Sequential(\n","            Down(in_channels=M*2, out_channels=64),\n","            Down(in_channels=64, out_channels=32),\n","            Down(in_channels=32, out_channels=4),\n","            Reshape(-1, 4*12*12),\n","            LinearBlock(4*12*12, 64),\n","            nn.Linear(64, 1),\n","            )\n","        self.b1 = nn.Linear(100, 1)\n","        self.b2 = nn.Linear(100, 1)\n","           \n","        # Estimate V using auto encoder\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=32),\n","            Down(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            DoubleConv(in_channels=self.dz+2, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=32),\n","            DoubleConv(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=4),\n","            OutConv(in_channels=4, out_channels=1),\n","            ) \n","        self.im_size = im_size\n","        x = torch.linspace(-1, 1, im_size)\n","        y = torch.linspace(-1, 1, im_size)\n","        x_grid, y_grid = torch.meshgrid(x, y)\n","        # Add as constant, with extra dims for N and C\n","        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n","        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - tmp.squeeze().permute(2,3,0,1)\n","            ang = self.est(torch.cat((inp.real, inp.imag), dim=1)) #vj,Rb,ang\n","\n","            sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n","            Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n","                device=sb.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","\n","            ch = torch.pi*torch.arange(self.M, device=inp.device)\n","            hj = ((ang.tanh() @ ch[None,:])*1j).exp() # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n","            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","        \n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            # View z as 4D tensor to be tiled across new N and F dimensions            \n","            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n","            # Tile across to match image size\n","            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n","            # Expand grids to batches and concatenate on the channel dimension\n","            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n","                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n","            v = self.decoder(zbd).exp()\n","            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 'v23300' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d, snr=30, seed=1)\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","NN = NN12\n","model = NN(M,K,N).cuda()\n","for w in model.parameters():\n","    nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(epoch, Rb[0], Rb.sum()/3/128, hh)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)), '\\n')\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            \n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')\n","\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v23310\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","class NN12(nn.Module):\n","    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        # Estimate H and coarse V\n","        self.est = nn.Sequential(\n","            Down(in_channels=M*2, out_channels=64),\n","            Down(in_channels=64, out_channels=32),\n","            Down(in_channels=32, out_channels=4),\n","            Reshape(-1, 4*12*12),\n","            LinearBlock(4*12*12, 64),\n","            nn.Linear(64, 1),\n","            )\n","        self.b1 = nn.Linear(100, 1)\n","        self.b2 = nn.Linear(100, 1)\n","           \n","        # Estimate V using auto encoder\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=32),\n","            Down(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            DoubleConv(in_channels=self.dz+2, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=32),\n","            DoubleConv(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=4),\n","            OutConv(in_channels=4, out_channels=1),\n","            ) \n","        self.im_size = im_size\n","        x = torch.linspace(-1, 1, im_size)\n","        y = torch.linspace(-1, 1, im_size)\n","        x_grid, y_grid = torch.meshgrid(x, y)\n","        # Add as constant, with extra dims for N and C\n","        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n","        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - tmp.squeeze().permute(2,3,0,1)\n","            ang = self.est(torch.cat((inp.real, inp.imag), dim=1)) #vj,Rb,ang\n","\n","            sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n","            Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n","                device=sb.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","\n","            ch = torch.pi*torch.arange(self.M, device=inp.device)\n","            hj = ((ang.tanh() @ ch[None,:])*1j).exp() # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n","            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","        \n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            # View z as 4D tensor to be tiled across new N and F dimensions            \n","            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n","            # Tile across to match image size\n","            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n","            # Expand grids to batches and concatenate on the channel dimension\n","            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n","                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n","            v = self.decoder(zbd).exp()\n","            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 'v23310' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d, snr=30, seed=1)\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","NN = NN12\n","model = NN(M,K,N).cuda()\n","for w in model.parameters():\n","    nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(epoch, Rb[0], Rb.sum()/3/128, hh)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)), '\\n')\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            \n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')\n","\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v23400\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","class NN13(nn.Module):\n","    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        # Estimate H and coarse V\n","        self.est = nn.Sequential(\n","            Down(in_channels=M*2, out_channels=64),\n","            Down(in_channels=64, out_channels=32),\n","            Down(in_channels=32, out_channels=4),\n","            Reshape(-1, 4*12*12),\n","            LinearBlock(4*12*12, 64),\n","            nn.Linear(64, 1),\n","            )\n","        self.b1 = nn.Linear(100, 1)\n","        self.b2 = nn.Linear(100, 1)\n","           \n","        # Estimate V using auto encoder\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            Down(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            DoubleConv(in_channels=self.dz+2, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            ) \n","        self.im_size = im_size\n","        x = torch.linspace(-1, 1, im_size)\n","        y = torch.linspace(-1, 1, im_size)\n","        x_grid, y_grid = torch.meshgrid(x, y)\n","        # Add as constant, with extra dims for N and C\n","        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n","        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - tmp.squeeze().permute(2,3,0,1)\n","            ang = self.est(torch.cat((inp.real, inp.imag), dim=1)) #vj,Rb,ang\n","\n","            sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n","            Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n","                device=sb.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","\n","            ch = torch.pi*torch.arange(self.M, device=inp.device)\n","            hj = ((ang.tanh() @ ch[None,:])*1j).exp() # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n","            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","        \n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            # View z as 4D tensor to be tiled across new N and F dimensions            \n","            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n","            # Tile across to match image size\n","            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n","            # Expand grids to batches and concatenate on the channel dimension\n","            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n","                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n","            v = self.decoder(zbd).exp()\n","            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 'v23400' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d, snr=30, seed=1)\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","NN = NN13\n","model = NN(M,K,N).cuda()\n","for w in model.parameters():\n","    nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(epoch, Rb[0], Rb.sum()/3/128, hh)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)), '\\n')\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            \n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')\n","\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v23410\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","class NN13(nn.Module):\n","    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        # Estimate H and coarse V\n","        self.est = nn.Sequential(\n","            Down(in_channels=M*2, out_channels=64),\n","            Down(in_channels=64, out_channels=32),\n","            Down(in_channels=32, out_channels=4),\n","            Reshape(-1, 4*12*12),\n","            LinearBlock(4*12*12, 64),\n","            nn.Linear(64, 1),\n","            )\n","        self.b1 = nn.Linear(100, 1)\n","        self.b2 = nn.Linear(100, 1)\n","           \n","        # Estimate V using auto encoder\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            Down(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            DoubleConv(in_channels=self.dz+2, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            ) \n","        self.im_size = im_size\n","        x = torch.linspace(-1, 1, im_size)\n","        y = torch.linspace(-1, 1, im_size)\n","        x_grid, y_grid = torch.meshgrid(x, y)\n","        # Add as constant, with extra dims for N and C\n","        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n","        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - tmp.squeeze().permute(2,3,0,1)\n","            ang = self.est(torch.cat((inp.real, inp.imag), dim=1)) #vj,Rb,ang\n","\n","            sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n","            Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n","                device=sb.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","\n","            ch = torch.pi*torch.arange(self.M, device=inp.device)\n","            hj = ((ang.tanh() @ ch[None,:])*1j).exp() # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n","            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","        \n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            # View z as 4D tensor to be tiled across new N and F dimensions            \n","            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n","            # Tile across to match image size\n","            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n","            # Expand grids to batches and concatenate on the channel dimension\n","            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n","                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n","            v = self.decoder(zbd).exp()\n","            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 'v23410' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d, snr=30, seed=1)\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","NN = NN13\n","model = NN(M,K,N).cuda()\n","for w in model.parameters():\n","    nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(epoch, Rb[0], Rb.sum()/3/128, hh)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)), '\\n')\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            \n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')\n","\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% v24000\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","class NN14(nn.Module):\n","    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        # Estimate H and coarse V\n","        self.est = nn.Sequential(\n","            Down(in_channels=M*2, out_channels=64),\n","            Down(in_channels=64, out_channels=32),\n","            Down(in_channels=32, out_channels=4),\n","            Reshape(-1, 4*12*12),\n","            LinearBlock(4*12*12, 64),\n","            nn.Linear(64, 1),\n","            )\n","        self.b1 = nn.Linear(100, 1)\n","        self.b2 = nn.Linear(100, 1)\n","           \n","        # Estimate V using auto encoder\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            Down(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            DoubleConv(in_channels=self.dz+2, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            ) \n","        self.im_size = im_size\n","        x = torch.linspace(-1, 1, im_size)\n","        y = torch.linspace(-1, 1, im_size)\n","        x_grid, y_grid = torch.meshgrid(x, y)\n","        # Add as constant, with extra dims for N and C\n","        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n","        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - tmp.squeeze().permute(2,3,0,1)\n","            ang = self.est(torch.cat((inp.real, inp.imag), dim=1)) #vj,Rb,ang\n","\n","            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n","            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n","            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n","                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","\n","            ch = torch.pi*torch.arange(self.M, device=inp.device)\n","            hj = ((ang.tanh() @ ch[None,:])*1j).exp() # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n","            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","            shat = shat/shat.detach().abs().max()\n","\n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            # View z as 4D tensor to be tiled across new N and F dimensions            \n","            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n","            # Tile across to match image size\n","            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n","            # Expand grids to batches and concatenate on the channel dimension\n","            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n","                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n","            v = self.decoder(zbd).exp()\n","            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","    \n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 'v24000' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d, snr=30, seed=1)\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","NN = NN14\n","model = NN(M,K,N).cuda()\n","for w in model.parameters():\n","    nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(epoch, Rb[0], Rb.sum()/3/128, hh)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt)), '\\n')\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            \n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')\n","\n","# %%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% t2 -- test sbd with fewer layers, one batch, \n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","class NN_gtdecoder(nn.Module):\n","    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        # Estimate H and coarse V\n","        # self.est = nn.Sequential(\n","        #     Down(in_channels=M*2, out_channels=64),\n","        #     Down(in_channels=64, out_channels=32),\n","        #     Down(in_channels=32, out_channels=4),\n","        #     Reshape(-1, 4*12*12),\n","        #     LinearBlock(4*12*12, 64),\n","        #     nn.Linear(64, 1),\n","        #     )\n","        # self.b1 = nn.Linear(100, 1)\n","        # self.b2 = nn.Linear(100, 1)\n","           \n","        # Estimate V using auto encoder\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            Down(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            DoubleConv(in_channels=self.dz+2, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            ) \n","        self.im_size = im_size\n","        x = torch.linspace(-1, 1, im_size)\n","        y = torch.linspace(-1, 1, im_size)\n","        x_grid, y_grid = torch.meshgrid(x, y)\n","        # Add as constant, with extra dims for N and C\n","        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n","        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - tmp.squeeze().permute(2,3,0,1)\n","\n","            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n","            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n","            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n","                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n","            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","            shat = shat/shat.detach().abs().max()\n","\n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            # View z as 4D tensor to be tiled across new N and F dimensions            \n","            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n","            # Tile across to match image size\n","            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n","            # Expand grids to batches and concatenate on the channel dimension\n","            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n","                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n","            v = self.decoder(zbd).exp()\n","            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 't2' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 30 # how many samples ------------------------------------ TODO\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = I # how many samples ------------------------------------ TODO\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 3000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d[:I], snr=30, seed=1) # ------------------------------------ TODO\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n","xval_cuda = xval[:30].to(torch.cfloat).cuda()  # ------------------------------------ TODO\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","loss1, loss2, loss3 = [], [], []\n","NN = NN_gtdecoder\n","model = NN(M,K,N).cuda()\n","\n","# for w in model.parameters():\n","#     nn.init.normal_(w, mean=0., std=0.01)\n","\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    # loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n","    # loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n","    # loss3.append(l3.detach().cpu().item()/opts['batch_size'])\n","    if epoch%20 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        # plt.figure()\n","        # plt.plot(loss1, '-or')\n","        # plt.title(f'Loss1 fuction at epoch{epoch}')\n","        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n","\n","        # plt.figure()\n","        # plt.plot(loss2, '-or')\n","        # plt.title(f'Loss2 fuction at epoch{epoch}')\n","        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n","\n","        # plt.figure()\n","        # plt.plot(loss3, '-or')\n","        # plt.title(f'Loss3 fuction at epoch{epoch}')\n","        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss  = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/30)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(Rb[0], Rb.sum()/3/30)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)))\n","\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            \n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% t2_more -- test sbd with more layers, one batch,\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","class NN_gtdecoder(nn.Module):\n","    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","         \n","        # Estimate V using auto encoder\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=32),\n","            Down(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            DoubleConv(in_channels=self.dz+2, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=32),\n","            DoubleConv(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=4),\n","            OutConv(in_channels=4, out_channels=1),\n","            ) \n","        self.im_size = im_size\n","        x = torch.linspace(-1, 1, im_size)\n","        y = torch.linspace(-1, 1, im_size)\n","        x_grid, y_grid = torch.meshgrid(x, y)\n","        # Add as constant, with extra dims for N and C\n","        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n","        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - tmp.squeeze().permute(2,3,0,1)\n","\n","            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n","            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n","            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n","                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n","            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","            shat = shat/shat.detach().abs().max()\n","\n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            # View z as 4D tensor to be tiled across new N and F dimensions            \n","            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n","            # Tile across to match image size\n","            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n","            # Expand grids to batches and concatenate on the channel dimension\n","            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n","                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n","            v = self.decoder(zbd).exp()\n","            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 't2_more' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 30 # how many samples ------------------------------------ TODO\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = I # how many samples ------------------------------------ TODO\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 3000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d[:I], snr=30, seed=1) # ------------------------------------ TODO\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n","xval_cuda = xval[:30].to(torch.cfloat).cuda()  # ------------------------------------ TODO\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","loss1, loss2, loss3 = [], [], []\n","NN = NN_gtdecoder\n","model = NN(M,K,N).cuda()\n","\n","# for w in model.parameters():\n","#     nn.init.normal_(w, mean=0., std=0.01)\n","\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    # loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n","    # loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n","    # loss3.append(l3.detach().cpu().item()/opts['batch_size'])\n","    if epoch%20 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        # plt.figure()\n","        # plt.plot(loss1, '-or')\n","        # plt.title(f'Loss1 fuction at epoch{epoch}')\n","        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n","\n","        # plt.figure()\n","        # plt.plot(loss2, '-or')\n","        # plt.title(f'Loss2 fuction at epoch{epoch}')\n","        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n","\n","        # plt.figure()\n","        # plt.plot(loss3, '-or')\n","        # plt.title(f'Loss3 fuction at epoch{epoch}')\n","        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss  = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/30)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(Rb[0], Rb.sum()/3/30)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)))\n","\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            \n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% t3 -- upconv with fewer layers, one batch,\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","class NN_gtupconv(nn.Module):\n","    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            Down(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            nn.Linear(self.dz, 25*25),\n","            Reshape(-1, 1, 25, 25),\n","            Up_(in_channels=1, out_channels=64),\n","            Up_(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            ) \n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - tmp.squeeze().permute(2,3,0,1)\n","\n","            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n","                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n","            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","            shat = shat/shat.detach().abs().max()\n","\n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            v = self.decoder(z).exp()\n","            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","#@title\n","rid = 't3' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 30 # how many samples ------------------------------------ TODO\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = I # how many samples ------------------------------------ TODO\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 3000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d[:I], snr=30, seed=1) # ------------------------------------ TODO\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n","xval_cuda = xval[:30].to(torch.cfloat).cuda()  # ------------------------------------ TODO\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","loss1, loss2, loss3 = [], [], []\n","NN = NN_gtupconv\n","model = NN(M,K,N).cuda()\n","\n","# for w in model.parameters():\n","#     nn.init.normal_(w, mean=0., std=0.01)\n","\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    # loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n","    # loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n","    # loss3.append(l3.detach().cpu().item()/opts['batch_size'])\n","    if epoch%20 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        # plt.figure()\n","        # plt.plot(loss1, '-or')\n","        # plt.title(f'Loss1 fuction at epoch{epoch}')\n","        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n","\n","        # plt.figure()\n","        # plt.plot(loss2, '-or')\n","        # plt.title(f'Loss2 fuction at epoch{epoch}')\n","        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n","\n","        # plt.figure()\n","        # plt.plot(loss3, '-or')\n","        # plt.title(f'Loss3 fuction at epoch{epoch}')\n","        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss  = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/30)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(Rb[0], Rb.sum()/3/30)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)))\n","\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            \n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% t3_more --upconv with more layers, one batch,\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","class NN_gtupconv(nn.Module):\n","    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=32),\n","            Down(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            nn.Linear(self.dz, 25*25),\n","            Reshape(-1, 1, 25, 25),\n","            Up_(in_channels=1, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=32),\n","            Up_(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=4),\n","            OutConv(in_channels=4, out_channels=1),\n","            ) \n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - tmp.squeeze().permute(2,3,0,1)\n","\n","            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n","                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n","            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","            shat = shat/shat.detach().abs().max()\n","\n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            v = self.decoder(z).exp()\n","            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=1e-3):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 't3_more' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 30 # how many samples ------------------------------------ TODO\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = I # how many samples ------------------------------------ TODO\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 3000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d[:I], snr=30, seed=1) # ------------------------------------ TODO\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n","xval_cuda = xval[:30].to(torch.cfloat).cuda()  # ------------------------------------ TODO\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","loss1, loss2, loss3 = [], [], []\n","NN = NN_gtupconv\n","model = NN(M,K,N).cuda()\n","\n","# for w in model.parameters():\n","#     nn.init.normal_(w, mean=0., std=0.01)\n","\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    # loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n","    # loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n","    # loss3.append(l3.detach().cpu().item()/opts['batch_size'])\n","    if epoch%20 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        # plt.figure()\n","        # plt.plot(loss1, '-or')\n","        # plt.title(f'Loss1 fuction at epoch{epoch}')\n","        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n","\n","        # plt.figure()\n","        # plt.plot(loss2, '-or')\n","        # plt.title(f'Loss2 fuction at epoch{epoch}')\n","        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n","\n","        # plt.figure()\n","        # plt.plot(loss3, '-or')\n","        # plt.title(f'Loss3 fuction at epoch{epoch}')\n","        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss  = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/30)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(Rb[0], Rb.sum()/3/30)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)))\n","\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            \n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% t2_05 -- test sbd with fewer layers, one batch, beta=0.5(t2 as 1e-3)\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","class NN_gtdecoder(nn.Module):\n","    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        # Estimate H and coarse V\n","        # self.est = nn.Sequential(\n","        #     Down(in_channels=M*2, out_channels=64),\n","        #     Down(in_channels=64, out_channels=32),\n","        #     Down(in_channels=32, out_channels=4),\n","        #     Reshape(-1, 4*12*12),\n","        #     LinearBlock(4*12*12, 64),\n","        #     nn.Linear(64, 1),\n","        #     )\n","        # self.b1 = nn.Linear(100, 1)\n","        # self.b2 = nn.Linear(100, 1)\n","           \n","        # Estimate V using auto encoder\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            Down(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            DoubleConv(in_channels=self.dz+2, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            ) \n","        self.im_size = im_size\n","        x = torch.linspace(-1, 1, im_size)\n","        y = torch.linspace(-1, 1, im_size)\n","        x_grid, y_grid = torch.meshgrid(x, y)\n","        # Add as constant, with extra dims for N and C\n","        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n","        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - tmp.squeeze().permute(2,3,0,1)\n","\n","            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n","            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n","            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n","                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n","            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","            shat = shat/shat.detach().abs().max()\n","\n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            # View z as 4D tensor to be tiled across new N and F dimensions            \n","            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n","            # Tile across to match image size\n","            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n","            # Expand grids to batches and concatenate on the channel dimension\n","            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n","                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n","            v = self.decoder(zbd).exp()\n","            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 't2_05' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 30 # how many samples ------------------------------------ TODO\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = I # how many samples ------------------------------------ TODO\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d[:I], snr=30, seed=1) # ------------------------------------ TODO\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n","xval_cuda = xval[:30].to(torch.cfloat).cuda()  # ------------------------------------ TODO\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","loss1, loss2, loss3 = [], [], []\n","NN = NN_gtdecoder\n","model = NN(M,K,N).cuda()\n","\n","# for w in model.parameters():\n","#     nn.init.normal_(w, mean=0., std=0.01)\n","\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    # loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n","    # loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n","    # loss3.append(l3.detach().cpu().item()/opts['batch_size'])\n","    if epoch%40 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        # plt.figure()\n","        # plt.plot(loss1, '-or')\n","        # plt.title(f'Loss1 fuction at epoch{epoch}')\n","        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n","\n","        # plt.figure()\n","        # plt.plot(loss2, '-or')\n","        # plt.title(f'Loss2 fuction at epoch{epoch}')\n","        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n","\n","        # plt.figure()\n","        # plt.plot(loss3, '-or')\n","        # plt.title(f'Loss3 fuction at epoch{epoch}')\n","        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss  = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/30)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(Rb[0], Rb.sum()/3/30)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)))\n","\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            \n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% t2_05sig -- test sbd with fewer layers, one batch, beta=0.5(t2 as 1e-3), sigmoid*10\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","class NN_gtdecoder(nn.Module):\n","    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        # Estimate H and coarse V\n","        # self.est = nn.Sequential(\n","        #     Down(in_channels=M*2, out_channels=64),\n","        #     Down(in_channels=64, out_channels=32),\n","        #     Down(in_channels=32, out_channels=4),\n","        #     Reshape(-1, 4*12*12),\n","        #     LinearBlock(4*12*12, 64),\n","        #     nn.Linear(64, 1),\n","        #     )\n","        # self.b1 = nn.Linear(100, 1)\n","        # self.b2 = nn.Linear(100, 1)\n","           \n","        # Estimate V using auto encoder\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            Down(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            DoubleConv(in_channels=self.dz+2, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            ) \n","        self.im_size = im_size\n","        x = torch.linspace(-1, 1, im_size)\n","        y = torch.linspace(-1, 1, im_size)\n","        x_grid, y_grid = torch.meshgrid(x, y)\n","        # Add as constant, with extra dims for N and C\n","        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n","        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - tmp.squeeze().permute(2,3,0,1)\n","\n","            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n","            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n","            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n","                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n","            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","            shat = shat/shat.detach().abs().max()\n","\n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            # View z as 4D tensor to be tiled across new N and F dimensions            \n","            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n","            # Tile across to match image size\n","            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n","            # Expand grids to batches and concatenate on the channel dimension\n","            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n","                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n","            v = self.decoder(zbd).sigmoid()*10\n","            v_all.append(threshold(v, floor=1e-3)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 't2_05sig' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 30 # how many samples ------------------------------------ TODO\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = I # how many samples ------------------------------------ TODO\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d[:I], snr=30, seed=1) # ------------------------------------ TODO\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n","xval_cuda = xval[:30].to(torch.cfloat).cuda()  # ------------------------------------ TODO\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","loss1, loss2, loss3 = [], [], []\n","NN = NN_gtdecoder\n","model = NN(M,K,N).cuda()\n","\n","# for w in model.parameters():\n","#     nn.init.normal_(w, mean=0., std=0.01)\n","\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    # loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n","    # loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n","    # loss3.append(l3.detach().cpu().item()/opts['batch_size'])\n","    if epoch%40 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        # plt.figure()\n","        # plt.plot(loss1, '-or')\n","        # plt.title(f'Loss1 fuction at epoch{epoch}')\n","        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n","\n","        # plt.figure()\n","        # plt.plot(loss2, '-or')\n","        # plt.title(f'Loss2 fuction at epoch{epoch}')\n","        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n","\n","        # plt.figure()\n","        # plt.plot(loss3, '-or')\n","        # plt.title(f'Loss3 fuction at epoch{epoch}')\n","        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss  = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/30)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(Rb[0], Rb.sum()/3/30)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)))\n","\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            \n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% t2_05sig_e-4 -- test sbd with fewer layers, one batch, beta=0.5(t2 as 1e-3), sigmoid*10, lr=1e-4\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","class NN_gtdecoder(nn.Module):\n","    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        # Estimate H and coarse V\n","        # self.est = nn.Sequential(\n","        #     Down(in_channels=M*2, out_channels=64),\n","        #     Down(in_channels=64, out_channels=32),\n","        #     Down(in_channels=32, out_channels=4),\n","        #     Reshape(-1, 4*12*12),\n","        #     LinearBlock(4*12*12, 64),\n","        #     nn.Linear(64, 1),\n","        #     )\n","        # self.b1 = nn.Linear(100, 1)\n","        # self.b2 = nn.Linear(100, 1)\n","           \n","        # Estimate V using auto encoder\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            Down(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            DoubleConv(in_channels=self.dz+2, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            ) \n","        self.im_size = im_size\n","        x = torch.linspace(-1, 1, im_size)\n","        y = torch.linspace(-1, 1, im_size)\n","        x_grid, y_grid = torch.meshgrid(x, y)\n","        # Add as constant, with extra dims for N and C\n","        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n","        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - tmp.squeeze().permute(2,3,0,1)\n","\n","            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n","            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n","            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n","                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n","            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","            shat = shat/shat.detach().abs().max()\n","\n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            # View z as 4D tensor to be tiled across new N and F dimensions            \n","            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n","            # Tile across to match image size\n","            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n","            # Expand grids to batches and concatenate on the channel dimension\n","            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n","                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n","            v = self.decoder(zbd).sigmoid()*10\n","            v_all.append(threshold(v, floor=1e-3)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 't2_05sige-4' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 30 # how many samples ------------------------------------ TODO\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = I # how many samples ------------------------------------ TODO\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 2000\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d[:I], snr=30, seed=1) # ------------------------------------ TODO\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n","xval_cuda = xval[:30].to(torch.cfloat).cuda()  # ------------------------------------ TODO\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","loss1, loss2, loss3 = [], [], []\n","NN = NN_gtdecoder\n","model = NN(M,K,N).cuda()\n","\n","# for w in model.parameters():\n","#     nn.init.normal_(w, mean=0., std=0.01)\n","\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    # loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n","    # loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n","    # loss3.append(l3.detach().cpu().item()/opts['batch_size'])\n","    if epoch%40 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        # plt.figure()\n","        # plt.plot(loss1, '-or')\n","        # plt.title(f'Loss1 fuction at epoch{epoch}')\n","        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n","\n","        # plt.figure()\n","        # plt.plot(loss2, '-or')\n","        # plt.title(f'Loss2 fuction at epoch{epoch}')\n","        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n","\n","        # plt.figure()\n","        # plt.plot(loss3, '-or')\n","        # plt.title(f'Loss3 fuction at epoch{epoch}')\n","        # plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss  = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/30)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(Rb[0], Rb.sum()/3/30)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)))\n","\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            \n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% t4 -- t2_05sig based, vj detach at each recursive\n","from math import ceil\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","class NN_gtsbd(nn.Module):\n","    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        # Estimate H and coarse V\n","        # self.est = nn.Sequential(\n","        #     Down(in_channels=M*2, out_channels=64),\n","        #     Down(in_channels=64, out_channels=32),\n","        #     Down(in_channels=32, out_channels=4),\n","        #     Reshape(-1, 4*12*12),\n","        #     LinearBlock(4*12*12, 64),\n","        #     nn.Linear(64, 1),\n","        #     )\n","        # self.b1 = nn.Linear(100, 1)\n","        # self.b2 = nn.Linear(100, 1)\n","           \n","        # Estimate V using auto encoder\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            Down(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            DoubleConv(in_channels=self.dz+2, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            ) \n","        self.im_size = im_size\n","        x = torch.linspace(-1, 1, im_size)\n","        y = torch.linspace(-1, 1, im_size)\n","        x_grid, y_grid = torch.meshgrid(x, y)\n","        # Add as constant, with extra dims for N and C\n","        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n","        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","        I = x.shape[0]\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - tmp.squeeze().permute(2,3,0,1)\n","\n","            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n","            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n","            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n","                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n","            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","            shat = shat/shat.detach().abs().max()\n","\n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            # View z as 4D tensor to be tiled across new N and F dimensions            \n","            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n","            # Tile across to match image size\n","            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n","            # Expand grids to batches and concatenate on the channel dimension\n","            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n","                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n","            v = self.decoder(zbd).exp()\n","            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 't4' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d, snr=30, seed=1)\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","NN = NN_gtsbd\n","model = NN(M,K,N).cuda()\n","# for w in model.parameters():\n","#     nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(epoch, Rb[0], Rb.sum()/3/128, hh)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)), '\\n')\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            \n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% t4_1\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","class NN_gtsbd(nn.Module):\n","    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        # Estimate H and coarse V\n","        # self.est = nn.Sequential(\n","        #     Down(in_channels=M*2, out_channels=64),\n","        #     Down(in_channels=64, out_channels=32),\n","        #     Down(in_channels=32, out_channels=4),\n","        #     Reshape(-1, 4*12*12),\n","        #     LinearBlock(4*12*12, 64),\n","        #     nn.Linear(64, 1),\n","        #     )\n","        # self.b1 = nn.Linear(100, 1)\n","        # self.b2 = nn.Linear(100, 1)\n","           \n","        # Estimate V using auto encoder\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            Down(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            DoubleConv(in_channels=self.dz+2, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            ) \n","        self.im_size = im_size\n","        x = torch.linspace(-1, 1, im_size)\n","        y = torch.linspace(-1, 1, im_size)\n","        x_grid, y_grid = torch.meshgrid(x, y)\n","        # Add as constant, with extra dims for N and C\n","        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n","        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","        I = x.shape[0]\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - tmp.squeeze().permute(2,3,0,1)\n","\n","            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n","            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n","            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n","                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n","            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","            shat = shat/shat.detach().abs().max()\n","\n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            # View z as 4D tensor to be tiled across new N and F dimensions            \n","            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n","            # Tile across to match image size\n","            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n","            # Expand grids to batches and concatenate on the channel dimension\n","            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n","                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n","            v = self.decoder(zbd).sigmoid()*10\n","            v_all.append(threshold(v, floor=1e-3)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 't4_1' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d, snr=30, seed=1)\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","NN = NN_gtsbd\n","model = NN(M,K,N).cuda()\n","# for w in model.parameters():\n","#     nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(epoch, Rb[0], Rb.sum()/3/128, hh)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)), '\\n')\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            \n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% t4_2\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","class NN_gtsbd(nn.Module):\n","    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        # Estimate H and coarse V\n","        # self.est = nn.Sequential(\n","        #     Down(in_channels=M*2, out_channels=64),\n","        #     Down(in_channels=64, out_channels=32),\n","        #     Down(in_channels=32, out_channels=4),\n","        #     Reshape(-1, 4*12*12),\n","        #     LinearBlock(4*12*12, 64),\n","        #     nn.Linear(64, 1),\n","        #     )\n","        # self.b1 = nn.Linear(100, 1)\n","        # self.b2 = nn.Linear(100, 1)\n","           \n","        # Estimate V using auto encoder\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            Down(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            DoubleConv(in_channels=self.dz+2, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            ) \n","        self.im_size = im_size\n","        x = torch.linspace(-1, 1, im_size)\n","        y = torch.linspace(-1, 1, im_size)\n","        x_grid, y_grid = torch.meshgrid(x, y)\n","        # Add as constant, with extra dims for N and C\n","        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n","        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","        I = x.shape[0]\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - tmp.squeeze().permute(2,3,0,1)\n","\n","            # sb = self.b2(self.b1(inp.abs()).squeeze()).mean(dim=1).exp()\n","            # Rb = (sb[:None]*torch.ones(batch_size, self.M, \\\n","            #     device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n","                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            hj = hgt[:,i].repeat(I).reshape(I,3) # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n","            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","            shat = shat/shat.detach().abs().max()\n","\n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            # View z as 4D tensor to be tiled across new N and F dimensions            \n","            zr = z.view((batch_size, self.dz)+ (1, 1))  #Shape: IxDxNxF\n","            # Tile across to match image size\n","            zr = zr.expand(-1, -1, self.im_size, self.im_size)  #Shape: IxDx64x64\n","            # Expand grids to batches and concatenate on the channel dimension\n","            zbd = torch.cat((self.x_grid.expand(batch_size, -1, -1, -1),\n","                        self.y_grid.expand(batch_size, -1, -1, -1), zr), dim=1) # Shape: Ix(dz*K+2)xNxF\n","            v = self.decoder(zbd).sigmoid()*10\n","            v_all.append(threshold(v, floor=1e-3)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 't4_2' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d, snr=30, seed=1)\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","NN = NN_gtsbd\n","model = NN(M,K,N).cuda()\n","for w in model.parameters():\n","    nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(epoch, Rb[0], Rb.sum()/3/128, hh)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)), '\\n')\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            \n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% t5\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","class NN_gtupconv(nn.Module):\n","    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            Down(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            nn.Linear(self.dz, 25*25),\n","            Reshape(-1, 1, 25, 25),\n","            Up_(in_channels=1, out_channels=64),\n","            Up_(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            ) \n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - tmp.squeeze().permute(2,3,0,1)\n","\n","            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n","                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            hj = hgt[:,i].repeat(x.shape[0]).reshape(x.shape[0],3) # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n","            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","            shat = shat/shat.detach().abs().max()\n","\n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            v = self.decoder(z).exp()\n","            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 't5' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d, snr=30, seed=1)\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","NN = NN_gtupconv\n","model = NN(M,K,N).cuda()\n","# for w in model.parameters():\n","#     nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(epoch, Rb[0], Rb.sum()/3/128, hh)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)), '\\n')\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            \n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% t5_1\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","class NN_gtupconv(nn.Module):\n","    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            Down(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            nn.Linear(self.dz, 25*25),\n","            Reshape(-1, 1, 25, 25),\n","            Up_(in_channels=1, out_channels=64),\n","            Up_(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            ) \n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - tmp.squeeze().permute(2,3,0,1)\n","\n","            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n","                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            hj = hgt[:,i].repeat(x.shape[0]).reshape(x.shape[0],3) # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n","            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","            shat = shat/shat.detach().abs().max()\n","\n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            v = self.decoder(z).sigmoid()*10\n","            v_all.append(threshold(v, floor=1e-3)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 't5_1' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d, snr=30, seed=1)\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","NN = NN_gtupconv\n","model = NN(M,K,N).cuda()\n","# for w in model.parameters():\n","#     nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(epoch, Rb[0], Rb.sum()/3/128, hh)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)), '\\n')\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            \n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% t5_2\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","class NN_gtupconv(nn.Module):\n","    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            Down(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            nn.Linear(self.dz, 25*25),\n","            Reshape(-1, 1, 25, 25),\n","            Up_(in_channels=1, out_channels=64),\n","            Up_(in_channels=64, out_channels=16),\n","            OutConv(in_channels=16, out_channels=1),\n","            ) \n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - tmp.squeeze().permute(2,3,0,1)\n","\n","            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n","                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            hj = hgt[:,i].repeat(x.shape[0]).reshape(x.shape[0],3) # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n","            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","            shat = shat/shat.detach().abs().max()\n","\n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            v = self.decoder(z).sigmoid()*10\n","            v_all.append(threshold(v, floor=1e-3)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 't5_2' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-4\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d, snr=30, seed=1)\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","NN = NN_gtupconv\n","model = NN(M,K,N).cuda()\n","for w in model.parameters():\n","    nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(epoch, Rb[0], Rb.sum()/3/128, hh)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)), '\\n')\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            \n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% t5_more\n","\"\"\"Groud truth H and b, full data, fixed H, upconv with more layers\n","\"\"\"\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","if torch.__version__[:5] != '1.8.1':\n","    def mydet(x):\n","        return x.det()\n","    RAdam = torch.optim.RAdam\n","else:\n","    RAdam = optim.RAdam\n","\n","torch.autograd.set_detect_anomaly(True)\n","from vae_model import *\n","class NN_gtupconv(nn.Module):\n","    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n","    Input shape [I,M,N,F], e.g.[32,3,100,100]\n","    J <=K\n","    \"\"\"\n","    def __init__(self, M=3, K=3, im_size=100):\n","        super().__init__()\n","        self.dz = 32\n","        self.K, self.M = K, M\n","\n","        self.encoder = nn.Sequential(\n","            Down(in_channels=1, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=32),\n","            Down(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=1),\n","            )\n","        self.fc1 = nn.Linear(25*25, 2*self.dz)\n","        self.decoder = nn.Sequential(\n","            nn.Linear(self.dz, 25*25),\n","            Reshape(-1, 1, 25, 25),\n","            Up_(in_channels=1, out_channels=64),\n","            DoubleConv(in_channels=64, out_channels=32),\n","            Up_(in_channels=32, out_channels=16),\n","            DoubleConv(in_channels=16, out_channels=4),\n","            OutConv(in_channels=4, out_channels=1),\n","            ) \n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        batch_size, _, N, F = x.shape\n","        z_all, v_all, h_all = [], [], []\n","\n","        \"Neural nets for H,V\"\n","        for i in range(self.K):\n","            if i == 0:\n","                inp = x\n","            else:\n","                tmp = hj[...,None]@W@inp.permute(2,3,0,1)[...,None]\n","                inp = inp - tmp.squeeze().permute(2,3,0,1)\n","\n","            Rb = (1.4e-3*torch.ones(batch_size, self.M, \\\n","                device=x.device)).diag_embed().to(torch.cfloat) # shape:[I, M, M]\n","            hj = hgt[:,i].repeat(x.shape[0]).reshape(x.shape[0],3) # shape:[I, M]\n","            h_all.append(hj)\n","\n","            \"Wienter filter to get coarse shat\"\n","            Rx = hj[...,None] @ hj[:,None].conj() + Rb # shape of [I,M,M]\n","            W = hj[:, None,].conj() @ Rx.inverse()  # shape of [N,F,I,1,M]\n","            shat = (W @ x.permute(2,3,0,1)[...,None]).squeeze().permute(2,0,1) #[I, N, F]\n","            shat = shat/shat.detach().abs().max()\n","\n","            \"Encoder\"\n","            xx = self.encoder(shat[:,None].abs())\n","            \"Get latent variable\"\n","            zz = self.fc1(xx.reshape(batch_size,-1))\n","            mu = zz[:,::2]\n","            logvar = zz[:,1::2]\n","            z = self.reparameterize(mu, logvar)\n","            z_all.append(z)\n","            \n","            \"Decoder to get V\"\n","            v = self.decoder(z).exp()\n","            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n","        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n","        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n","\n","        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n","\n","def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.5):\n","    x = x.permute(0,2,3,1)\n","    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n","    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","    try:\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","    except:\n","        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n","        print('error happpened, data saved and stop')\n","        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n","    return -ll.sum().real + beta*kl\n","\n","#%%\n","rid = 't5_more' # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, K = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['batch_size'] = 64\n","opts['lr'] = 1e-3\n","opts['n_epochs'] = 1500\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","d = awgn_batch(d, snr=30, seed=1)\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]) # [sample,M,N,F]\n","xtr = xtr.to(torch.cfloat)\n","data = Data.TensorDataset(xtr[:I])\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n","xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n","xval_cuda = xval[:128].to(torch.cfloat).cuda()\n","\n","loss_iter, loss_tr, loss_eval = [], [], []\n","NN = NN_gtupconv\n","model = NN(M,K,N).cuda()\n","# for w in model.parameters():\n","#     nn.init.normal_(w, mean=0., std=0.01)\n","optimizer = RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","for epoch in range(opts['n_epochs']):\n","    model.train()\n","    for i, (x,) in enumerate(tr): \n","        x = x.cuda()\n","        optimizer.zero_grad()         \n","        Rs, Hhat, Rb, mu, logvar= model(x)\n","        loss = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n"," \n","    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n","    if epoch%10 == 0:\n","        plt.figure()\n","        plt.plot(loss_tr, '-or')\n","        plt.title(f'Loss fuction at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n","\n","        plt.figure()\n","        plt.plot(loss_tr[-50:], '-or')\n","        plt.title(f'Last 50 of loss at epoch{epoch}')\n","        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n","            loss = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n","            loss_eval.append(loss.cpu().item()/128)\n","            plt.figure()\n","            plt.plot(loss_eval, '-xb')\n","            plt.title(f'Accumulated validation loss at epoch{epoch}')\n","            plt.savefig(fig_loc + f'Epoch{epoch}_val')\n","\n","            hh, rs0= Hhat[0], Rs[0]\n","            Rx = hh @ rs0 @ hh.conj().t() + Rb[0]\n","            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@x.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n","            print(epoch, Rb[0], Rb.sum()/3/128, hh)\n","            print(f'epoch{epoch} h_corr is ', h_corr(hh.cpu(), torch.tensor(hgt0)), '\\n')\n","            for ii in range(K):\n","                plt.figure()\n","                plt.imshow(shat[:,:,ii,0].abs())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n","                plt.show()\n","\n","                plt.figure()\n","                plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n","                plt.colorbar()\n","                plt.title(f'Epoch{epoch}_estimated V-{ii}')\n","                plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n","                plt.show()\n","                plt.close('all')\n","            \n","        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n","print('done')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPpmgFYLsgSaMlipMa8R5/N","collapsed_sections":["NVnL-nCeBA3t","3jW2Qw0aymjk","uPI2xhUkPW6M","BqCh4vrkSS2C","w_il05ERz3u0","hyhzT1CIcERI","RsvZxNctBe2D"],"name":"[]nem_realdata_dynamic_record.ipynb","provenance":[{"file_id":"1rEeDE0AIi8lEjzIdIoiysB5PVijO7f1Z","timestamp":1627578797711},{"file_id":"1sL0NuzGnbFzk2Y5XY_n4woZ0c4lJBoEy","timestamp":1627388962384},{"file_id":"1QdweiBwMNieeGBdDFCzphgwJoB7PW0Xo","timestamp":1619782442433},{"file_id":"1W6TyLRUPhBNNN7bLUJ2u4rX4K7CBEjCU","timestamp":1619749565297},{"file_id":"1N5hj6oGZ8XxOExPPtxDCML9bchrleb2O","timestamp":1619749345089},{"file_id":"1vI54wk15P3rTcruo_pRVWX2GldhPqU1D","timestamp":1619744297962},{"file_id":"1HyayE3FINrKpEPXQ5hCl3yQ6S_gaJRF7","timestamp":1619187752515},{"file_id":"137mHbM4Vq4YtcOF9la2sOul4aNWZwRzn","timestamp":1619037563775}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
