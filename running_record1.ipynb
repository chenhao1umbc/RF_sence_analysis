{"cells":[{"cell_type":"markdown","metadata":{"id":"NVnL-nCeBA3t"},"source":["## Real data running history\n","3 neural networks for 3 components"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"wtuWghQF4NBH","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=5000 the same initialization, warm start,warm shared Hhat, lr_gamma=0.01, 3000tr samples, trim=1\n","\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 5000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.stack([gtr[:,None] for j in range(J)], dim=1)\n","\n","model, optimizer = {}, {}\n","loss_iter, loss_tr = [], []\n","for j in range(J):\n","    model[j] = UNetHalf(opts['n_ch'], 1).cuda()\n","    optimizer[j] = optim.RAdam(model[j].parameters(),\n","                    lr= opts['lr'],\n","                    betas=(0.9, 0.999),\n","                    eps=1e-8,\n","                    weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for j in range(J):\n","        for param in model[j].parameters():\n","            param.requires_grad_(False)\n","        model[j].eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            # Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","            for j in range(J):\n","                out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update neural network\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","        for j in range(J):\n","            model[j].train()\n","            for param in model[j].parameters():\n","                param.requires_grad_(True)\n","            out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            optimizer[j].zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        for j in range(J):\n","            torch.nn.utils.clip_grad_norm_(model[j].parameters(), max_norm=1)\n","            optimizer[j].step()\n","            torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"nrIREPPmhvbC","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=5100 the same initialization, cold start,warm shared Hhat, lr_gamma=0.01, 3000tr samples, trim=1\n","\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 5100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.stack([gtr[:,None] for j in range(J)], dim=1)\n","\n","model, optimizer = {}, {}\n","loss_iter, loss_tr = [], []\n","for j in range(J):\n","    model[j] = UNetHalf(opts['n_ch'], 1).cuda()\n","    optimizer[j] = optim.RAdam(model[j].parameters(),\n","                    lr= opts['lr'],\n","                    betas=(0.9, 0.999),\n","                    eps=1e-8,\n","                    weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for j in range(J):\n","        for param in model[j].parameters():\n","            param.requires_grad_(False)\n","        model[j].eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            # Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","            for j in range(J):\n","                out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        # #%% update neural network\n","        # with torch.no_grad():\n","        #     gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","        #     vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","        #     Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","        for j in range(J):\n","            model[j].train()\n","            for param in model[j].parameters():\n","                param.requires_grad_(True)\n","            out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            optimizer[j].zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        for j in range(J):\n","            torch.nn.utils.clip_grad_norm_(model[j].parameters(), max_norm=1)\n","            optimizer[j].step()\n","            torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"EJQuTsL0TNfU","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=5200 the same initialization, cold start, cold not shared Hhat(due to mistak, it is cold shared Hhat)\n","\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 5200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.stack([gtr[:,None] for j in range(J)], dim=1)\n","\n","model, optimizer = {}, {}\n","loss_iter, loss_tr = [], []\n","for j in range(J):\n","    model[j] = UNetHalf(opts['n_ch'], 1).cuda()\n","    optimizer[j] = optim.RAdam(model[j].parameters(),\n","                    lr= opts['lr'],\n","                    betas=(0.9, 0.999),\n","                    eps=1e-8,\n","                    weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for j in range(J):\n","        for param in model[j].parameters():\n","            param.requires_grad_(False)\n","        model[j].eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            # Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","            for j in range(J):\n","                out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        # #%% update neural network\n","        # with torch.no_grad():\n","        #     gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","        #     vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","        #     Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","        #     Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","        for j in range(J):\n","            model[j].train()\n","            for param in model[j].parameters():\n","                param.requires_grad_(True)\n","            out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            optimizer[j].zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        for j in range(J):\n","            torch.nn.utils.clip_grad_norm_(model[j].parameters(), max_norm=1)\n","            optimizer[j].step()\n","            torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"MyCXm7Y5gEFk","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=5201 the same initialization, cold start, cold not shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 5201 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.stack([gtr[:,None] for j in range(J)], dim=1)\n","\n","model, optimizer = {}, {}\n","loss_iter, loss_tr = [], []\n","for j in range(J):\n","    model[j] = UNetHalf(opts['n_ch'], 1).cuda()\n","    optimizer[j] = optim.RAdam(model[j].parameters(),\n","                    lr= opts['lr'],\n","                    betas=(0.9, 0.999),\n","                    eps=1e-8,\n","                    weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for j in range(J):\n","        for param in model[j].parameters():\n","            param.requires_grad_(False)\n","        model[j].eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","            for j in range(J):\n","                out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        # #%% update neural network\n","        # with torch.no_grad():\n","        #     gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","        #     vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","        #     Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","        #     Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","        for j in range(J):\n","            model[j].train()\n","            for param in model[j].parameters():\n","                param.requires_grad_(True)\n","            out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            optimizer[j].zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        for j in range(J):\n","            torch.nn.utils.clip_grad_norm_(model[j].parameters(), max_norm=1)\n","            optimizer[j].step()\n","            torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"bMDmPtpF_2m0","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=5300 the same initialization, cold start,cold shared Hhat, lr_gamma=0.01, 3000tr samples, trim=1\n","\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 5300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.stack([gtr[:,None] for j in range(J)], dim=1)\n","\n","model, optimizer = {}, {}\n","loss_iter, loss_tr = [], []\n","for j in range(J):\n","    model[j] = UNetHalf(opts['n_ch'], 1).cuda()\n","    optimizer[j] = optim.RAdam(model[j].parameters(),\n","                    lr= opts['lr'],\n","                    betas=(0.9, 0.999),\n","                    eps=1e-8,\n","                    weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for j in range(J):\n","        for param in model[j].parameters():\n","            param.requires_grad_(False)\n","        model[j].eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr.cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            # Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","            for j in range(J):\n","                out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        # #%% update neural network\n","        # with torch.no_grad():\n","        #     gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","        #     vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","        #     Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","        for j in range(J):\n","            model[j].train()\n","            for param in model[j].parameters():\n","                param.requires_grad_(True)\n","            out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            optimizer[j].zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        for j in range(J):\n","            torch.nn.utils.clip_grad_norm_(model[j].parameters(), max_norm=1)\n","            optimizer[j].step()\n","            torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"az3WS8VeJ3ks","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=5400 warm start,warm not shared Hhat(due to mistak, it is warm not shared Hhat)\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 5400 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.stack([gtr[:,None] for j in range(J)], dim=1)\n","\n","model, optimizer = {}, {}\n","loss_iter, loss_tr = [], []\n","for j in range(J):\n","    model[j] = UNetHalf(opts['n_ch'], 1).cuda()\n","    optimizer[j] = optim.RAdam(model[j].parameters(),\n","                    lr= opts['lr'],\n","                    betas=(0.9, 0.999),\n","                    eps=1e-8,\n","                    weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for j in range(J):\n","        for param in model[j].parameters():\n","            param.requires_grad_(False)\n","        model[j].eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            # Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","            for j in range(J):\n","                out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update neural network\n","        with torch.no_grad():\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","        for j in range(J):\n","            model[j].train()\n","            for param in model[j].parameters():\n","                param.requires_grad_(True)\n","            out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            optimizer[j].zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        for j in range(J):\n","            torch.nn.utils.clip_grad_norm_(model[j].parameters(), max_norm=1)\n","            optimizer[j].step()\n","            torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ezDYPNg_VZV5","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=5401 the same initialization, warm start,warm not shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 5401 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.stack([gtr[:,None] for j in range(J)], dim=1)\n","\n","model, optimizer = {}, {}\n","loss_iter, loss_tr = [], []\n","for j in range(J):\n","    model[j] = UNetHalf(opts['n_ch'], 1).cuda()\n","    optimizer[j] = optim.RAdam(model[j].parameters(),\n","                    lr= opts['lr'],\n","                    betas=(0.9, 0.999),\n","                    eps=1e-8,\n","                    weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for j in range(J):\n","        for param in model[j].parameters():\n","            param.requires_grad_(False)\n","        model[j].eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","            for j in range(J):\n","                out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update neural network\n","        with torch.no_grad():\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","        for j in range(J):\n","            model[j].train()\n","            for param in model[j].parameters():\n","                param.requires_grad_(True)\n","            out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            optimizer[j].zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        for j in range(J):\n","            torch.nn.utils.clip_grad_norm_(model[j].parameters(), max_norm=1)\n","            optimizer[j].step()\n","            torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n"]},{"cell_type":"markdown","metadata":{"id":"3jW2Qw0aymjk"},"source":["## single model\n","11- means one neural network with 3 channels ; 12- means one neural network with 1 channel\n","\n","The best result is 125240, similar one is 125243"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"_xrsikAz2IB0","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=115200 cold start, cold not shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 115200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 3  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1) # shape of \n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.sigmoid(model(g)).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        # #%% update variable\n","        # with torch.no_grad():\n","        #     gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","        #     vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","        #     Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","        #     Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","        out = torch.sigmoid(model(g)).permute(0,2,3,1)\n","        optimizer.zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"87IWUIeX15DD","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=115300 cold start, cold shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 115300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 3  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1) # shape of \n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            # Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.sigmoid(model(g)).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        # #%% update variable\n","        # with torch.no_grad():\n","        #     gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","        #     vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","        #     Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","        #     Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","        out = torch.sigmoid(model(g)).permute(0,2,3,1)\n","        optimizer.zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rwz9o349M-I8","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125000 cold start, cold not shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        # #%% update variable\n","        # with torch.no_grad():\n","        #     gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","        #     vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","        #     Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","        #     Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WkjG2xQJlRYQ","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125100 warm start, warm not shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"UYMou3qsDCPe","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125110 warm start, warm not shared Hhat, awgn20db\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=20, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"moLXl2evJCQh","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid125111 warm start, warm not shared Hhat, awgn20db, 100iter\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125111 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 101\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=20, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"abn0pzz4ErlA","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125120 warm start, warm not shared Hhat, awgn15db\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125120 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"BmBJFFqKJTgC","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid125121 warm start, warm not shared Hhat, awgn15db, 100 iter\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125121 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 101\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"dhta5U5WJ_ra","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid125130 warm start, warm not shared Hhat, awgn10db, 100iter\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125130 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 101\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Ouc0IlPeQRU0","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125200 warm start, warm shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"xtUNj7E1dtsj","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125210 warm start, warm shared Hhat, added_noise*3\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125210 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.randn(J, 1, opts['d_gamma'], opts['d_gamma'])*3\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"XLvtfOfBdxDm","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125220 warm start, warm shared Hhat, added_noise\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125220 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.randn(J, 1, opts['d_gamma'], opts['d_gamma'])\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"1O_oAwmId7vO","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125230 warm start, warm shared Hhat, awgn_snr10\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125230 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"iW5STi0u4ceR","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125240 code is missing"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"29PDQV7EALnK","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125241 warm start, warm shared Hhat, awgn_snr0\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125241 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=0, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"YoUhXuzGAR94","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125242 warm start, warm shared Hhat, awgn_snr5\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125242 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=5, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"pC5d4B6SAXpb","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125243 warm start, warm shared Hhat, awgn_snr15\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125243 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"gCc7S-8KAg6U","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125244 warm start, warm shared Hhat, awgn_snr20\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125244 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=20, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"9cjHdElWwbts","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125250 warm start, warm shared Hhat, awgn_snr10, gamma learning rate 0.01\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125250 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.01)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"rkAAOE84jG8Y","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125260 warm start, warm shared Hhat,awgn_snr10, gamma learning rate 0.01, changed model rate 0.005, em eps as 5e-4\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125260 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.005\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.01)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 8 and abs((ll_traj[ii] - ll_traj[ii-5])/ll_traj[ii-5]) <5e-4:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Ahzy5wWXYsE5","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125265 warm start, warm shared Hhat,awgn_snr10, EM iter 201, em eps as 5e-4\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125265 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 8 and abs((ll_traj[ii] - ll_traj[ii-5])/ll_traj[ii-5]) <5e-4:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"4oTLEz-A13-V","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid=125270 warm start, warm shared Hhat,awgn_snr10, gamma learning rate 0.005\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125270 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.005)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"markdown","metadata":{"id":"uPI2xhUkPW6M"},"source":["## 13 series\n","1 channel, eps is 5e-4, EM iter is 201, overall iter is 71\n","\n","139- loading previous model as the starting point"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"D2zaI_O7Pmuw","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid130000 setting as rid=125243 warm start, warm shared Hhat, awgn_snr15\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 130000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mnvvt9uNm_c0","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid130001 setting as rid=125243 warm start, warm shared Hhat, awgn_snr15\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 130001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"_SAOIZSSe2Gh","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid132000  16to100, warm start, warm shared Hhat, awgn_snr15\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf16to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 132000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 16 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"U05rQhMfswma","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid132100 , 16to100, warm start, warm shared Hhat, awgn_snr20\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf16to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 132100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 16 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=20, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"p5jDiXNLs5Q-","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid132200 , 16to100, warm start, warm shared Hhat, awgn_snr10\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf16to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 132200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 16 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"KS5r4s-Ds-Uc","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid132300 , 16to100, warm start, warm shared Hhat, awgn_snr5\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf16to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 132300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 16 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=5, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"652DnBhwtExs","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid133000 , stack half unet, warm start, warm shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_stack\n","torch.manual_seed(1)\n","\n","rid = 133000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.randn(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","from skimage.transform import resize\n","xx_all = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))[:,None]\n","loss_iter, loss_tr = [], []\n","model = UNetHalf8to100_stack(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        xx = xx_all[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"APOMrT69rGLs","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid133010 , stack half unet, warm start, warm shared Hhat, 256 inner channels\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_stack_256 as HUnet\n","torch.manual_seed(1)\n","\n","rid = 133010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.randn(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","from skimage.transform import resize\n","xx_all = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))[:,None]\n","loss_iter, loss_tr = [], []\n","model = HUnet(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        xx = xx_all[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"o3KUa7E3zFi3","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid133100 , stack half unet structure2, warm start, warm shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_stack2\n","torch.manual_seed(1)\n","\n","rid = 133100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.randn(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","from skimage.transform import resize\n","xx_all = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))[:,None]\n","loss_iter, loss_tr = [], []\n","model = UNetHalf8to100_stack2(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        xx = xx_all[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"RARDxOuvrTQP","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid133110 , stack half unet structure2, warm start, warm shared Hhat, 256 inner channels\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_stack2_256 as HUnet\n","torch.manual_seed(1)\n","\n","rid = 133110 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.randn(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","from skimage.transform import resize\n","xx_all = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))[:,None]\n","loss_iter, loss_tr = [], []\n","model = HUnet(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        xx = xx_all[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"HwL3UT1JOC5n","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid133200, interpolate, half unet, warm start, warm shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_stack3\n","torch.manual_seed(1)\n","\n","rid = 133200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.randn(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","from skimage.transform import resize\n","xx_all = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))[:,None, None]\n","loss_iter, loss_tr = [], []\n","model = UNetHalf8to100_stack3(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        inp = torch.rand(opts['batch_size'], J, 1, opts['d_gamma'],opts['d_gamma']*2).cuda()\n","        inp[...,0::2] = xx_all[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            inp = inp.detach()\n","            for j in range(J):\n","                inp[:,j,...,1::2] = g[:, j]\n","                outs.append(torch.sigmoid(model(inp[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        inp = inp.detach()\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(inp[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WJWsiQBXM_jx","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid133201 , interpolate, half unet, warm start, warm shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_stack3\n","torch.manual_seed(1)\n","\n","rid = 133201 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.randn(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","from skimage.transform import resize\n","# xx_all = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    # order=1, preserve_range=True ))[:,None, None]\n","xx_all = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","loss_iter, loss_tr = [], []\n","model = UNetHalf8to100_stack3(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        inp = torch.rand(opts['batch_size'], J, 1, opts['d_gamma'],opts['d_gamma']*2).cuda()\n","        inp[...,0::2] = xx_all[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            inp = inp.detach()\n","            for j in range(J):\n","                inp[:,j,...,1::2] = g[:, j]\n","                outs.append(torch.sigmoid(model(inp[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        inp = inp.detach()\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(inp[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"7TpSr7fzTxnf","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid133210 ,interpolate, half unet, warm start, warm shared Hhat, 256 inner channels\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_stack3_256 as HUnet\n","torch.manual_seed(1)\n","\n","rid = 133210 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.randn(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","from skimage.transform import resize\n","# xx_all = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    # order=1, preserve_range=True ))[:,None, None]\n","xx_all = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","loss_iter, loss_tr = [], []\n","model = HUnet(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        inp = torch.rand(opts['batch_size'], J, 1, opts['d_gamma'],opts['d_gamma']*2).cuda()\n","        inp[...,0::2] = xx_all[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            inp = inp.detach()\n","            for j in range(J):\n","                inp[:,j,...,1::2] = g[:, j]\n","                outs.append(torch.sigmoid(model(inp[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        inp = inp.detach()\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(inp[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Fq4z20Nmcc8U","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid135000 warm start, warm shared Hhat, awgn_snr15, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 130000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"XFb-ZHCGSm5r","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid135100 warm start, warm shared Hhat, awgn_snr10, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 135100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"5YSTb70VFQ7r","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid135110 warm start, warm shared Hhat, awgn_snr10, 2 more layers, 256 inner channels\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 135110 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WqL4MAdjMzHk","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid135111 warm start, warm shared Hhat, awgn_snr10, 2 more layers, 256 inner channels\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 135111 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"hDINoHcjNJhs","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid135120 warm start, warm shared Hhat, awgn_snr15, 2 more layers, 256 inner channels\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 135120 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"lEpdOxt7NMtP","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid135130 warm start, warm shared Hhat, awgn_snr5, 2 more layers, 256 inner channels\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 135130 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=5, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mmhjldaOSuC-","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid135200 warm start, warm shared Hhat, awgn_snr5, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 135200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=5, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"yZyYOEEzSxDj","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid135300 warm start, warm shared Hhat, awgn_snr20, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 135300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=20, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"1tsojMUvTIbz","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid136000  partially covered with noise, awgn_snr10, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 136000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 10\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","#%%\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"qMxTfNpFTOMe","vscode":{"languageId":"python"}},"outputs":[],"source":["\n","#@title rid136100 awgn_snr10, gtr partially covered with noise\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 136100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 10\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","#%%\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"0c--DxdzzDZC","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid136200 partially covered with noise, awgn_snr15, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 136200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 15\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"YHcUJXhKzEy8","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid136300 partially covered with noise, awgn_snr5, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 136300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 5\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"bPgsZ8HczJpy","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid136400 partially covered with noise, awgn_snr20, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 136400 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 20\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Cx7EOtCXBzuE","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid137000  partially covered with noise, awgn_snr10, 19 layers (5 more)\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_19 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 137000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 10\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","#%%\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"9Jpo11dtooxs","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid137100  partially covered with noise, awgn_snr15, 19 layers (5 more)\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_19 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 137100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 15\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","#%%\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"_H9CY9x1oqLz","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid137200  partially covered with noise, awgn_snr5, 19 layers (5 more)\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_19 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 137200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 5\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","#%%\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"FfEDIViYSq6B","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid139000 warm start, warm shared Hhat, awgn_snr15, loaded125240\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 139000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","# model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","model = torch.load('../data/nem_ss/models/model_rid125240.pt')\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","Hhat = torch.load('../data/nem_ss/models/Hhat_rid125240.pt')\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"fd8iT5ThUa3c","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid139100 warm start, warm shared Hhat, awgn_snr15, loaded125240, g step 5e-3\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 139100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","# model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","model = torch.load('../data/nem_ss/models/model_rid125240.pt')\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","Hhat = torch.load('../data/nem_ss/models/Hhat_rid125240.pt')\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.005)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"markdown","metadata":{"id":"BqCh4vrkSS2C"},"source":["## 14 series using new structures\n","140-2channel, $\\gamma$ as resized mixture; <br/> \n","141-2channel, $\\gamma$ as random noise; similar as 140<br/> \n","142-update Hhat individually; -- too slow cannot finish <br/> \n","143-update Hhat as one, which was I supposed to do before; -- similar result as before <br/>\n","144-added batchnorm before sigmoid/ normalize Hhat; -- too slow, barely finish<br/>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ma_I7AomJgtq","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140000 , unet8to100 warm start, warm shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNet8to100\n","torch.manual_seed(1)\n","\n","rid = 140000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNet8to100(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        xx = x[...,0].abs()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(xx[:,None], g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(xx[:,None], g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ikHO0O2CNz7T","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140100 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"GG-lPXAF5hVO","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140110 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, 128 inner channel\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140110 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"lx6KhQH83w8k","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140120 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140120 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Th87MSoE36Os","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140121 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside, save the temp results\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140121 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","        print('Hhat.norm()', Hhat.cpu().norm())\n","        print('Rb.norm.max and Rb.norm()', Rb.cpu().norm(dim=(-1,-2)).mean(), Rb.cpu().norm())\n","        print('v.max.mean for all sources, vhat.norm()',vhat.detach().cpu().real.amax(dim=(1,2)).mean(dim=0), vhat.detach().cpu().norm())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"8RytjcyaOsz9","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140122 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside, save the temp results\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140122 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","        print('Hhat.norm()', Hhat.cpu().norm())\n","        print('Rb.norm.max and Rb.norm()', Rb.cpu().norm(dim=(-1,-2)).mean(), Rb.cpu().norm())\n","        print('v.max.mean for all sources, vhat.norm()',vhat.detach().cpu().real.amax(dim=(1,2)).mean(dim=0), vhat.detach().cpu().norm())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"eN9QO3hx3YNX","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140130 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside and vj/vj.max\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140130 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"RtbJ-9McLQsW","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140131 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside and vj/vj.max\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140131 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print('Hhat.norm()', Hhat.cpu().norm())\n","        print(' vhat.norm()', vhat.detach().cpu().norm())\n","    \n","        print(f'batch {i} is done \\n')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"jc1kRTNzPu2Q","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140132 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside and vj/vj.max, another run, saving temp results\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140132 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print('Hhat.norm()', Hhat.cpu().norm())\n","        print(' vhat.norm()', vhat.detach().cpu().norm())\n","    \n","        print(f'batch {i} is done \\n')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"fxTow9_6MAQm","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140133 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside and vj/vj.max, another run\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140133 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print('Hhat.norm()', Hhat.cpu().norm())\n","        print(' vhat.norm()', vhat.detach().cpu().norm())\n","    \n","        print(f'batch {i} is done \\n')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"0slb_cpCs7BF","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140140 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside and vj/vj.max, another run, changed step size 2times for the model -- did not run\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140140 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.002\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print('Hhat.norm()', Hhat.cpu().norm())\n","        print(' vhat.norm()', vhat.detach().cpu().norm())\n","    \n","        print(f'batch {i} is done \\n')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"c0G5_8MN3IkS","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140200 warm start, warm shared Hhat, 16 layers, 2 channel input,  label as basis\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    lb[j,0] = basis[:, j]\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"tVrFGAvp5n7T","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140210 warm start, warm shared Hhat, 16 layers, 2 channel input,  label as basis, 128 inner channel\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140210 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    lb[j,0] = basis[:, j]\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"RROdQz1k3I-z","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140300 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis with FC 9*8\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_16_FC as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]]).cuda()\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = g[:,j]\n","                ins[:,j, :,:,8] = basis[:,j]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = g[:,j]\n","            ins[:,j, :,:,8] = basis[:,j]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Ie9k1XEz3N-F","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140301 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis with FC 9*8\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_16_FC as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140301 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]]).cuda()\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = g[:,j]\n","                ins[:,j, :,:,8] = basis[:,j]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = g[:,j]\n","            ins[:,j, :,:,8] = basis[:,j]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"wuHiNaW_4zxx","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140310 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis with FC 9*8, 128 inner channel\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_16_FC_128 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140310 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]]).cuda()\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = g[:,j]\n","                ins[:,j, :,:,8] = basis[:,j]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = g[:,j]\n","            ins[:,j, :,:,8] = basis[:,j]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"g3ZPWOJG1tbQ","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid140400 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis using conv\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_stack1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140400 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]]).cuda()\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = g[:,j]\n","                ins[:,j, :,:,8] = basis[:,j]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = g[:,j]\n","            ins[:,j, :,:,8] = basis[:,j]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ypOyVhJoRpuU","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid141100 warm start, warm shared Hhat, 16 layers, 2 channel input, gamma=label as random noise\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 141100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"MuH2cQhrFbDU","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid141101 warm start, warm shared Hhat, 16 layers, 2 channel input, gamma=label as random noise\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 141101 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"e4Ou25wxo-5n","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid141102 warm start, warm shared Hhat, 16 layers, 2 channel input, gamma=label as random noise -- just 11 epochs because stopping critierion is wrong\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 141102 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[epoch-5:epoch-2])/3, sum(loss_tr[epoch-2:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        if (s1-s2)/s1 < 5e-4 :\n","            print('break-2')\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ywv0UoJ4q0x7","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid141103 warm start, warm shared Hhat, 16 layers, 2 channel input, gamma=label as random noise\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 141103 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')    \n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[epoch-5:epoch-2])/3, sum(loss_tr[epoch-2:])/3\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     if (s1-s2)/s1 < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Mxvid5X_SDRf","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid141200 warm start, warm shared Hhat, 16 layers, 2 channel input,  gamma=label as basis\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 141200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    gtr[j,0] = basis[:, j]\n","gtr = gtr.repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"OMo_td8jVs-U","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid141300 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis with FC 9*8\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_16_FC as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 141300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,1,opts['d_gamma'])\n","for j in range(J):\n","    gtr[j,0] = basis[:, j]\n","gtr = gtr.repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = lb0\n","                ins[:,j, :,:,8] = g[:,j,0]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = lb0\n","            ins[:,j, :,:,8] = g[:,j,0]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"PKxm-7e0V-FI","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid141301 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis with FC 9*8\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_16_FC as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 141301 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,1,opts['d_gamma'])\n","for j in range(J):\n","    gtr[j,0] = basis[:, j]\n","gtr = gtr.repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = lb0\n","                ins[:,j, :,:,8] = g[:,j,0]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = lb0\n","            ins[:,j, :,:,8] = g[:,j,0]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"BJ1goPwDWKpu","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid141400 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis using conv\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_stack1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 141400 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,1,opts['d_gamma'])\n","for j in range(J):\n","    gtr[j,0] = basis[:, j]\n","gtr = gtr.repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = lb0\n","                ins[:,j, :,:,8] = g[:,j,0]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = lb0\n","            ins[:,j, :,:,8] = g[:,j,0]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"n2NZFUIsV_qb","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid141401 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis using conv\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_stack1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 141401 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,1,opts['d_gamma'])\n","for j in range(J):\n","    gtr[j,0] = basis[:, j]\n","gtr = gtr.repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = lb0\n","                ins[:,j, :,:,8] = g[:,j,0]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = lb0\n","            ins[:,j, :,:,8] = g[:,j,0]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"X3h1cXAkGrMM","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid142000 based on rid140130, changed Hhat to seperate one -- could not finished within max time\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 142000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Htr, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"AC7NiGgLGt-g","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid142100 based on 140120, changed Hhat to seperate ones -- could not finished within max time\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 142100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Htr, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"HuyTL5vv2kKN","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid142200 based on rid141100, changed Hhat as Htr -- could not finished within max time\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 142200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Htr, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"3Z8y54ba2sAh","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid 142300 based on rid141200 changed Hhat to Htr -- could not finished within max time\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 142300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    gtr[j,0] = basis[:, j]\n","gtr = gtr.repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Htr, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"rDhkyNLtOpCz","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid143000 based on 140120, changed Hhat to M*J\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 143000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Hhat = Hhat.mean(0)\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"rVLqLif-hI5g","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid143100 based on 140120, changed Hhat to M*J before NN\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 143100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Hhat = Hhat.mean(0)\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"KBGUPf3khL5e","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144000 similar to r140120 with batch norm before sigmoid inside -- much slower, 70 epoches is not enough\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140120 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"XnwsA-GGn_HP","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144001 similar to r140120 with batch norm before sigmoid inside, 100 epoch\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 100\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"HhPCL3AzklZA","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144002 similar to r140120 with batch norm before sigmoid inside, 150 epoch\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 144002 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 150\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"s7pdVpD2rop6","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144003, load the result of 144001 as initial\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144003 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 70\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","# model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","model = torch.load('../data/nem_ss/models/model_rid144001.pt')\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('Hhat_rid144001.pt')\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"KGjxHE2YoK5-","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144010 similar to r140120 with batch norm before sigmoid inside, double learning rate\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.002\n","opts['n_epochs'] = 100\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"xjQQ16SMr5ee","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144011, load the result of 144010 as initial\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144011 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.002\n","opts['n_epochs'] = 70\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","# model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","model = torch.load('../data/nem_ss/models/model_rid144010.pt')\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/models/Hhat_rid144010.pt')\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"SMRJqMVX2zgE","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144020 similar to r140120 with batch norm before sigmoid inside, triple learning rate\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144020 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.003\n","opts['n_epochs'] = 70\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"IArHbiAj26iK","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144030 similar to r140120 with batch norm before sigmoid inside, *5 learning rate\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144030 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.005\n","opts['n_epochs'] = 70\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"F_CPQUlK9Agj","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144040 similar to r140120 with batch norm before sigmoid inside, *7 learning rate\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144040 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.007\n","opts['n_epochs'] = 70\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"-o9eCmap9A-9","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144050 similar to r140120 with batch norm before sigmoid inside, *10 learning rate\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144050 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.01\n","opts['n_epochs'] = 70\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"1Tfh9KrQhOWR","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144100 based on 140120, changed Hhat to M*J before NN with batch-norm before sigmoid -- result is very similar to 144000\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 143000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Hhat = Hhat.mean(0)\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Nck9V_kmvS3B","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144200 similar to r140120 with batch norm before sigmoid inside, with Hj normalized\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = Hhat/((Hhat.abs()**2).sum(1)**0.5)[:,None,:]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"kKSSnCblvYdl","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144300 just as rid140120 with Hj normalized\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = Hhat/((Hhat.abs()**2).sum(1)**0.5)[:,None,:]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"1JGaDZoMonZ9","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144400 based on 140120, changed Hhat to M*J AFTER NN -- should be marked as 143-...\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144400 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        if not (epoch==0 and i==0): Hhat = Hhat.mean(0)\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Hhat = Hhat.mean(0)\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"HUa7cOJ9g1kG","vscode":{"languageId":"python"}},"outputs":[],"source":["#%%\n","#@title rid144500 relu as last layer with Hj normalized\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_relu as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144500 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = Hhat/((Hhat.abs()**2).sum(1)**0.5)[:,None,:]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')\n","#%%\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"RhhnONHRg6S9","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid144520 relu as last layer with Hj normalized, gradiant clip 0.5\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_relu as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144520 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.002\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = Hhat/((Hhat.abs()**2).sum(1)**0.5)[:,None,:]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"7asrenmVHxwA","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title r145000 based on rid140130, 3 fewer batch norm to make it faster\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_lsbn as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 145000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"5CgNnL30QEUg","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title r145001 based on rid140130, 3 fewer batch norm to make it faster\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_lsbn as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 145001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"o1FZghn9JDcF","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title r145002 based on rid140130, 3 fewer batch norm to make it faster\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_lsbn as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 145002 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-6:-3])/3, sum(loss_tr[-3:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ZII0tMY6JXHE","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title r145100 based on rid140120, 3 fewer batch norm to make it faster -- not really working well\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_lsbn as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 145100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"RFuBLusSQNiP","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title r145101 based on rid140120, 3 fewer batch norm to make it faster\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_lsbn as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 145101 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"IL2C-7QgSA_m","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid146000 less batch norm with bn before sigmoid, with stopping criteria 5e-4 -- learning rate too small, stopped\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_lsbn2 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 146000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[epoch-5:epoch-2])/3, sum(loss_tr[epoch-2:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        if (s1-s2)/s1 < 5e-4 :\n","            print('break-2')\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"VM8tmI3wSEnj","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid146010 less batch norm with bn before sigmoid, without stopping criteria 5e-4-- learning rate too small, stopped\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_lsbn2 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 146010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')    \n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[epoch-5:epoch-2])/3, sum(loss_tr[epoch-2:])/3\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     if (s1-s2)/s1 < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"kxsYP4l2TOKh","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid146110 warm start, warm shared Hhat, 16 layers, 2 channel input, gamma=label as random noise-- learning rate too small, stopped\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_lsbn2 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 146110 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')  "]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"XxomcurZT2Jn","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid146111 warm start, warm shared Hhat, 16 layers, 2 channel input, gamma=label as random noise -- learning rate too small, stopped\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_lsbn2 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 146111 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')  "]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"jwCeIVAfoX45","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid147000 similar to r140120 with batch norm before sigmoid inside  -- last layer with batchnorm or batchnor+relu are very slow\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig2 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 147000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[epoch-5:epoch-2])/3, sum(loss_tr[epoch-2:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"n0bMRtzfoepU","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid148000 similar to r140120 with batch norm before sigmoid inside -- last layer with batchnorm or batchnor+relu are very slow\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig3 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 148000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[epoch-5:epoch-2])/3, sum(loss_tr[epoch-2:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"zPjjM6pldFaJ","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid149000 similar to r140120 with batch norm before sigmoid inside --last layer 1 conv changed to 3 conv\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig4 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 149000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-6:-3])/3, sum(loss_tr[-3:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"rTB0hXG9H2-m","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid149100 sigmoid changed to -- 1-conv relu+vj/vj.max \n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_2 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 149100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-6:-3])/3, sum(loss_tr[-3:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"4o7usneqH6ce","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid149200 sigmoid changed to 3-conv relu+vj/vj.max  -- shows up nan error\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_3 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 149200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-6:-3])/3, sum(loss_tr[-3:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break"]},{"cell_type":"markdown","metadata":{"id":"w_il05ERz3u0"},"source":["## 15 series using new structures\n","basically, record all the models and loss, so far 150000_35 is chosen for the 3 class final data. Hope to defeat overfitting(after too many epoches vj is too small) and simple stopping criteria.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"VyJGL4ru4Nb0","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid150000 similar to 140100, but vj/vjmax_detach ------------*star*\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 150000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"QUQvzApz6efF","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid150100 similar to 149000, but vj/vjmax_detach\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 150100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"AYau35bEZsW0","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid151000, same as rid140120, but record all the models\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 151000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    # if epoch >20 :\n","    #     s1, s2 = sum(loss_tr[((-2*n-10):(-n-10)])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Caz1899ObdVo","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid151001, same as rid140120, but with stopping criteria and recording\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 151001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >20 :\n","        s1, s2 = sum(loss_tr[(-2*n-10):(-n-10)])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"nigmG0MhF3Rq","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid152000, same as rid140120, with validation\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 152000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","d, *_ = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xval[:200])\n","val = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gval = torch.tensor(resize(xval[...,0].abs(), [xval.shape[0],opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","g = torch.load('../data/nem_ss/gval_500.pt')\n","gval = g[:200]/g[:200].amax(dim=[1,2])[...,None,None]  #standardization \n","gval = torch.cat([gval[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I_val,J,1,8,8]\n","\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr, loss_val = [], [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","l = torch.load('../data/nem_ss/140100_lb.pt')\n","lb = l.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","    \n","    if epoch > 20:\n","        loss_val.append(val_run(val, gval, model, lb))\n","        torch.cuda.empty_cache()\n","        plt.figure()\n","        plt.plot(loss_val, '-or')\n","        plt.title(f'Val loss fuction up to epoch{epoch}')\n","        plt.savefig(fig_loc + f'id{rid}_ValLoss_epoch{epoch}')\n","        torch.save(loss_val, mod_loc +f'loss_val_rid{rid}.pt')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')\n","    \n","\n","    # if epoch >20 :\n","    #     s1, s2 = sum(loss_tr[(-2*n-10):(-n-10)])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"markdown","metadata":{"id":"hyhzT1CIcERI"},"source":["## 16 series -- working on the 6 classes data"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"SU5mjShWcO0F","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid160000 based on rid150000 for 6 classes------------*star*\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 160000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"56FAuehPEraW","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid160001 based on rid150000 for 6 classes, batch size 64 is too big to run on colab\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 160001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jP_zaqohpWuK","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid160100 based on rid150000 for 6 classes\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 160100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WrDopGHuvk4l","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid160200 based on rid150000 for 6 classes\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_7 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 160200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"7WNqA_lGvit3","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid160201 based on rid150000 for 6 classes\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_7 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 160201 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"]},{"cell_type":"markdown","metadata":{"id":"RsvZxNctBe2D"},"source":["## 17 series\n","170 Based on vj/vj.detach().max(), only train existing classes, really weakly setting\n","171 Unsupervised setting, trying to explore vj could be all zero for J<M, for the future weakly setting\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"XoZ7E4bDME0I","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid170000 based on rid160100 for 6 classes\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","def batch_process(model, g, lb, idx):\n","    \"\"\"This function try to process forward pass in a batch way\n","\n","    Args:\n","        model (object): [Neural network]\n","        g (gamma): [shape of [I,J,1,8,8]]\n","        lb (regularizer): [shape of[I,J,1,8,8]]\n","        idx (which g,lb to use): [shape of [?,2]]\n","    \"\"\"\n","    G = g[idx[:,0], idx[:,1]]  # shape of [?<I,?<J,1, 8, 8]\n","    L = lb[idx[:,0], idx[:,1]]\n","    inputs = torch.cat((G, L), dim=2).reshape(-1, 2, L.shape[-2], L.shape[-1])\n","    outs = []\n","    bs, i = 30, 0  # batch size\n","    while i*bs <= inputs.shape[0]:\n","        outs.append(model(inputs[i*bs:i*bs+bs]).squeeze())\n","        i += 1 \n","    res = torch.cat(outs).reshape(G.shape[0], G.shape[1],100,100)\n","    return res\n","\n","rid = 170000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 2850 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 50\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # how many samples to average for stopping \n","\n","d, lb = torch.load('../data/nem_ss/weakly50percomb_tr3kM6FT100_xlb.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","lbs = torch.zeros(I, J)\n","for i, v in enumerate(lb):\n","    lbs[i*50:(i+1)*50, v] = 1\n","\"shuffle data\"\n","ind = torch.randperm(I)\n","xtr, lbs = xtr[ind], lbs[ind]\n","data = Data.TensorDataset(xtr, lbs)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.ones(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.ones(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x, y) in enumerate(tr): # x is data, y is label\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat@Rs.permute(1,2,0,3,4)@Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj, s = [], y.sum() # s means how many components in one batch\n","        idx = torch.nonzero(y)\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4)@Hhat.transpose(-1,-2).conj()@Rx.inverse()  #shape of [N,F,I,J,M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - \\\n","                (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF #shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            \"calculate vj\"\n","            out = torch.ones(vhat.shape, device=vhat.device)*1e-20\n","            out[idx[:,0],:,:,idx[:,1]] = batch_process(model, g, lb, idx)[:,0]\n","            vhat.real = threshold(out)\n","            \n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_lh(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.detach().item()/s)\n","            if torch.isnan(ll_traj[-1]) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        out = torch.ones(vhat.shape, device=vhat.device)*1e-20\n","        out[idx[:,0],:,:,idx[:,1]] = batch_process(model, g, lb, idx)[:,0]\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        loss = loss_func(vhat, Rsshatnf.cuda())\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item()/s)\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item()/s)\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"uDswEqGw3iYG","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid170100 based on rid170000 for 6 classes, not shared H\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","def batch_process(model, g, lb, idx):\n","    \"\"\"This function try to process forward pass in a batch way\n","\n","    Args:\n","        model (object): [Neural network]\n","        g (gamma): [shape of [I,J,1,8,8]]\n","        lb (regularizer): [shape of[I,J,1,8,8]]\n","        idx (which g,lb to use): [shape of [?,2]]\n","    \"\"\"\n","    G = g[idx[:,0], idx[:,1]]  # shape of [?<I,?<J,1, 8, 8]\n","    L = lb[idx[:,0], idx[:,1]]\n","    inputs = torch.cat((G, L), dim=2).reshape(-1, 2, L.shape[-2], L.shape[-1])\n","    outs = []\n","    bs, i = 30, 0  # batch size\n","    while i*bs <= inputs.shape[0]:\n","        outs.append(model(inputs[i*bs:i*bs+bs]).squeeze())\n","        i += 1 \n","    res = torch.cat(outs).reshape(G.shape[0], G.shape[1],100,100)\n","    return res\n","\n","rid = 170100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 2850 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 50\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # how many samples to average for stopping \n","\n","d, lb = torch.load('../data/nem_ss/weakly50percomb_tr3kM6FT100_xlb.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","lbs = torch.zeros(I, J)\n","for i, v in enumerate(lb):\n","    lbs[i*50:(i+1)*50, v] = 1\n","\"shuffle data\"\n","ind = torch.randperm(I)\n","xtr, lbs = xtr[ind], lbs[ind]\n","data = Data.TensorDataset(xtr, lbs)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.ones(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","# Hhat = torch.ones(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x, y) in enumerate(tr): # x is data, y is label\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat@Rs.permute(1,2,0,3,4)@Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj, s = [], y.sum() # s means how many components in one batch\n","        idx = torch.nonzero(y)\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4)@Hhat.transpose(-1,-2).conj()@Rx.inverse()  #shape of [N,F,I,J,M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - \\\n","                (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF #shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            \"calculate vj\"\n","            out = torch.ones(vhat.shape, device=vhat.device)*1e-20\n","            out[idx[:,0],:,:,idx[:,1]] = batch_process(model, g, lb, idx)[:,0]\n","            vhat.real = threshold(out)\n","            \n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_lh(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.detach().item()/s)\n","            if torch.isnan(ll_traj[-1]) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        out = torch.ones(vhat.shape, device=vhat.device)*1e-20\n","        out[idx[:,0],:,:,idx[:,1]] = batch_process(model, g, lb, idx)[:,0]\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        loss = loss_func(vhat, Rsshatnf.cuda())\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item()/s)\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item()/s)\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"0A9dyp6B3mmU","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid171000, relu, ceiling=1\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import Model171000 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 171000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","# d, *_ = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","# xval = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","# data = Data.TensorDataset(xval[:200])\n","# val = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# # from skimage.transform import resize\n","# # gval = torch.tensor(resize(xval[...,0].abs(), [xval.shape[0],opts['d_gamma'],opts['d_gamma']],\\\n","# #     order=1, preserve_range=True ))\n","# g = torch.load('../data/nem_ss/gval_500.pt')\n","# gval = g[:200]/g[:200].amax(dim=[1,2])[...,None,None]  #standardization \n","# gval = torch.cat([gval[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I_val,J,1,8,8]\n","\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr, loss_val = [], [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","l = torch.load('../data/nem_ss/140100_lb.pt')\n","lb = l.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out, ceiling=1)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out,ceiling=1)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","    \n","    # if epoch > 20:\n","    #     loss_val.append(val_run(val, gval, model, lb))\n","    #     torch.cuda.empty_cache()\n","    #     plt.figure()\n","    #     plt.plot(loss_val, '-or')\n","    #     plt.title(f'Val loss fuction up to epoch{epoch}')\n","    #     plt.savefig(fig_loc + f'id{rid}_ValLoss_epoch{epoch}')\n","    #     torch.save(loss_val, mod_loc +f'loss_val_rid{rid}.pt')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')\n","    \n","\n","    # if epoch >20 :\n","    #     s1, s2 = sum(loss_tr[(-2*n-10):(-n-10)])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Bfxp7NsO9qNg","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid171001, relu, ceiling=1, not*3\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import Model171000 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 171001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","# d, *_ = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","# xval = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","# data = Data.TensorDataset(xval[:200])\n","# val = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# # from skimage.transform import resize\n","# # gval = torch.tensor(resize(xval[...,0].abs(), [xval.shape[0],opts['d_gamma'],opts['d_gamma']],\\\n","# #     order=1, preserve_range=True ))\n","# g = torch.load('../data/nem_ss/gval_500.pt')\n","# gval = g[:200]/g[:200].amax(dim=[1,2])[...,None,None]  #standardization \n","# gval = torch.cat([gval[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I_val,J,1,8,8]\n","\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr, loss_val = [], [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","l = torch.load('../data/nem_ss/140100_lb.pt')\n","lb = l.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out, ceiling=1)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out,ceiling=1)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","    \n","    # if epoch > 20:\n","    #     loss_val.append(val_run(val, gval, model, lb))\n","    #     torch.cuda.empty_cache()\n","    #     plt.figure()\n","    #     plt.plot(loss_val, '-or')\n","    #     plt.title(f'Val loss fuction up to epoch{epoch}')\n","    #     plt.savefig(fig_loc + f'id{rid}_ValLoss_epoch{epoch}')\n","    #     torch.save(loss_val, mod_loc +f'loss_val_rid{rid}.pt')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')\n","    \n","\n","    # if epoch >20 :\n","    #     s1, s2 = sum(loss_tr[(-2*n-10):(-n-10)])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"xAnhPbwk9rZc","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid171010, e^x, ceiling=1\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import Model171010 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 171010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","# d, *_ = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","# xval = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","# data = Data.TensorDataset(xval[:200])\n","# val = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# # from skimage.transform import resize\n","# # gval = torch.tensor(resize(xval[...,0].abs(), [xval.shape[0],opts['d_gamma'],opts['d_gamma']],\\\n","# #     order=1, preserve_range=True ))\n","# g = torch.load('../data/nem_ss/gval_500.pt')\n","# gval = g[:200]/g[:200].amax(dim=[1,2])[...,None,None]  #standardization \n","# gval = torch.cat([gval[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I_val,J,1,8,8]\n","\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr, loss_val = [], [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","l = torch.load('../data/nem_ss/140100_lb.pt')\n","lb = l.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out, ceiling=1)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out,ceiling=1)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","    \n","    # if epoch > 20:\n","    #     loss_val.append(val_run(val, gval, model, lb))\n","    #     torch.cuda.empty_cache()\n","    #     plt.figure()\n","    #     plt.plot(loss_val, '-or')\n","    #     plt.title(f'Val loss fuction up to epoch{epoch}')\n","    #     plt.savefig(fig_loc + f'id{rid}_ValLoss_epoch{epoch}')\n","    #     torch.save(loss_val, mod_loc +f'loss_val_rid{rid}.pt')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')\n","    \n","\n","    # if epoch >20 :\n","    #     s1, s2 = sum(loss_tr[(-2*n-10):(-n-10)])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mlESL3Qj-5ZM","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid171011, e^x, ceiling=1, gamma rate=0.01\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import Model171010 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 171011 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","# d, *_ = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","# xval = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","# data = Data.TensorDataset(xval[:200])\n","# val = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# # from skimage.transform import resize\n","# # gval = torch.tensor(resize(xval[...,0].abs(), [xval.shape[0],opts['d_gamma'],opts['d_gamma']],\\\n","# #     order=1, preserve_range=True ))\n","# g = torch.load('../data/nem_ss/gval_500.pt')\n","# gval = g[:200]/g[:200].amax(dim=[1,2])[...,None,None]  #standardization \n","# gval = torch.cat([gval[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I_val,J,1,8,8]\n","\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr, loss_val = [], [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","l = torch.load('../data/nem_ss/140100_lb.pt')\n","lb = l.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.01)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out, ceiling=1)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out,ceiling=1)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","    \n","    # if epoch > 20:\n","    #     loss_val.append(val_run(val, gval, model, lb))\n","    #     torch.cuda.empty_cache()\n","    #     plt.figure()\n","    #     plt.plot(loss_val, '-or')\n","    #     plt.title(f'Val loss fuction up to epoch{epoch}')\n","    #     plt.savefig(fig_loc + f'id{rid}_ValLoss_epoch{epoch}')\n","    #     torch.save(loss_val, mod_loc +f'loss_val_rid{rid}.pt')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')\n","    \n","\n","    # if epoch >20 :\n","    #     s1, s2 = sum(loss_tr[(-2*n-10):(-n-10)])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"markdown","metadata":{"id":"RsvZxNctBe2D"},"source":["## 18 series\n","Using the Hierarchiecal Clustering Initialization to see if [gamma1, gamma2] structure can be replaced by only gamma2\\\n","gamma1 is resized mixture\\\n","gamma2 is random noise served as label\\"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mlESL3Qj-5ZM","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid180000, M=6, totally random\n","#%% this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 180000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.ones(I, N, F, J).abs().to(torch.cdouble)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid180010, M=6, same random for all samples\n","#%% this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 180010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.ones(I, N, F, J).abs().to(torch.cdouble)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","gtr = torch.rand(J,1,opts['d_gamma'], opts['d_gamma']).repeat(I,1,1,1)\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid180100, M=3, totally random\n","#%% this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 180100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.ones(I, N, F, J).abs().to(torch.cdouble)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid180101, M=3, totally random\n","#%% this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 180101 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.ones(I, N, F, J).abs().to(torch.cdouble)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid180110, M=3, same random for all samples\n","#%% this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 180110 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.ones(I, N, F, J).abs().to(torch.cdouble)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","gtr = torch.rand(J,1,opts['d_gamma'], opts['d_gamma']).repeat(I,1,1,1,1)\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid180120, M=3, totally random, gamma with inner loop\n","#%% this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%%\n","rid = 180120 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.ones(I, N, F, J).abs().to(torch.cdouble)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.1) #### \n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            l0, lpre = 0, 0\n","            rec = []\n","            for ig in range(100):\n","                outs = []\n","                for j in range(J):\n","                    outs.append(model(g[:,j]))\n","                out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","                vhat = vhat.detach()\n","                vhat.real = threshold(out)\n","                loss = loss_func(vhat, Rsshatnf.cuda())\n","                optim_gamma.zero_grad()   \n","                loss.backward()\n","                rec.append(loss.detach().item())\n","                if ig == 0:\n","                    l0 = loss.detach().item()\n","                    lpre = l0\n","                else:\n","                    curr = loss.detach().item()\n","                    if (curr/l0).real < 5e-4 or g.grad.norm() < 5e-4:\n","                        # print(f'gamma break at {ig}, due to loss decreasing slow')\n","                        break\n","                    elif curr.real>lpre.real:\n","                        print(f'gamma break at {ig}, , due to loss increasing')\n","                        break\n","                    lpre = curr\n","                torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n","                optim_gamma.step()\n","                torch.cuda.empty_cache()\n","               \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid180130, M=3, totally random, gamma with inner loop, gammalr=0.01\n","#%% this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%%\n","rid = 180130 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.ones(I, N, F, J).abs().to(torch.cdouble)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.01) # gamma learning rate \n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            l0, lpre = 0, 0\n","            rec = []\n","            for ig in range(100):\n","                outs = []\n","                for j in range(J):\n","                    outs.append(model(g[:,j]))\n","                out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","                vhat = vhat.detach()\n","                vhat.real = threshold(out)\n","                loss = loss_func(vhat, Rsshatnf.cuda())\n","                optim_gamma.zero_grad()   \n","                loss.backward()\n","                rec.append(loss.detach().item())\n","                if ig == 0:\n","                    l0 = loss.detach().item()\n","                    lpre = l0\n","                else:\n","                    curr = loss.detach().item()\n","                    if (curr/l0).real < 5e-4 or g.grad.norm() < 5e-4:\n","                        # print(f'gamma break at {ig}, due to loss decreasing slow')\n","                        break\n","                    elif curr.real>lpre.real:\n","                        print(f'gamma break at {ig}, , due to loss increasing')\n","                        break\n","                    lpre = curr\n","                torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n","                optim_gamma.step()\n","                torch.cuda.empty_cache()\n","               \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid180140, M=3, totally random, gamma with inner loop, gammalr=0.001\n","#%% this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%%\n","rid = 180140 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.ones(I, N, F, J).abs().to(torch.cdouble)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001) # gamma learning rate \n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            l0, lpre = 0, 0\n","            rec = []\n","            for ig in range(100):\n","                outs = []\n","                for j in range(J):\n","                    outs.append(model(g[:,j]))\n","                out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","                vhat = vhat.detach()\n","                vhat.real = threshold(out)\n","                loss = loss_func(vhat, Rsshatnf.cuda())\n","                optim_gamma.zero_grad()   \n","                loss.backward()\n","                rec.append(loss.detach().item())\n","                if ig == 0:\n","                    l0 = loss.detach().item()\n","                    lpre = l0\n","                else:\n","                    curr = loss.detach().item()\n","                    if (curr/l0).real < 5e-4 or g.grad.norm() < 5e-4:\n","                        # print(f'gamma break at {ig}, due to loss decreasing slow')\n","                        break\n","                    elif curr.real>lpre.real:\n","                        print(f'gamma break at {ig}, , due to loss increasing')\n","                        break\n","                    lpre = curr\n","                torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n","                optim_gamma.step()\n","                torch.cuda.empty_cache()\n","               \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid180200, M=3, resized mixture as gamma\n","#%% this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 180200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.ones(I, N, F, J).abs().to(torch.cdouble)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","# gtr = torch.rand(J,1,opts['d_gamma'], opts['d_gamma']).repeat(I,1,1,1)\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1).to(torch.float)\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid180210, M=3, resized mixture as gamma, batch=64\n","# this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 180210 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.ones(I, N, F, J).abs().to(torch.cdouble)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","# gtr = torch.rand(J,1,opts['d_gamma'], opts['d_gamma']).repeat(I,1,1,1)\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1).to(torch.float)\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid180220, M=3, resized mixture as gamma, batch=64, FCN\n","# this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.fcn_model import FCN1 \n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 180220 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = FCN1().cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.ones(I, N, F, J).abs().to(torch.cdouble)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","gtr = torch.rand(I,J,opts['d_gamma'],opts['d_gamma'])\n","# gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","# gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","# gtr = torch.cat([gtr for j in range(J)], dim=1).to(torch.float)\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.01)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            l0, lpre = 0, 0\n","            # rec = []\n","            for ig in range(100):\n","                outs = model(g)\n","                out = outs.permute(0,2,3,1).to(torch.double)\n","                vhat = vhat.detach()\n","                vhat.real = threshold(out)\n","                loss = loss_func(vhat, Rsshatnf.cuda())\n","                optim_gamma.zero_grad()   \n","                loss.backward()\n","                # rec.append(loss.detach().item())\n","                if ig == 0:\n","                    l0 = loss.detach().item()\n","                    lpre = l0\n","                else:\n","                    curr = loss.detach().item()\n","                    if (curr/l0).real < 5e-4 or g.grad.norm() < 5e-4:\n","                        # print(f'gamma break at {ig}, due to loss decreasing slow')\n","                        break\n","                    elif curr.real>lpre.real:\n","                        print(f'gamma break at {ig}, , due to loss increasing')\n","                        break\n","                    lpre = curr\n","                # torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n","                optim_gamma.step()\n","                torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid181000, based on 150000, using HCI\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 181000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid181010, based on 150000, using HCI with hybrid precision -- works well\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","# torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 181010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr.to(torch.cdouble))\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1).to(torch.float32)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.float64)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#@title rid181020, based on 150000, using HCI with hybrid precision, Rb=1e-3\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","# torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 181020 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr.to(torch.cdouble))\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1).to(torch.float32)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*1e-3\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.float64)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182000, delta=100, H is ground truth, gamma iter=1, glr=0.001\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 100, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            # torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182010, delta=1, H is ground truth, gamma iter=1, glr=0.001\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            # torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182020, delta=0.01, H is ground truth, gamma iter=1, glr=0.001\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182020 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1e-2, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            # torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182100, delta=100, H is ground truth, gamma iter=1, glr=0.01\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 100, 1e-2 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            # torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182110, delta=1, H is ground truth, gamma iter=1, glr=0.01\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182110 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            # torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182120, delta=0.01, H is ground truth, gamma iter=1, glr=0.01\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182120 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1e-2, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            # torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182200, delta=100, H is ground truth, gamma iter=1, glr=0.01, gnorm clip to 10\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 100, 1e-2 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182210, delta=1, H is ground truth, gamma iter=1, glr=0.01, gnorm clip to 10\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182210 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            # torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182220, delta=0.01, H is ground truth, gamma iter=1, glr=0.01, gnorm clip to 10\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182220 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1e-2, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            # torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182230, delta=100, H is ground truth, gamma iter=1, glr=0.001, gnorm clip to 1\n","# to compare with 182240, just gamma1, gamma2 vs gamma\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182230 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 100, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182240, delta=100, H is ground truth, gamma iter=1, glr=0.001, gnorm clip to 1, run gamma1 gamma2 \n","# to see if I could reproduce previouse results. It should work better, only diff is H is ground truth\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182240 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 100, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1).to(torch.float)\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182241\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182241 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 100, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","# _, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","# Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1).to(torch.float)\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182242\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182242 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 100, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1).to(torch.float)\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182250, delta=0.001\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182250 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.001, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182260, delta=1\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182260 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Hhat = torch.tensor(h).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182300, delta=0.001\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.001, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182301, delta=0.001\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.001, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182310, delta=1\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182310 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182320, delta=100\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182320 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 100, 1e-3 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182330, delta=0.001, glr=0.01\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182330 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.001, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182331, delta=0.001, glr=0.01\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182331 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.001, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182340/r182341, delta=1, glr=0.01\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182340 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182342, delta=1, glr=0.01\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182342 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 128\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182343, delta=1, glr=0.01\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182343 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 128\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.randn(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182344, delta=1, glr=0.01\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182344 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.randn(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182345, delta=1, glr=0.01\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182345 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 128\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 281\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.randn(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182346, delta=1, glr=0.01\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182346 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 281\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.randn(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182350, delta=100, glr=0.01\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182350 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 100, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182360\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182360 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.001, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 51\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182370\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182370 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 51\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r182380\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 182380 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 100, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 51\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r184000/r184001\n","\n","# This is based on Dr. Kim's note. The key points are:\n","# The purpose it, with good H, gamma can be just a random variable and NEM works\n","# 1. H using ground truth\n","# 2. Rb = delta*I, delta could be 1e-4 to 1e4\n","# 3. gamma is random vector, and Rs initialized as diag(half_unet(gamma))\n","# 4. make sure EM likelihood increases and converges\n","# 5. number of gamma loop starts with 1 and could increases as needed\n","#this is hybrid precision, float32 for neural network, float64 for EM\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 184001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = torch.load('../data/nem_ss/test500M6FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r184100\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 184100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.001 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt').to(torch.cdouble).cuda()\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = torch.tensor(hhat).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","gtr = (gtr0 + noise/10).to(torch.float) # 10db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r184110\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 184110 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt').to(torch.cdouble).cuda()\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = torch.tensor(hhat).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","gtr = (gtr0 + noise/10).to(torch.float) # 10db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r184111/184112 -- continue with 184110 with shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 184111 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","# model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","model = torch.load(f'../data/nem_ss/models/rid1+/rid184110/model_rid184110_23.pt')\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt').to(torch.cdouble).cuda()\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","# Htr = torch.tensor(hhat).to(torch.cdouble).repeat(I,1,1)\n","Hhat = torch.tensor(hhat).to(torch.cdouble).repeat(opts['batch_size'],1,1).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","gtr = (gtr0 + noise/10).to(torch.float) # 10db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r184113/114 -- single precision \n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 184113 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt').to(torch.cfloat)\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","# model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","model = torch.load(f'../data/nem_ss/models/rid184110/model_rid184110_23.pt')\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt')\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","# Htr = torch.tensor(hhat).to(torch.cdouble).repeat(I,1,1)\n","Hhat = torch.tensor(hhat).to(torch.cfloat).repeat(opts['batch_size'],1,1).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cfloat)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","gtr = (gtr0 + noise/10).to(torch.float) # 10db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat = out.to(torch.cfloat)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r1841120\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 184120 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt').to(torch.cdouble).cuda()\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = torch.tensor(hhat).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","gtr = (gtr0 + noise/100).to(torch.float) # 20db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r184130\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 184130 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.001 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt').to(torch.cdouble).cuda()\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = torch.tensor(hhat).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","gtr = (gtr0 + noise/100).to(torch.float) # 20db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r185000 \n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 185000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","# model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","model = torch.load(f'../data/nem_ss/models/rid1+/rid184110/model_rid184110_23.pt')\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt')\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = hhat.to(torch.cdouble).repeat(I,1,1)\n","\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","gtr = (gtr0 + noise/10).to(torch.float) # 10db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r185100 \n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 185100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","# model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","model = torch.load(f'../data/nem_ss/models/rid1+/rid184110/model_rid184110_23.pt')\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt')\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = hhat.to(torch.cfloat).repeat(I,1,1)\n","\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cfloat)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","gtr = (gtr0 + noise/10).to(torch.float) # 10db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat = out.to(torch.cfloat)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"markdown","metadata":{},"source":["## 19 series\n","Using spatial broadcast decoder(SBD), only $\\gamma$ and HCI to get good results as before"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r190000\n","# bases on r182340, sbd\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD1 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 190000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 128  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N,opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","# _, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r190010\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD1 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 190010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 64  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N,opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","# _, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = h.to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r190020\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD1 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 190020 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 64  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N,opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191000\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD2 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 32  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191001\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD2 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 32  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 181\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191002\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD2 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191002 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 32  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 181\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = torch.load('../data/nem_ss/models/rid191001/'+f'model_rid191001_180.pt')\n","# model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191003\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD2 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191003 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 32  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 361\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191010\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD2 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 32  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191100\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD3 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191101\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD3 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191101 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 181\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191110\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD3 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191110 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191200\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD4 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191210\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD4 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191210 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191300\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD4 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.001 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _, h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            l0, lpre = 0, 0\n","            for ig in range(50):\n","                outs = []\n","                for j in range(J):\n","                    outs.append(model(g[:,j]))\n","                out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","                vhat = vhat.detach()\n","                vhat.real = threshold(out)\n","                loss = loss_func(vhat, Rsshatnf.cuda())\n","                optim_gamma.zero_grad()   \n","                loss.backward()\n","                if ig == 0:\n","                    l0 = loss.detach().item()\n","                    lpre = l0\n","                else:\n","                    curr = loss.detach().item()\n","                    l1, l2 = abs(curr-l0), abs(lpre-l0)\n","                    if abs((l2-l1)/l1) < 5e-4 or g.grad.norm() < 5e-4 :\n","                        print(f'gamma break at {ig}, due to loss decreasing slow')\n","                        break\n","                    elif curr.real>lpre.real:\n","                        print(f'gamma break at {ig}, , due to loss increasing')\n","                        break\n","                    lpre = curr\n","                torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n","                optim_gamma.step()\n","                torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r191310\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD4 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 191310 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.001 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _, h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            l0, lpre = 0, 0\n","            for ig in range(50):\n","                outs = []\n","                for j in range(J):\n","                    outs.append(model(g[:,j]))\n","                out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","                vhat = vhat.detach()\n","                vhat.real = threshold(out)\n","                loss = loss_func(vhat, Rsshatnf.cuda())\n","                optim_gamma.zero_grad()   \n","                loss.backward()\n","                if ig == 0:\n","                    l0 = loss.detach().item()\n","                    lpre = l0\n","                else:\n","                    curr = loss.detach().item()\n","                    l1, l2 = abs(curr-l0), abs(lpre-l0)\n","                    if abs((l2-l1)/l1) < 5e-4 or g.grad.norm() < 5e-4 :\n","                        print(f'gamma break at {ig}, due to loss decreasing slow')\n","                        break\n","                    elif curr.real>lpre.real:\n","                        print(f'gamma break at {ig}, , due to loss increasing')\n","                        break\n","                    lpre = curr\n","                torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n","                optim_gamma.step()\n","                torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r192000\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD3 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 192000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.01\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r192001\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD3 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 192001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.01\n","opts['n_epochs'] = 181\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r192010\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD3 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 192010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.005\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r192020\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD3 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 192020 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.002\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r192100\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD4 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 192100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.001 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.005\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _, h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            l0, lpre = 0, 0\n","            for ig in range(10):\n","                outs = []\n","                for j in range(J):\n","                    outs.append(model(g[:,j]))\n","                out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","                vhat = vhat.detach()\n","                vhat.real = threshold(out)\n","                loss = loss_func(vhat, Rsshatnf.cuda())\n","                optim_gamma.zero_grad()   \n","                loss.backward()\n","                if ig == 0:\n","                    l0 = loss.detach().item()\n","                    lpre = l0\n","                else:\n","                    curr = loss.detach().item()\n","                    l1, l2 = abs(curr-l0), abs(lpre-l0)\n","                    if abs((l2-l1)/l1) < 5e-4 or g.grad.norm() < 5e-4 :\n","                        print(f'gamma break at {ig}, due to loss decreasing slow')\n","                        break\n","                    elif curr.real>lpre.real:\n","                        print(f'gamma break at {ig}, , due to loss increasing')\n","                        break\n","                    lpre = curr\n","                torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n","                optim_gamma.step()\n","                torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r192110\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD4 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 192110 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.001 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 16  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.005\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _, h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            l0, lpre = 0, 0\n","            for ig in range(5):\n","                outs = []\n","                for j in range(J):\n","                    outs.append(model(g[:,j]))\n","                out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","                vhat = vhat.detach()\n","                vhat.real = threshold(out)\n","                loss = loss_func(vhat, Rsshatnf.cuda())\n","                optim_gamma.zero_grad()   \n","                loss.backward()\n","                if ig == 0:\n","                    l0 = loss.detach().item()\n","                    lpre = l0\n","                else:\n","                    curr = loss.detach().item()\n","                    l1, l2 = abs(curr-l0), abs(lpre-l0)\n","                    if abs((l2-l1)/l1) < 5e-4 or g.grad.norm() < 5e-4 :\n","                        print(f'gamma break at {ig}, due to loss decreasing slow')\n","                        break\n","                    elif curr.real>lpre.real:\n","                        print(f'gamma break at {ig}, , due to loss increasing')\n","                        break\n","                    lpre = curr\n","                torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n","                optim_gamma.step()\n","                torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r193000\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import SBD5 as SBD\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 193000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 0.01, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = 32  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 32 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = SBD(opts['d_gamma'], N, opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","# h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt').to(torch.cdouble).cuda()\n","_, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,opts['d_gamma'])\n","\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"markdown","metadata":{},"source":["## Sirie 20+, \n","all conv followed L-Relu and batch norm. Model is defined in each file"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2000 based on r1841120\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as NN\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%% define model\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","#%%\n","rid = 2000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN().cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt').to(torch.cdouble).cuda()\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","# Htr = torch.tensor(hhat).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","gtr = (gtr0 + noise/100).to(torch.float) # 20db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2001\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as NN\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%% define model\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","#%%\n","rid = 2001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN().cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt').to(torch.cdouble).cuda()\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","# Htr = torch.tensor(hhat).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","gtr = (gtr0 + noise/10).to(torch.float) # 20db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2002\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as NN\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%% define model\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","#%%\n","rid = 2002 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN().cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt').to(torch.cdouble).cuda()\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","# Htr = torch.tensor(hhat).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","# gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","# noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","# gtr = (gtr0 + noise/10).to(torch.float) # 20db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2003\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as NN\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%% define model\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","#%%\n","rid = 2003 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN().cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt').to(torch.cdouble).cuda()\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","# Htr = torch.tensor(hhat).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","# gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","# noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","# gtr = (gtr0 + noise/10).to(torch.float) # 20db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2004 based on r1841120\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as NN\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%% define model\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","#%%\n","rid = 2004 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN().cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt')\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = Hhat.to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","gtr = (gtr0 + noise/100).to(torch.float) # 20db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2005\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as NN\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%% define model\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","#%%\n","rid = 2005 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN().cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt')\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = Hhat.to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","gtr = (gtr0 + noise/10).to(torch.float) # 20db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2006\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as NN\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%% define model\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","#%%\n","rid = 2006 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN().cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt')\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = Hhat.to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","# gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","# noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","# gtr = (gtr0 + noise/10).to(torch.float) # 20db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2007\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as NN\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%% define model\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","#%%\n","rid = 2007 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN().cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt')\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = Hhat.to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","# gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","# noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","# gtr = (gtr0 + noise/10).to(torch.float) # 20db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2008\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as NN\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%% define model\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","#%%\n","rid = 2008 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN().cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt')\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = Hhat.to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","# noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","# gtr = (gtr0 + noise/100).to(torch.float) # 20db snr to resized signal\n","gtr = awgn_batch(gtr0, snr=10, seed=0).to(torch.float)\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2009\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as NN\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%% define model\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","#%%\n","rid = 2009 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN().cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt')\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = Hhat.to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","# noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","# gtr = (gtr0 + noise/100).to(torch.float) # 20db snr to resized signal\n","gtr = awgn_batch(gtr0, snr=0, seed=0).to(torch.float)\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2010\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as NN\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%% define model\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 256\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","#%%\n","rid = 2010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN().cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt')\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = Hhat.to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","# noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","# gtr = (gtr0 + noise/100).to(torch.float) # 20db snr to resized signal\n","gtr = awgn_batch(gtr0, snr=10, seed=0).to(torch.float)\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2011\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as NN\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%% define model\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 256\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","#%%\n","rid = 2011 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN().cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt')\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = Hhat.to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","# noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","# gtr = (gtr0 + noise/100).to(torch.float) # 20db snr to resized signal\n","gtr = awgn_batch(gtr0, snr=0, seed=0).to(torch.float)\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2012\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as NN\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%% define model\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 256\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","#%%\n","rid = 2012 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN().cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt')\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = Hhat.to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","gtr = (gtr0 + noise/10).to(torch.float) # 20db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2013\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%% define model\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","#%%\n","rid = 2013 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN().cuda()\n","model = nn.DataParallel(model)  \n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt')\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = Hhat.to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","gtr = (gtr0 + noise/10).to(torch.float) # 20db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2014\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%% define model\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","#%%\n","rid = 2014 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = torch.load('../data/nem_ss/models/rid2005/model_rid2005_40.pt')\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt')\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = Hhat.to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","gtr = (gtr0 + noise/10).to(torch.float) # 20db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2015\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%% define model\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","#%%\n","rid = 2015 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = torch.load('../data/nem_ss/models/rid2005/model_rid2005_40.pt')\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt')\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = Hhat.to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","noise = torch.rand(gtr0.shape)\n","gtr = (gtr0 + noise/10).to(torch.float) # 10db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2016\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%% define model\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","#%%\n","rid = 2016 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = torch.load('../data/nem_ss/models/rid2005/model_rid2005_40.pt')\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt')\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = Hhat.to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","noise = torch.rand(gtr0.shape)\n","gtr = (gtr0 + noise/10).to(torch.float) # 10db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = optim.RAdam([g],\n","                lr= glr,\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2017\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%% define model\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","#%%\n","rid = 2017 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = torch.load('../data/nem_ss/models/rid2016/model_rid2016_48.pt')\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt')\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = Hhat.to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","noise = torch.rand(gtr0.shape)\n","gtr = (gtr0 + noise/10).to(torch.float) # 10db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = optim.RAdam([g],\n","                lr= glr,\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2018\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%% define model\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","#%%\n","rid = 2018 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = torch.load('../data/nem_ss/models/rid2014/model_rid2014_70.pt')\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt')\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = Hhat.to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","gtr = (gtr0 + noise/10).to(torch.float) # 20db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = optim.RAdam([g],\n","                lr= glr,\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2019\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%% define model\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","#%%\n","rid = 2019 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN().cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt')\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = Hhat.to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","noise = torch.rand(gtr0.shape)\n","gtr = (gtr0 + noise/10).to(torch.float) # 10db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2020\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","#%% define model\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","#%%\n","rid = 2020 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.005 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 91\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = NN().cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/HCinit_hhat_M6_FT100.pt')\n","# _, _ , h = torch.load('../data/nem_ss/val500M6FT100_xsh.pt')\n","Htr = Hhat.to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","# gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma'])\n","gtr0 = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","noise = torch.rand(gtr0.shape)\n","gtr = (gtr0 + noise/10).to(torch.float) # 10db snr to resized signal\n","\n","#%%\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = optim.RAdam([g],\n","                lr= glr,\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            for jj in range(6):\n","                plt.figure()\n","                plt.imshow(vhat[0,...,jj].real.cpu())\n","                plt.colorbar()\n","                plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","                plt.savefig(fig_loc + f'id{rid}_v{jj}_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r2021\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 2021 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","from unet.unet_model import *\n","class UNetHalf(nn.Module):\n","    \"16 layers here\"\n","    def __init__(self, n_channels, n_classes, bilinear=False):\n","        \"\"\"Only the up part of the unet\n","        Args:\n","            n_channels ([type]): [how many input channels=n_sources]\n","            n_classes ([type]): [how many output classes=n_sources]\n","            bilinear (bool, optional): [use interpolation or deconv]. Defaults to False(use deconv).\n","        \"\"\"\n","        super().__init__()\n","        self.n_ch = n_channels\n","        self.n_classes = n_classes\n","        self.bilinear = bilinear\n","        self.n_ch = 256\n","\n","        self.inc = DoubleConv(n_channels, self.n_ch)\n","        self.up1 = Up_(self.n_ch, self.n_ch//2, True)\n","        self.up2 = Up_(self.n_ch//2, self.n_ch//4, bilinear)\n","        self.up3 = Up_(self.n_ch//4, self.n_ch//8, bilinear)\n","        self.up4 = Up_(self.n_ch//8, self.n_ch//16, bilinear)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(self.n_ch//16, self.n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(self.n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(self.n_ch//16, self.n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(self.n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(self.n_ch//16, self.n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(self.n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(self.n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","    \n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","model_old = torch.load('../data/nem_ss/models/rid1+/rid182340/model_rid182340_40.pt')\n","dict2load = model.cuda().state_dict()\n","l1 = list(dict2load.keys())\n","l2 = list(model_old.state_dict().keys())\n","for i, k in enumerate(l1):\n","    if i <=79:\n","        dict2load[k] = model_old.state_dict()[k]\n","    elif i > 79 and i < 85:\n","        pass\n","    else:\n","        idx = i-5\n","        dict2load[k] = model_old.state_dict()[l2[idx]]\n","model.load_state_dict(dict2load)\n","\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt')\n","# _, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr0 = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr0 = gtr0/gtr0.amax(dim=[3,4])[...,None,None]\n","gtr0 = torch.cat([gtr0 for j in range(J)], dim=1).to(torch.float)\n","noise = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma']) # shape of [J,1,8,8], cpu()\n","gtr = (gtr0 + noise/10).to(torch.float) # 10db snr to resized signal\n","\n","\n","#%%\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r2022\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 2022 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","from unet.unet_model import *\n","class UNetHalf(nn.Module):\n","    \"16 layers here\"\n","    def __init__(self, n_channels, n_classes, bilinear=False):\n","        \"\"\"Only the up part of the unet\n","        Args:\n","            n_channels ([type]): [how many input channels=n_sources]\n","            n_classes ([type]): [how many output classes=n_sources]\n","            bilinear (bool, optional): [use interpolation or deconv]. Defaults to False(use deconv).\n","        \"\"\"\n","        super().__init__()\n","        self.n_ch = n_channels\n","        self.n_classes = n_classes\n","        self.bilinear = bilinear\n","        self.n_ch = 256\n","\n","        self.inc = DoubleConv(n_channels, self.n_ch)\n","        self.up1 = Up_(self.n_ch, self.n_ch//2, True)\n","        self.up2 = Up_(self.n_ch//2, self.n_ch//4, bilinear)\n","        self.up3 = Up_(self.n_ch//4, self.n_ch//8, bilinear)\n","        self.up4 = Up_(self.n_ch//8, self.n_ch//16, bilinear)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(self.n_ch//16, self.n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(self.n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(self.n_ch//16, self.n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(self.n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(self.n_ch//16, self.n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(self.n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(self.n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","    \n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","model_old = torch.load('../data/nem_ss/models/rid1+/rid182340/model_rid182340_40.pt')\n","dict2load = model.cuda().state_dict()\n","l1 = list(dict2load.keys())\n","l2 = list(model_old.state_dict().keys())\n","for i, k in enumerate(l1):\n","    if i <=79:\n","        dict2load[k] = model_old.state_dict()[k]\n","    elif i > 79 and i < 85:\n","        pass\n","    else:\n","        idx = i-5\n","        dict2load[k] = model_old.state_dict()[l2[idx]]\n","model.load_state_dict(dict2load)\n","\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt')\n","# _, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr0 = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr0 = gtr0/gtr0.amax(dim=[3,4])[...,None,None]\n","gtr0 = torch.cat([gtr0 for j in range(J)], dim=1).to(torch.float)\n","noise = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma']) # shape of [J,1,8,8], cpu()\n","gtr = (gtr0 + noise/10).to(torch.float) # 10db snr to resized signal\n","\n","\n","#%%\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = optim.RAdam([g],\n","                lr= glr,\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r2023\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 2023 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","from unet.unet_model import *\n","class UNetHalf(nn.Module):\n","    \"16 layers here\"\n","    def __init__(self, n_channels, n_classes, bilinear=False):\n","        \"\"\"Only the up part of the unet\n","        Args:\n","            n_channels ([type]): [how many input channels=n_sources]\n","            n_classes ([type]): [how many output classes=n_sources]\n","            bilinear (bool, optional): [use interpolation or deconv]. Defaults to False(use deconv).\n","        \"\"\"\n","        super().__init__()\n","        self.n_ch = n_channels\n","        self.n_classes = n_classes\n","        self.bilinear = bilinear\n","        self.n_ch = 256\n","\n","        self.inc = DoubleConv(n_channels, self.n_ch)\n","        self.up1 = Up_(self.n_ch, self.n_ch//2, True)\n","        self.up2 = Up_(self.n_ch//2, self.n_ch//4, bilinear)\n","        self.up3 = Up_(self.n_ch//4, self.n_ch//8, bilinear)\n","        self.up4 = Up_(self.n_ch//8, self.n_ch//16, bilinear)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(self.n_ch//16, self.n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(self.n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(self.n_ch//16, self.n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(self.n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(self.n_ch//16, self.n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(self.n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(self.n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","    \n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt')\n","# _, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr0 = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr0 = gtr0/gtr0.amax(dim=[3,4])[...,None,None]\n","gtr0 = torch.cat([gtr0 for j in range(J)], dim=1).to(torch.float)\n","noise = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma']) # shape of [J,1,8,8], cpu()\n","gtr = (gtr0 + noise/10).to(torch.float) # 10db snr to resized signal\n","\n","\n","#%%\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = torch.optim.SGD([g], lr=glr)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%%@title r2024\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 2024 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","from unet.unet_model import *\n","class UNetHalf(nn.Module):\n","    \"16 layers here\"\n","    def __init__(self, n_channels, n_classes, bilinear=False):\n","        \"\"\"Only the up part of the unet\n","        Args:\n","            n_channels ([type]): [how many input channels=n_sources]\n","            n_classes ([type]): [how many output classes=n_sources]\n","            bilinear (bool, optional): [use interpolation or deconv]. Defaults to False(use deconv).\n","        \"\"\"\n","        super().__init__()\n","        self.n_ch = n_channels\n","        self.n_classes = n_classes\n","        self.bilinear = bilinear\n","        self.n_ch = 256\n","\n","        self.inc = DoubleConv(n_channels, self.n_ch)\n","        self.up1 = Up_(self.n_ch, self.n_ch//2, True)\n","        self.up2 = Up_(self.n_ch//2, self.n_ch//4, bilinear)\n","        self.up3 = Up_(self.n_ch//4, self.n_ch//8, bilinear)\n","        self.up4 = Up_(self.n_ch//8, self.n_ch//16, bilinear)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(self.n_ch//16, self.n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(self.n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(self.n_ch//16, self.n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(self.n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(self.n_ch//16, self.n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(self.n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(self.n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","    \n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 48\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # for stopping \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\n","\"initial\"\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","h = torch.load('../data/nem_ss/HCinit_hhat_M3_FT100.pt')\n","# _, _ , h = d = torch.load('../data/nem_ss/test500M3FT100_xsh.pt')\n","Htr = torch.tensor(h).to(torch.cdouble).repeat(I,1,1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n","gtr0 = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr0 = gtr0/gtr0.amax(dim=[3,4])[...,None,None]\n","gtr0 = torch.cat([gtr0 for j in range(J)], dim=1).to(torch.float)\n","noise = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma']) # shape of [J,1,8,8], cpu()\n","gtr = (gtr0 + noise/10).to(torch.float) # 10db snr to resized signal\n","\n","\n","#%%\n","#@title gamma does not have inner loop\n","for epoch in range(opts['n_epochs']):    \n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        for param in model.parameters():\n","            param.requires_grad_(False)\n","        model.eval()\n","        #%% EM part       \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n","\n","        x = x.cuda()\n","        g.requires_grad_()\n","        optim_gamma = optim.RAdam([g],\n","                lr= glr,\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(g[:,j]))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            # vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","\n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break\n"]},{"cell_type":"markdown","metadata":{},"source":["## test files"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% t2005_m6\n","\"\"\"Key varibales are \n","M - how many channels\n","which_class - list of source index, which sources are in the mixture\n","J - the algorithm presumes how many classes in the mixture\n","ind - from 0 to 99, index of test sample\n","max_iter - how many EM iterations\n","seed - random seed number\n","rid - training model index\n","model - saved trained model\n","\"\"\"\n","from utils import *\n","\n","import matplotlib\n","matplotlib.rc('font', size=16)\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 150\n","torch.set_printoptions(linewidth=160)\n","from skimage.transform import resize\n","import itertools\n","import time\n","t = time.time()\n","d, s, h = torch.load('../data/nem_ss/test500M6FT100_xsh.pt')\n","h, N, F = torch.tensor(h), s.shape[-1], s.shape[-2] # h is M*J matrix, here 6*6\n","ratio = d.abs().amax(dim=(1,2,3))\n","x_all = (d/ratio[:,None,None,None]).permute(0,2,3,1)\n","s_all = s.abs().permute(0,2,3,1) \n","glr = 0.005\n","\n","#%% loading data and functions\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","def cluster_init(x, J=3, K=14, init=1, Rbscale=1e-3, showfig=False):\n","    \"\"\"psudo code, https://www.saedsayad.com/clustering_hierarchical.htm\n","    Given : A set X of obejects{x1,...,xn}\n","            A cluster distance function dist(c1, c2)\n","    for i=1 to n\n","        ci = {xi}\n","    end for\n","    C = {c1, ..., cn}\n","    I = n+1\n","    While I>1 do\n","        (cmin1, cmin2) = minimum dist(ci, cj) for all ci, cj in C\n","        remove cmin1 and cmin2 from C\n","        add {cmin1, cmin2} to C\n","        I = I - 1\n","    end while\n","\n","    However, this naive algorithm does not fit large samples. \n","    Here we use scipy function for the linkage.\n","    J is how many clusters\n","    \"\"\"   \n","    dtype = x.dtype\n","    N, F, M = x.shape\n","\n","    \"get data and clusters ready\"\n","    x_norm = ((x[:,:,None,:]@x[..., None].conj())**0.5)[:,:,0]\n","    if init==1: x_ = x/x_norm * (-1j*x[...,0:1].angle()).exp() # shape of [N, F, M] x_bar\n","    else: x_ = x * (-1j*x[...,0:1].angle()).exp() # the x_tilde in Duong's paper\n","    data = x_.reshape(N*F, M)\n","    I = data.shape[0]\n","    C = [[i] for i in range(I)]  # initial cluster\n","\n","    \"calc. affinity matrix and linkage\"\n","    perms = torch.combinations(torch.arange(len(C)))\n","    d = data[perms]\n","    table = ((d[:,0] - d[:,1]).abs()**2).sum(dim=-1)**0.5\n","    from scipy.cluster.hierarchy import dendrogram, linkage\n","    z = linkage(table, method='average')\n","    if showfig: dn = dendrogram(z, p=3, truncate_mode='level')\n","\n","    \"find the max J cluster and sample index\"\n","    zind = torch.tensor(z).to(torch.int)\n","    flag = torch.cat((torch.ones(I), torch.zeros(I)))\n","    c = C + [[] for i in range(I)]\n","    for i in range(z.shape[0]-K): # threshold of K level to stop\n","        c[i+I] = c[zind[i][0]] + c[zind[i][1]]\n","        flag[i+I], flag[zind[i][0]], flag[zind[i][1]] = 1, 0, 0\n","    ind = (flag == 1).nonzero(as_tuple=True)[0]\n","    dict_c = {}  # which_cluster: how_many_nodes\n","    for i in range(ind.shape[0]):\n","        dict_c[ind[i].item()] = len(c[ind[i]])\n","    dict_c_sorted = {k:v for k,v in sorted(dict_c.items(), key=lambda x: -x[1])}\n","    cs = []\n","    for i, (k,v) in enumerate(dict_c_sorted.items()):\n","        if i == J:\n","            break\n","        cs.append(c[k])\n","\n","    \"initil the EM variables\"\n","    Hhat = torch.rand(M, J, dtype=dtype)\n","    Rj = torch.rand(J, M, M, dtype=dtype)\n","    for i in range(J):\n","        d = data[torch.tensor(cs[i])] # shape of [I_cj, M]\n","        Hhat[:,i] = d.mean(0)\n","        Rj[i] = (d[..., None] @ d[:,None,:].conj()).mean(0)\n","    vhat = torch.ones(N, F, J).abs().to(dtype)\n","    Rb = torch.eye(M).to(dtype)*Rbscale\n","\n","    return vhat, Hhat, Rb, Rj\n","\n","def nem_hci(x, J=6, Hscale=1, Rbscale=1, max_iter=501, seed=1, model=''):\n","\n","    def log_likelihood(x, vhat, Hhat, Rb, ):\n","        \"\"\" Hhat shape of [I, M, J] # I is NO. of samples, M is NO. of antennas, J is NO. of sources\n","            vhat shape of [I, N, F, J]\n","            Rb shape of [I, M, M]\n","            x shape of [I, N, F, M]\n","        \"\"\"\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rcj = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj()\n","        Rxperm = Rcj + Rb \n","        Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","        l = -(np.pi*torch.linalg.det(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","        return l.sum().real, Rs, Rxperm, Rcj\n","\n","    torch.manual_seed(seed) \n","    if model == '':\n","        print('A model is needed')\n","\n","    model = torch.load(model)\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    #%% EM part\n","    \"initial\"        \n","    N, F, M = x.shape\n","    NF= N*F\n","    graw = torch.tensor(resize(x[...,0].abs().numpy(), [8,8], order=1, preserve_range=True))\n","    graw = (graw/graw.max())[None,...]  #standardization shape of [1, 8, 8]\n","    g = torch.stack([graw[:,None] for j in range(J)], dim=1)  # shape of [1,J,8,8]\n","    noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","    g = (g + noise/10).to(torch.float).cuda() \n","    # g = torch.rand(1, J, 1, 8, 8).cuda()\n","    x = x.cuda()\n","\n","    vhat = torch.randn(1, N, F, J).abs().to(torch.cdouble).cuda()\n","    outs = []\n","    for j in range(J):\n","        outs.append(model(g[:,j]))\n","    out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","    vhat.real = threshold(out)\n","    # Hhat = torch.randn(1, M, J).to(torch.cdouble).cuda()*Hscale\n","    Hhat = cluster_init(x.cpu(), J=J)[1].cuda()\n","    Rb = torch.ones(1, M).diag_embed().cuda().to(torch.cdouble)*Rbscale\n","    Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((0,1))/NF\n","    Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","    Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","    g.requires_grad_()\n","    optim_gamma = torch.optim.SGD([g], lr= glr)\n","    ll_traj = []\n","\n","    for ii in range(max_iter): # EM iterations\n","        \"E-step\"\n","        W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","        shat = W.permute(2,0,1,3,4) @ x[...,None]\n","        Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","        Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","        Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","        \"M-step\"\n","        Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","        Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","            Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","        Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","        Rb.imag = Rb.imag - Rb.imag\n","\n","        # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","        # vj.imag = vj.imag - vj.imag\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out, ceiling=1)\n","        loss = loss_func(vhat, Rsshatnf.cuda())\n","        optim_gamma.zero_grad()   \n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","        optim_gamma.step()\n","        torch.cuda.empty_cache()\n","        \n","        \"compute log-likelyhood\"\n","        vhat = vhat.detach()\n","        ll, Rs, Rx, Rcj = log_likelihood(x, vhat, Hhat, Rb)\n","        ll_traj.append(ll.item())\n","        if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","        if ii > 20 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <5e-4:\n","            print(f'EM early stop at iter {ii}')\n","            break\n","\n","    return (shat.cpu(), Hhat.cpu(), vhat.cpu().squeeze()), g.detach().cpu(), Rb.cpu(), ll_traj\n","\n","#%%\n","for db in [30,20,10,5,0]:\n","    res_s, res_h = [], []\n","    for ii in range(100):\n","        which_class, ind = [0,1,2,3,4,5], ii\n","        M, J = 6, len(which_class)\n","        for i, v in enumerate(which_class):\n","            if i == 0 : d = 0\n","            d = d + h[:M, v, None] @ s[ind, v].reshape(1, N*F)\n","        r = d.abs().max()\n","        d = d.reshape(M, N, F).permute(1,2,0)/r\n","        data = awgn(d, db, seed=0)\n","\n","        # rid = 160100\n","        # model = f'../data/nem_ss/models/rid1+/rid{rid}/model_rid{rid}_33.pt'\n","        # shv, g, Rb, loss = nem_func_less(data, J=J, seed=10, model=model, max_iter=301)\n","\n","        rid = 2005\n","        model = f'../data/nem_ss/models/rid{rid}/model_rid{rid}_40.pt'\n","        shv, g, Rb, loss = nem_hci(data, J=J, seed=10, model=model, max_iter=301)\n","\n","        shat, Hhat, vhat = shv\n","        res_s.append(s_corr(shat.squeeze().abs(), s_all[ind].abs()))\n","        res_h.append(h_corr(Hhat.squeeze(), h[:M, which_class]))\n","        print(ii)\n","        print('s_corr', res_s[-1])\n","        print('h_corr', res_h[-1])\n","    print(f's mean at db{db}', sum(res_s)/100)\n","    print(f'h mean at db{db}', sum(res_h)/100)\n","print('done')\n","\n","if False:\n","    for i in range(6):\n","        plt.figure()\n","        plt.imshow((shat.squeeze()[...,i]*Hhat[0,0,i]).abs()*ratio[ind])\n","        plt.colorbar()\n","        # plt.savefig('nem_'+name[i]+'.eps')\n","        plt.show()\n","        # plt.title('plot of s from NEM')\n","\n","\n","    for i, v in enumerate(which_class):\n","        plt.figure()\n","        plt.imshow(s[ind, v].squeeze().abs())\n","        plt.colorbar()\n","        plt.title('GT')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% t2014_25_M3\n","\"\"\"Key varibales are \n","M - how many channels\n","which_class - list of source index, which sources are in the mixture\n","J - the algorithm presumes how many classes in the mixture\n","ind - from 0 to 99, index of test sample\n","max_iter - how many EM iterations\n","seed - random seed number\n","rid - training model index\n","model - saved trained model\n","\"\"\"\n","from utils import *\n","\n","import matplotlib\n","matplotlib.rc('font', size=16)\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n","plt.rcParams['figure.dpi'] = 150\n","torch.set_printoptions(linewidth=160)\n","from skimage.transform import resize\n","import itertools\n","import time\n","t = time.time()\n","d, s, h = torch.load('../data/nem_ss/test500M6FT100_xsh.pt')\n","h, N, F = torch.tensor(h), s.shape[-1], s.shape[-2] # h is M*J matrix, here 6*6\n","ratio = d.abs().amax(dim=(1,2,3))\n","x_all = (d/ratio[:,None,None,None]).permute(0,2,3,1)\n","s_all = s.abs().permute(0,2,3,1) \n","glr = 0.005\n","\n","#%% loading data and functions\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","def cluster_init(x, J=3, K=14, init=1, Rbscale=1e-3, showfig=False):\n","    \"\"\"psudo code, https://www.saedsayad.com/clustering_hierarchical.htm\n","    Given : A set X of obejects{x1,...,xn}\n","            A cluster distance function dist(c1, c2)\n","    for i=1 to n\n","        ci = {xi}\n","    end for\n","    C = {c1, ..., cn}\n","    I = n+1\n","    While I>1 do\n","        (cmin1, cmin2) = minimum dist(ci, cj) for all ci, cj in C\n","        remove cmin1 and cmin2 from C\n","        add {cmin1, cmin2} to C\n","        I = I - 1\n","    end while\n","\n","    However, this naive algorithm does not fit large samples. \n","    Here we use scipy function for the linkage.\n","    J is how many clusters\n","    \"\"\"   \n","    dtype = x.dtype\n","    N, F, M = x.shape\n","\n","    \"get data and clusters ready\"\n","    x_norm = ((x[:,:,None,:]@x[..., None].conj())**0.5)[:,:,0]\n","    if init==1: x_ = x/x_norm * (-1j*x[...,0:1].angle()).exp() # shape of [N, F, M] x_bar\n","    else: x_ = x * (-1j*x[...,0:1].angle()).exp() # the x_tilde in Duong's paper\n","    data = x_.reshape(N*F, M)\n","    I = data.shape[0]\n","    C = [[i] for i in range(I)]  # initial cluster\n","\n","    \"calc. affinity matrix and linkage\"\n","    perms = torch.combinations(torch.arange(len(C)))\n","    d = data[perms]\n","    table = ((d[:,0] - d[:,1]).abs()**2).sum(dim=-1)**0.5\n","    from scipy.cluster.hierarchy import dendrogram, linkage\n","    z = linkage(table, method='average')\n","    if showfig: dn = dendrogram(z, p=3, truncate_mode='level')\n","\n","    \"find the max J cluster and sample index\"\n","    zind = torch.tensor(z).to(torch.int)\n","    flag = torch.cat((torch.ones(I), torch.zeros(I)))\n","    c = C + [[] for i in range(I)]\n","    for i in range(z.shape[0]-K): # threshold of K level to stop\n","        c[i+I] = c[zind[i][0]] + c[zind[i][1]]\n","        flag[i+I], flag[zind[i][0]], flag[zind[i][1]] = 1, 0, 0\n","    ind = (flag == 1).nonzero(as_tuple=True)[0]\n","    dict_c = {}  # which_cluster: how_many_nodes\n","    for i in range(ind.shape[0]):\n","        dict_c[ind[i].item()] = len(c[ind[i]])\n","    dict_c_sorted = {k:v for k,v in sorted(dict_c.items(), key=lambda x: -x[1])}\n","    cs = []\n","    for i, (k,v) in enumerate(dict_c_sorted.items()):\n","        if i == J:\n","            break\n","        cs.append(c[k])\n","\n","    \"initil the EM variables\"\n","    Hhat = torch.rand(M, J, dtype=dtype)\n","    Rj = torch.rand(J, M, M, dtype=dtype)\n","    for i in range(J):\n","        d = data[torch.tensor(cs[i])] # shape of [I_cj, M]\n","        Hhat[:,i] = d.mean(0)\n","        Rj[i] = (d[..., None] @ d[:,None,:].conj()).mean(0)\n","    vhat = torch.ones(N, F, J).abs().to(dtype)\n","    Rb = torch.eye(M).to(dtype)*Rbscale\n","\n","    return vhat, Hhat, Rb, Rj\n","\n","def nem_hci(x, J=6, Hscale=1, Rbscale=1, max_iter=501, seed=1, model=''):\n","\n","    def log_likelihood(x, vhat, Hhat, Rb, ):\n","        \"\"\" Hhat shape of [I, M, J] # I is NO. of samples, M is NO. of antennas, J is NO. of sources\n","            vhat shape of [I, N, F, J]\n","            Rb shape of [I, M, M]\n","            x shape of [I, N, F, M]\n","        \"\"\"\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rcj = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj()\n","        Rxperm = Rcj + Rb \n","        Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","        l = -(np.pi*torch.linalg.det(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","        return l.sum().real, Rs, Rxperm, Rcj\n","\n","    torch.manual_seed(seed) \n","    if model == '':\n","        print('A model is needed')\n","\n","    model = torch.load(model)\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    #%% EM part\n","    \"initial\"        \n","    N, F, M = x.shape\n","    NF= N*F\n","    graw = torch.tensor(resize(x[...,0].abs().numpy(), [8,8], order=1, preserve_range=True))\n","    graw = (graw/graw.max())[None,...]  #standardization shape of [1, 8, 8]\n","    g = torch.stack([graw[:,None] for j in range(J)], dim=1)  # shape of [1,J,8,8]\n","    noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","    g = (g + noise/10).to(torch.float).cuda() \n","    # g = torch.rand(1, J, 1, 8, 8).cuda()\n","    x = x.cuda()\n","\n","    vhat = torch.randn(1, N, F, J).abs().to(torch.cdouble).cuda()\n","    outs = []\n","    for j in range(J):\n","        outs.append(model(g[:,j]))\n","    out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","    vhat.real = threshold(out)\n","    # Hhat = torch.randn(1, M, J).to(torch.cdouble).cuda()*Hscale\n","    Hhat = cluster_init(x.cpu(), J=J)[1].cuda()\n","    Rb = torch.ones(1, M).diag_embed().cuda().to(torch.cdouble)*Rbscale\n","    Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((0,1))/NF\n","    Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","    Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","    g.requires_grad_()\n","    optim_gamma = torch.optim.SGD([g], lr= glr)\n","    ll_traj = []\n","\n","    for ii in range(max_iter): # EM iterations\n","        \"E-step\"\n","        W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","        shat = W.permute(2,0,1,3,4) @ x[...,None]\n","        Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","        Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","        Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","        \"M-step\"\n","        Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","        Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","            Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","        Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","        Rb.imag = Rb.imag - Rb.imag\n","\n","        # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","        # vj.imag = vj.imag - vj.imag\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out, ceiling=1)\n","        loss = loss_func(vhat, Rsshatnf.cuda())\n","        optim_gamma.zero_grad()   \n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","        optim_gamma.step()\n","        torch.cuda.empty_cache()\n","        \n","        \"compute log-likelyhood\"\n","        vhat = vhat.detach()\n","        ll, Rs, Rx, Rcj = log_likelihood(x, vhat, Hhat, Rb)\n","        ll_traj.append(ll.item())\n","        if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","        if ii > 20 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <5e-4:\n","            print(f'EM early stop at iter {ii}')\n","            break\n","\n","    return (shat.cpu(), Hhat.cpu(), vhat.cpu().squeeze()), g.detach().cpu(), Rb.cpu(), ll_traj\n","\n","#%%\n","for db in [30,20,10,5,0]:\n","    res_s, res_h = [], []\n","    for ii in range(100):\n","        which_class, ind = [0,1,2,3,4,5], ii\n","        M, J = 3, len(which_class)\n","        for i, v in enumerate(which_class):\n","            if i == 0 : d = 0\n","            d = d + h[:M, v, None] @ s[ind, v].reshape(1, N*F)\n","        r = d.abs().max()\n","        d = d.reshape(M, N, F).permute(1,2,0)/r\n","        data = awgn(d, db, seed=0)\n","\n","        # rid = 160100\n","        # model = f'../data/nem_ss/models/rid1+/rid{rid}/model_rid{rid}_33.pt'\n","        # shv, g, Rb, loss = nem_func_less(data, J=J, seed=10, model=model, max_iter=301)\n","\n","        rid = 2014\n","        model = f'../data/nem_ss/models/rid{rid}/model_rid{rid}_25.pt'\n","        shv, g, Rb, loss = nem_hci(data, J=J, seed=10, model=model, max_iter=301)\n","\n","        shat, Hhat, vhat = shv\n","        res_s.append(s_corr(shat.squeeze().abs(), s_all[ind].abs()))\n","        res_h.append(h_corr(Hhat.squeeze(), h[:M, which_class]))\n","        print(ii)\n","        print('s_corr', res_s[-1])\n","        print('h_corr', res_h[-1])\n","    print(f's mean at db{db}', sum(res_s)/100)\n","    print(f'h mean at db{db}', sum(res_h)/100)\n","print('done')\n","\n","if False:\n","    for i in range(6):\n","        plt.figure()\n","        plt.imshow((shat.squeeze()[...,i]*Hhat[0,0,i]).abs()*ratio[ind])\n","        plt.colorbar()\n","        # plt.savefig('nem_'+name[i]+'.eps')\n","        plt.show()\n","        # plt.title('plot of s from NEM')\n","\n","\n","    for i, v in enumerate(which_class):\n","        plt.figure()\n","        plt.imshow(s[ind, v].squeeze().abs())\n","        plt.colorbar()\n","        plt.title('GT')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% t2014_25_M6\n","\"\"\"Key varibales are \n","M - how many channels\n","which_class - list of source index, which sources are in the mixture\n","J - the algorithm presumes how many classes in the mixture\n","ind - from 0 to 99, index of test sample\n","max_iter - how many EM iterations\n","seed - random seed number\n","rid - training model index\n","model - saved trained model\n","\"\"\"\n","from utils import *\n","\n","import matplotlib\n","matplotlib.rc('font', size=16)\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n","plt.rcParams['figure.dpi'] = 150\n","torch.set_printoptions(linewidth=160)\n","from skimage.transform import resize\n","import itertools\n","import time\n","t = time.time()\n","d, s, h = torch.load('../data/nem_ss/test500M6FT100_xsh.pt')\n","h, N, F = torch.tensor(h), s.shape[-1], s.shape[-2] # h is M*J matrix, here 6*6\n","ratio = d.abs().amax(dim=(1,2,3))\n","x_all = (d/ratio[:,None,None,None]).permute(0,2,3,1)\n","s_all = s.abs().permute(0,2,3,1) \n","glr = 0.005\n","\n","#%% loading data and functions\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","def cluster_init(x, J=3, K=14, init=1, Rbscale=1e-3, showfig=False):\n","    \"\"\"psudo code, https://www.saedsayad.com/clustering_hierarchical.htm\n","    Given : A set X of obejects{x1,...,xn}\n","            A cluster distance function dist(c1, c2)\n","    for i=1 to n\n","        ci = {xi}\n","    end for\n","    C = {c1, ..., cn}\n","    I = n+1\n","    While I>1 do\n","        (cmin1, cmin2) = minimum dist(ci, cj) for all ci, cj in C\n","        remove cmin1 and cmin2 from C\n","        add {cmin1, cmin2} to C\n","        I = I - 1\n","    end while\n","\n","    However, this naive algorithm does not fit large samples. \n","    Here we use scipy function for the linkage.\n","    J is how many clusters\n","    \"\"\"   \n","    dtype = x.dtype\n","    N, F, M = x.shape\n","\n","    \"get data and clusters ready\"\n","    x_norm = ((x[:,:,None,:]@x[..., None].conj())**0.5)[:,:,0]\n","    if init==1: x_ = x/x_norm * (-1j*x[...,0:1].angle()).exp() # shape of [N, F, M] x_bar\n","    else: x_ = x * (-1j*x[...,0:1].angle()).exp() # the x_tilde in Duong's paper\n","    data = x_.reshape(N*F, M)\n","    I = data.shape[0]\n","    C = [[i] for i in range(I)]  # initial cluster\n","\n","    \"calc. affinity matrix and linkage\"\n","    perms = torch.combinations(torch.arange(len(C)))\n","    d = data[perms]\n","    table = ((d[:,0] - d[:,1]).abs()**2).sum(dim=-1)**0.5\n","    from scipy.cluster.hierarchy import dendrogram, linkage\n","    z = linkage(table, method='average')\n","    if showfig: dn = dendrogram(z, p=3, truncate_mode='level')\n","\n","    \"find the max J cluster and sample index\"\n","    zind = torch.tensor(z).to(torch.int)\n","    flag = torch.cat((torch.ones(I), torch.zeros(I)))\n","    c = C + [[] for i in range(I)]\n","    for i in range(z.shape[0]-K): # threshold of K level to stop\n","        c[i+I] = c[zind[i][0]] + c[zind[i][1]]\n","        flag[i+I], flag[zind[i][0]], flag[zind[i][1]] = 1, 0, 0\n","    ind = (flag == 1).nonzero(as_tuple=True)[0]\n","    dict_c = {}  # which_cluster: how_many_nodes\n","    for i in range(ind.shape[0]):\n","        dict_c[ind[i].item()] = len(c[ind[i]])\n","    dict_c_sorted = {k:v for k,v in sorted(dict_c.items(), key=lambda x: -x[1])}\n","    cs = []\n","    for i, (k,v) in enumerate(dict_c_sorted.items()):\n","        if i == J:\n","            break\n","        cs.append(c[k])\n","\n","    \"initil the EM variables\"\n","    Hhat = torch.rand(M, J, dtype=dtype)\n","    Rj = torch.rand(J, M, M, dtype=dtype)\n","    for i in range(J):\n","        d = data[torch.tensor(cs[i])] # shape of [I_cj, M]\n","        Hhat[:,i] = d.mean(0)\n","        Rj[i] = (d[..., None] @ d[:,None,:].conj()).mean(0)\n","    vhat = torch.ones(N, F, J).abs().to(dtype)\n","    Rb = torch.eye(M).to(dtype)*Rbscale\n","\n","    return vhat, Hhat, Rb, Rj\n","\n","def nem_hci(x, J=6, Hscale=1, Rbscale=1, max_iter=501, seed=1, model=''):\n","\n","    def log_likelihood(x, vhat, Hhat, Rb, ):\n","        \"\"\" Hhat shape of [I, M, J] # I is NO. of samples, M is NO. of antennas, J is NO. of sources\n","            vhat shape of [I, N, F, J]\n","            Rb shape of [I, M, M]\n","            x shape of [I, N, F, M]\n","        \"\"\"\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rcj = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj()\n","        Rxperm = Rcj + Rb \n","        Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","        l = -(np.pi*torch.linalg.det(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","        return l.sum().real, Rs, Rxperm, Rcj\n","\n","    torch.manual_seed(seed) \n","    if model == '':\n","        print('A model is needed')\n","\n","    model = torch.load(model)\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    #%% EM part\n","    \"initial\"        \n","    N, F, M = x.shape\n","    NF= N*F\n","    graw = torch.tensor(resize(x[...,0].abs().numpy(), [8,8], order=1, preserve_range=True))\n","    graw = (graw/graw.max())[None,...]  #standardization shape of [1, 8, 8]\n","    g = torch.stack([graw[:,None] for j in range(J)], dim=1)  # shape of [1,J,8,8]\n","    noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","    g = (g + noise/10).to(torch.float).cuda() \n","    # g = torch.rand(1, J, 1, 8, 8).cuda()\n","    x = x.cuda()\n","\n","    vhat = torch.randn(1, N, F, J).abs().to(torch.cdouble).cuda()\n","    outs = []\n","    for j in range(J):\n","        outs.append(model(g[:,j]))\n","    out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","    vhat.real = threshold(out)\n","    # Hhat = torch.randn(1, M, J).to(torch.cdouble).cuda()*Hscale\n","    Hhat = cluster_init(x.cpu(), J=J)[1].cuda()\n","    Rb = torch.ones(1, M).diag_embed().cuda().to(torch.cdouble)*Rbscale\n","    Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((0,1))/NF\n","    Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","    Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","    g.requires_grad_()\n","    optim_gamma = torch.optim.SGD([g], lr= glr)\n","    ll_traj = []\n","\n","    for ii in range(max_iter): # EM iterations\n","        \"E-step\"\n","        W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","        shat = W.permute(2,0,1,3,4) @ x[...,None]\n","        Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","        Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","        Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","        \"M-step\"\n","        Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","        Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","            Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","        Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","        Rb.imag = Rb.imag - Rb.imag\n","\n","        # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","        # vj.imag = vj.imag - vj.imag\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out, ceiling=1)\n","        loss = loss_func(vhat, Rsshatnf.cuda())\n","        optim_gamma.zero_grad()   \n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","        optim_gamma.step()\n","        torch.cuda.empty_cache()\n","        \n","        \"compute log-likelyhood\"\n","        vhat = vhat.detach()\n","        ll, Rs, Rx, Rcj = log_likelihood(x, vhat, Hhat, Rb)\n","        ll_traj.append(ll.item())\n","        if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","        if ii > 20 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <5e-4:\n","            print(f'EM early stop at iter {ii}')\n","            break\n","\n","    return (shat.cpu(), Hhat.cpu(), vhat.cpu().squeeze()), g.detach().cpu(), Rb.cpu(), ll_traj\n","\n","#%%\n","for db in [30,20,10,5,0]:\n","    res_s, res_h = [], []\n","    for ii in range(100):\n","        which_class, ind = [0,1,2,3,4,5], ii\n","        M, J = 6, len(which_class)\n","        for i, v in enumerate(which_class):\n","            if i == 0 : d = 0\n","            d = d + h[:M, v, None] @ s[ind, v].reshape(1, N*F)\n","        r = d.abs().max()\n","        d = d.reshape(M, N, F).permute(1,2,0)/r\n","        data = awgn(d, db, seed=0)\n","\n","        # rid = 160100\n","        # model = f'../data/nem_ss/models/rid1+/rid{rid}/model_rid{rid}_33.pt'\n","        # shv, g, Rb, loss = nem_func_less(data, J=J, seed=10, model=model, max_iter=301)\n","\n","        rid = 2014\n","        model = f'../data/nem_ss/models/rid{rid}/model_rid{rid}_25.pt'\n","        shv, g, Rb, loss = nem_hci(data, J=J, seed=10, model=model, max_iter=301)\n","\n","        shat, Hhat, vhat = shv\n","        res_s.append(s_corr(shat.squeeze().abs(), s_all[ind].abs()))\n","        res_h.append(h_corr(Hhat.squeeze(), h[:M, which_class]))\n","        print(ii)\n","        print('s_corr', res_s[-1])\n","        print('h_corr', res_h[-1])\n","    print(f's mean at db{db}', sum(res_s)/100)\n","    print(f'h mean at db{db}', sum(res_h)/100)\n","print('done')\n","\n","if False:\n","    for i in range(6):\n","        plt.figure()\n","        plt.imshow((shat.squeeze()[...,i]*Hhat[0,0,i]).abs()*ratio[ind])\n","        plt.colorbar()\n","        # plt.savefig('nem_'+name[i]+'.eps')\n","        plt.show()\n","        # plt.title('plot of s from NEM')\n","\n","\n","    for i, v in enumerate(which_class):\n","        plt.figure()\n","        plt.imshow(s[ind, v].squeeze().abs())\n","        plt.colorbar()\n","        plt.title('GT')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2014_60_M3\n","from utils import *\n","\n","import matplotlib\n","matplotlib.rc('font', size=16)\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 150\n","torch.set_printoptions(linewidth=160)\n","from skimage.transform import resize\n","import itertools\n","import time\n","t = time.time()\n","d, s, h = torch.load('../data/nem_ss/test500M6FT100_xsh.pt')\n","h, N, F = torch.tensor(h), s.shape[-1], s.shape[-2] # h is M*J matrix, here 6*6\n","ratio = d.abs().amax(dim=(1,2,3))\n","x_all = (d/ratio[:,None,None,None]).permute(0,2,3,1)\n","s_all = s.abs().permute(0,2,3,1) \n","glr = 0.005\n","\n","#%% loading data and functions\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","def cluster_init(x, J=3, K=14, init=1, Rbscale=1e-3, showfig=False):\n","    \"\"\"psudo code, https://www.saedsayad.com/clustering_hierarchical.htm\n","    Given : A set X of obejects{x1,...,xn}\n","            A cluster distance function dist(c1, c2)\n","    for i=1 to n\n","        ci = {xi}\n","    end for\n","    C = {c1, ..., cn}\n","    I = n+1\n","    While I>1 do\n","        (cmin1, cmin2) = minimum dist(ci, cj) for all ci, cj in C\n","        remove cmin1 and cmin2 from C\n","        add {cmin1, cmin2} to C\n","        I = I - 1\n","    end while\n","\n","    However, this naive algorithm does not fit large samples. \n","    Here we use scipy function for the linkage.\n","    J is how many clusters\n","    \"\"\"   \n","    dtype = x.dtype\n","    N, F, M = x.shape\n","\n","    \"get data and clusters ready\"\n","    x_norm = ((x[:,:,None,:]@x[..., None].conj())**0.5)[:,:,0]\n","    if init==1: x_ = x/x_norm * (-1j*x[...,0:1].angle()).exp() # shape of [N, F, M] x_bar\n","    else: x_ = x * (-1j*x[...,0:1].angle()).exp() # the x_tilde in Duong's paper\n","    data = x_.reshape(N*F, M)\n","    I = data.shape[0]\n","    C = [[i] for i in range(I)]  # initial cluster\n","\n","    \"calc. affinity matrix and linkage\"\n","    perms = torch.combinations(torch.arange(len(C)))\n","    d = data[perms]\n","    table = ((d[:,0] - d[:,1]).abs()**2).sum(dim=-1)**0.5\n","    from scipy.cluster.hierarchy import dendrogram, linkage\n","    z = linkage(table, method='average')\n","    if showfig: dn = dendrogram(z, p=3, truncate_mode='level')\n","\n","    \"find the max J cluster and sample index\"\n","    zind = torch.tensor(z).to(torch.int)\n","    flag = torch.cat((torch.ones(I), torch.zeros(I)))\n","    c = C + [[] for i in range(I)]\n","    for i in range(z.shape[0]-K): # threshold of K level to stop\n","        c[i+I] = c[zind[i][0]] + c[zind[i][1]]\n","        flag[i+I], flag[zind[i][0]], flag[zind[i][1]] = 1, 0, 0\n","    ind = (flag == 1).nonzero(as_tuple=True)[0]\n","    dict_c = {}  # which_cluster: how_many_nodes\n","    for i in range(ind.shape[0]):\n","        dict_c[ind[i].item()] = len(c[ind[i]])\n","    dict_c_sorted = {k:v for k,v in sorted(dict_c.items(), key=lambda x: -x[1])}\n","    cs = []\n","    for i, (k,v) in enumerate(dict_c_sorted.items()):\n","        if i == J:\n","            break\n","        cs.append(c[k])\n","\n","    \"initil the EM variables\"\n","    Hhat = torch.rand(M, J, dtype=dtype)\n","    Rj = torch.rand(J, M, M, dtype=dtype)\n","    for i in range(J):\n","        d = data[torch.tensor(cs[i])] # shape of [I_cj, M]\n","        Hhat[:,i] = d.mean(0)\n","        Rj[i] = (d[..., None] @ d[:,None,:].conj()).mean(0)\n","    vhat = torch.ones(N, F, J).abs().to(dtype)\n","    Rb = torch.eye(M).to(dtype)*Rbscale\n","\n","    return vhat, Hhat, Rb, Rj\n","\n","def nem_hci(x, J=6, Hscale=1, Rbscale=1, max_iter=501, seed=1, model=''):\n","\n","    def log_likelihood(x, vhat, Hhat, Rb, ):\n","        \"\"\" Hhat shape of [I, M, J] # I is NO. of samples, M is NO. of antennas, J is NO. of sources\n","            vhat shape of [I, N, F, J]\n","            Rb shape of [I, M, M]\n","            x shape of [I, N, F, M]\n","        \"\"\"\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rcj = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj()\n","        Rxperm = Rcj + Rb \n","        Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","        l = -(np.pi*torch.linalg.det(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","        return l.sum().real, Rs, Rxperm, Rcj\n","\n","    torch.manual_seed(seed) \n","    if model == '':\n","        print('A model is needed')\n","\n","    model = torch.load(model)\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    #%% EM part\n","    \"initial\"        \n","    N, F, M = x.shape\n","    NF= N*F\n","    graw = torch.tensor(resize(x[...,0].abs().numpy(), [8,8], order=1, preserve_range=True))\n","    graw = (graw/graw.max())[None,...]  #standardization shape of [1, 8, 8]\n","    g = torch.stack([graw[:,None] for j in range(J)], dim=1)  # shape of [1,J,8,8]\n","    noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","    g = (g + noise/10).to(torch.float).cuda() \n","    # g = torch.rand(1, J, 1, 8, 8).cuda()\n","    x = x.cuda()\n","\n","    vhat = torch.randn(1, N, F, J).abs().to(torch.cdouble).cuda()\n","    outs = []\n","    for j in range(J):\n","        outs.append(model(g[:,j]))\n","    out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","    vhat.real = threshold(out)\n","    # Hhat = torch.randn(1, M, J).to(torch.cdouble).cuda()*Hscale\n","    Hhat = cluster_init(x.cpu(), J=J)[1].cuda()\n","    Rb = torch.ones(1, M).diag_embed().cuda().to(torch.cdouble)*Rbscale\n","    Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((0,1))/NF\n","    Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","    Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","    g.requires_grad_()\n","    optim_gamma = torch.optim.SGD([g], lr= glr)\n","    ll_traj = []\n","\n","    for ii in range(max_iter): # EM iterations\n","        \"E-step\"\n","        W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","        shat = W.permute(2,0,1,3,4) @ x[...,None]\n","        Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","        Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","        Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","        \"M-step\"\n","        Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","        Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","            Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","        Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","        Rb.imag = Rb.imag - Rb.imag\n","\n","        # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","        # vj.imag = vj.imag - vj.imag\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out, ceiling=1)\n","        loss = loss_func(vhat, Rsshatnf.cuda())\n","        optim_gamma.zero_grad()   \n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","        optim_gamma.step()\n","        torch.cuda.empty_cache()\n","        \n","        \"compute log-likelyhood\"\n","        vhat = vhat.detach()\n","        ll, Rs, Rx, Rcj = log_likelihood(x, vhat, Hhat, Rb)\n","        ll_traj.append(ll.item())\n","        if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","        if ii > 20 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <5e-4:\n","            print(f'EM early stop at iter {ii}')\n","            break\n","\n","    return (shat.cpu(), Hhat.cpu(), vhat.cpu().squeeze()), g.detach().cpu(), Rb.cpu(), ll_traj\n","\n","#%%\n","for db in [30,20,10,5,0]:\n","    res_s, res_h = [], []\n","    for ii in range(100):\n","        which_class, ind = [0,1,2,3,4,5], ii\n","        M, J = 3, len(which_class)\n","        for i, v in enumerate(which_class):\n","            if i == 0 : d = 0\n","            d = d + h[:M, v, None] @ s[ind, v].reshape(1, N*F)\n","        r = d.abs().max()\n","        d = d.reshape(M, N, F).permute(1,2,0)/r\n","        data = awgn(d, db, seed=0)\n","\n","        # rid = 160100\n","        # model = f'../data/nem_ss/models/rid1+/rid{rid}/model_rid{rid}_33.pt'\n","        # shv, g, Rb, loss = nem_func_less(data, J=J, seed=10, model=model, max_iter=301)\n","\n","        rid = 2014\n","        model = f'../data/nem_ss/models/rid{rid}/model_rid{rid}_60.pt'\n","        shv, g, Rb, loss = nem_hci(data, J=J, seed=10, model=model, max_iter=301)\n","\n","        shat, Hhat, vhat = shv\n","        res_s.append(s_corr(shat.squeeze().abs(), s_all[ind].abs()))\n","        res_h.append(h_corr(Hhat.squeeze(), h[:M, which_class]))\n","        print(ii)\n","        print('s_corr', res_s[-1])\n","        print('h_corr', res_h[-1])\n","    print(f's mean at db{db}', sum(res_s)/100)\n","    print(f'h mean at db{db}', sum(res_h)/100)\n","print('done')\n","\n","if False:\n","    for i in range(6):\n","        plt.figure()\n","        plt.imshow((shat.squeeze()[...,i]*Hhat[0,0,i]).abs()*ratio[ind])\n","        plt.colorbar()\n","        # plt.savefig('nem_'+name[i]+'.eps')\n","        plt.show()\n","        # plt.title('plot of s from NEM')\n","\n","\n","    for i, v in enumerate(which_class):\n","        plt.figure()\n","        plt.imshow(s[ind, v].squeeze().abs())\n","        plt.colorbar()\n","        plt.title('GT')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2014_60M6\n","from utils import *\n","\n","import matplotlib\n","matplotlib.rc('font', size=16)\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 150\n","torch.set_printoptions(linewidth=160)\n","from skimage.transform import resize\n","import itertools\n","import time\n","t = time.time()\n","d, s, h = torch.load('../data/nem_ss/test500M6FT100_xsh.pt')\n","h, N, F = torch.tensor(h), s.shape[-1], s.shape[-2] # h is M*J matrix, here 6*6\n","ratio = d.abs().amax(dim=(1,2,3))\n","x_all = (d/ratio[:,None,None,None]).permute(0,2,3,1)\n","s_all = s.abs().permute(0,2,3,1) \n","glr = 0.005\n","\n","#%% loading data and functions\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","def cluster_init(x, J=3, K=14, init=1, Rbscale=1e-3, showfig=False):\n","    \"\"\"psudo code, https://www.saedsayad.com/clustering_hierarchical.htm\n","    Given : A set X of obejects{x1,...,xn}\n","            A cluster distance function dist(c1, c2)\n","    for i=1 to n\n","        ci = {xi}\n","    end for\n","    C = {c1, ..., cn}\n","    I = n+1\n","    While I>1 do\n","        (cmin1, cmin2) = minimum dist(ci, cj) for all ci, cj in C\n","        remove cmin1 and cmin2 from C\n","        add {cmin1, cmin2} to C\n","        I = I - 1\n","    end while\n","\n","    However, this naive algorithm does not fit large samples. \n","    Here we use scipy function for the linkage.\n","    J is how many clusters\n","    \"\"\"   \n","    dtype = x.dtype\n","    N, F, M = x.shape\n","\n","    \"get data and clusters ready\"\n","    x_norm = ((x[:,:,None,:]@x[..., None].conj())**0.5)[:,:,0]\n","    if init==1: x_ = x/x_norm * (-1j*x[...,0:1].angle()).exp() # shape of [N, F, M] x_bar\n","    else: x_ = x * (-1j*x[...,0:1].angle()).exp() # the x_tilde in Duong's paper\n","    data = x_.reshape(N*F, M)\n","    I = data.shape[0]\n","    C = [[i] for i in range(I)]  # initial cluster\n","\n","    \"calc. affinity matrix and linkage\"\n","    perms = torch.combinations(torch.arange(len(C)))\n","    d = data[perms]\n","    table = ((d[:,0] - d[:,1]).abs()**2).sum(dim=-1)**0.5\n","    from scipy.cluster.hierarchy import dendrogram, linkage\n","    z = linkage(table, method='average')\n","    if showfig: dn = dendrogram(z, p=3, truncate_mode='level')\n","\n","    \"find the max J cluster and sample index\"\n","    zind = torch.tensor(z).to(torch.int)\n","    flag = torch.cat((torch.ones(I), torch.zeros(I)))\n","    c = C + [[] for i in range(I)]\n","    for i in range(z.shape[0]-K): # threshold of K level to stop\n","        c[i+I] = c[zind[i][0]] + c[zind[i][1]]\n","        flag[i+I], flag[zind[i][0]], flag[zind[i][1]] = 1, 0, 0\n","    ind = (flag == 1).nonzero(as_tuple=True)[0]\n","    dict_c = {}  # which_cluster: how_many_nodes\n","    for i in range(ind.shape[0]):\n","        dict_c[ind[i].item()] = len(c[ind[i]])\n","    dict_c_sorted = {k:v for k,v in sorted(dict_c.items(), key=lambda x: -x[1])}\n","    cs = []\n","    for i, (k,v) in enumerate(dict_c_sorted.items()):\n","        if i == J:\n","            break\n","        cs.append(c[k])\n","\n","    \"initil the EM variables\"\n","    Hhat = torch.rand(M, J, dtype=dtype)\n","    Rj = torch.rand(J, M, M, dtype=dtype)\n","    for i in range(J):\n","        d = data[torch.tensor(cs[i])] # shape of [I_cj, M]\n","        Hhat[:,i] = d.mean(0)\n","        Rj[i] = (d[..., None] @ d[:,None,:].conj()).mean(0)\n","    vhat = torch.ones(N, F, J).abs().to(dtype)\n","    Rb = torch.eye(M).to(dtype)*Rbscale\n","\n","    return vhat, Hhat, Rb, Rj\n","\n","def nem_hci(x, J=6, Hscale=1, Rbscale=1, max_iter=501, seed=1, model=''):\n","\n","    def log_likelihood(x, vhat, Hhat, Rb, ):\n","        \"\"\" Hhat shape of [I, M, J] # I is NO. of samples, M is NO. of antennas, J is NO. of sources\n","            vhat shape of [I, N, F, J]\n","            Rb shape of [I, M, M]\n","            x shape of [I, N, F, M]\n","        \"\"\"\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rcj = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj()\n","        Rxperm = Rcj + Rb \n","        Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","        l = -(np.pi*torch.linalg.det(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","        return l.sum().real, Rs, Rxperm, Rcj\n","\n","    torch.manual_seed(seed) \n","    if model == '':\n","        print('A model is needed')\n","\n","    model = torch.load(model)\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    #%% EM part\n","    \"initial\"        \n","    N, F, M = x.shape\n","    NF= N*F\n","    graw = torch.tensor(resize(x[...,0].abs().numpy(), [8,8], order=1, preserve_range=True))\n","    graw = (graw/graw.max())[None,...]  #standardization shape of [1, 8, 8]\n","    g = torch.stack([graw[:,None] for j in range(J)], dim=1)  # shape of [1,J,8,8]\n","    noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","    g = (g + noise/10).to(torch.float).cuda() \n","    # g = torch.rand(1, J, 1, 8, 8).cuda()\n","    x = x.cuda()\n","\n","    vhat = torch.randn(1, N, F, J).abs().to(torch.cdouble).cuda()\n","    outs = []\n","    for j in range(J):\n","        outs.append(model(g[:,j]))\n","    out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","    vhat.real = threshold(out)\n","    # Hhat = torch.randn(1, M, J).to(torch.cdouble).cuda()*Hscale\n","    Hhat = cluster_init(x.cpu(), J=J)[1].cuda()\n","    Rb = torch.ones(1, M).diag_embed().cuda().to(torch.cdouble)*Rbscale\n","    Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((0,1))/NF\n","    Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","    Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","    g.requires_grad_()\n","    optim_gamma = torch.optim.SGD([g], lr= glr)\n","    ll_traj = []\n","\n","    for ii in range(max_iter): # EM iterations\n","        \"E-step\"\n","        W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","        shat = W.permute(2,0,1,3,4) @ x[...,None]\n","        Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","        Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","        Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","        \"M-step\"\n","        Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","        Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","            Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","        Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","        Rb.imag = Rb.imag - Rb.imag\n","\n","        # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","        # vj.imag = vj.imag - vj.imag\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out, ceiling=1)\n","        loss = loss_func(vhat, Rsshatnf.cuda())\n","        optim_gamma.zero_grad()   \n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","        optim_gamma.step()\n","        torch.cuda.empty_cache()\n","        \n","        \"compute log-likelyhood\"\n","        vhat = vhat.detach()\n","        ll, Rs, Rx, Rcj = log_likelihood(x, vhat, Hhat, Rb)\n","        ll_traj.append(ll.item())\n","        if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","        if ii > 20 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <5e-4:\n","            print(f'EM early stop at iter {ii}')\n","            break\n","\n","    return (shat.cpu(), Hhat.cpu(), vhat.cpu().squeeze()), g.detach().cpu(), Rb.cpu(), ll_traj\n","\n","#%%\n","for db in [30,20,10,5,0]:\n","    res_s, res_h = [], []\n","    for ii in range(100):\n","        which_class, ind = [0,1,2,3,4,5], ii\n","        M, J = 6, len(which_class)\n","        for i, v in enumerate(which_class):\n","            if i == 0 : d = 0\n","            d = d + h[:M, v, None] @ s[ind, v].reshape(1, N*F)\n","        r = d.abs().max()\n","        d = d.reshape(M, N, F).permute(1,2,0)/r\n","        data = awgn(d, db, seed=0)\n","\n","        # rid = 160100\n","        # model = f'../data/nem_ss/models/rid1+/rid{rid}/model_rid{rid}_33.pt'\n","        # shv, g, Rb, loss = nem_func_less(data, J=J, seed=10, model=model, max_iter=301)\n","\n","        rid = 2014\n","        model = f'../data/nem_ss/models/rid{rid}/model_rid{rid}_60.pt'\n","        shv, g, Rb, loss = nem_hci(data, J=J, seed=10, model=model, max_iter=301)\n","\n","        shat, Hhat, vhat = shv\n","        res_s.append(s_corr(shat.squeeze().abs(), s_all[ind].abs()))\n","        res_h.append(h_corr(Hhat.squeeze(), h[:M, which_class]))\n","        print(ii)\n","        print('s_corr', res_s[-1])\n","        print('h_corr', res_h[-1])\n","    print(f's mean at db{db}', sum(res_s)/100)\n","    print(f'h mean at db{db}', sum(res_h)/100)\n","print('done')\n","\n","if False:\n","    for i in range(6):\n","        plt.figure()\n","        plt.imshow((shat.squeeze()[...,i]*Hhat[0,0,i]).abs()*ratio[ind])\n","        plt.colorbar()\n","        # plt.savefig('nem_'+name[i]+'.eps')\n","        plt.show()\n","        # plt.title('plot of s from NEM')\n","\n","\n","    for i, v in enumerate(which_class):\n","        plt.figure()\n","        plt.imshow(s[ind, v].squeeze().abs())\n","        plt.colorbar()\n","        plt.title('GT')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2014_70M3\n","\"\"\"Key varibales are \n","M - how many channels\n","which_class - list of source index, which sources are in the mixture\n","J - the algorithm presumes how many classes in the mixture\n","ind - from 0 to 99, index of test sample\n","max_iter - how many EM iterations\n","seed - random seed number\n","rid - training model index\n","model - saved trained model\n","\"\"\"\n","from utils import *\n","\n","import matplotlib\n","matplotlib.rc('font', size=16)\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 150\n","torch.set_printoptions(linewidth=160)\n","from skimage.transform import resize\n","import itertools\n","import time\n","t = time.time()\n","d, s, h = torch.load('../data/nem_ss/test500M6FT100_xsh.pt')\n","h, N, F = torch.tensor(h), s.shape[-1], s.shape[-2] # h is M*J matrix, here 6*6\n","ratio = d.abs().amax(dim=(1,2,3))\n","x_all = (d/ratio[:,None,None,None]).permute(0,2,3,1)\n","s_all = s.abs().permute(0,2,3,1) \n","glr = 0.005\n","\n","#%% loading data and functions\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","def cluster_init(x, J=3, K=14, init=1, Rbscale=1e-3, showfig=False):\n","    \"\"\"psudo code, https://www.saedsayad.com/clustering_hierarchical.htm\n","    Given : A set X of obejects{x1,...,xn}\n","            A cluster distance function dist(c1, c2)\n","    for i=1 to n\n","        ci = {xi}\n","    end for\n","    C = {c1, ..., cn}\n","    I = n+1\n","    While I>1 do\n","        (cmin1, cmin2) = minimum dist(ci, cj) for all ci, cj in C\n","        remove cmin1 and cmin2 from C\n","        add {cmin1, cmin2} to C\n","        I = I - 1\n","    end while\n","\n","    However, this naive algorithm does not fit large samples. \n","    Here we use scipy function for the linkage.\n","    J is how many clusters\n","    \"\"\"   \n","    dtype = x.dtype\n","    N, F, M = x.shape\n","\n","    \"get data and clusters ready\"\n","    x_norm = ((x[:,:,None,:]@x[..., None].conj())**0.5)[:,:,0]\n","    if init==1: x_ = x/x_norm * (-1j*x[...,0:1].angle()).exp() # shape of [N, F, M] x_bar\n","    else: x_ = x * (-1j*x[...,0:1].angle()).exp() # the x_tilde in Duong's paper\n","    data = x_.reshape(N*F, M)\n","    I = data.shape[0]\n","    C = [[i] for i in range(I)]  # initial cluster\n","\n","    \"calc. affinity matrix and linkage\"\n","    perms = torch.combinations(torch.arange(len(C)))\n","    d = data[perms]\n","    table = ((d[:,0] - d[:,1]).abs()**2).sum(dim=-1)**0.5\n","    from scipy.cluster.hierarchy import dendrogram, linkage\n","    z = linkage(table, method='average')\n","    if showfig: dn = dendrogram(z, p=3, truncate_mode='level')\n","\n","    \"find the max J cluster and sample index\"\n","    zind = torch.tensor(z).to(torch.int)\n","    flag = torch.cat((torch.ones(I), torch.zeros(I)))\n","    c = C + [[] for i in range(I)]\n","    for i in range(z.shape[0]-K): # threshold of K level to stop\n","        c[i+I] = c[zind[i][0]] + c[zind[i][1]]\n","        flag[i+I], flag[zind[i][0]], flag[zind[i][1]] = 1, 0, 0\n","    ind = (flag == 1).nonzero(as_tuple=True)[0]\n","    dict_c = {}  # which_cluster: how_many_nodes\n","    for i in range(ind.shape[0]):\n","        dict_c[ind[i].item()] = len(c[ind[i]])\n","    dict_c_sorted = {k:v for k,v in sorted(dict_c.items(), key=lambda x: -x[1])}\n","    cs = []\n","    for i, (k,v) in enumerate(dict_c_sorted.items()):\n","        if i == J:\n","            break\n","        cs.append(c[k])\n","\n","    \"initil the EM variables\"\n","    Hhat = torch.rand(M, J, dtype=dtype)\n","    Rj = torch.rand(J, M, M, dtype=dtype)\n","    for i in range(J):\n","        d = data[torch.tensor(cs[i])] # shape of [I_cj, M]\n","        Hhat[:,i] = d.mean(0)\n","        Rj[i] = (d[..., None] @ d[:,None,:].conj()).mean(0)\n","    vhat = torch.ones(N, F, J).abs().to(dtype)\n","    Rb = torch.eye(M).to(dtype)*Rbscale\n","\n","    return vhat, Hhat, Rb, Rj\n","\n","def nem_hci(x, J=6, Hscale=1, Rbscale=1, max_iter=501, seed=1, model=''):\n","\n","    def log_likelihood(x, vhat, Hhat, Rb, ):\n","        \"\"\" Hhat shape of [I, M, J] # I is NO. of samples, M is NO. of antennas, J is NO. of sources\n","            vhat shape of [I, N, F, J]\n","            Rb shape of [I, M, M]\n","            x shape of [I, N, F, M]\n","        \"\"\"\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rcj = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj()\n","        Rxperm = Rcj + Rb \n","        Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","        l = -(np.pi*torch.linalg.det(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","        return l.sum().real, Rs, Rxperm, Rcj\n","\n","    torch.manual_seed(seed) \n","    if model == '':\n","        print('A model is needed')\n","\n","    model = torch.load(model)\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    #%% EM part\n","    \"initial\"        \n","    N, F, M = x.shape\n","    NF= N*F\n","    graw = torch.tensor(resize(x[...,0].abs().numpy(), [8,8], order=1, preserve_range=True))\n","    graw = (graw/graw.max())[None,...]  #standardization shape of [1, 8, 8]\n","    g = torch.stack([graw[:,None] for j in range(J)], dim=1)  # shape of [1,J,8,8]\n","    noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","    g = (g + noise/10).to(torch.float).cuda() \n","    # g = torch.rand(1, J, 1, 8, 8).cuda()\n","    x = x.cuda()\n","\n","    vhat = torch.randn(1, N, F, J).abs().to(torch.cdouble).cuda()\n","    outs = []\n","    for j in range(J):\n","        outs.append(model(g[:,j]))\n","    out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","    vhat.real = threshold(out)\n","    # Hhat = torch.randn(1, M, J).to(torch.cdouble).cuda()*Hscale\n","    Hhat = cluster_init(x.cpu(), J=J)[1].cuda()\n","    Rb = torch.ones(1, M).diag_embed().cuda().to(torch.cdouble)*Rbscale\n","    Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((0,1))/NF\n","    Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","    Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","    g.requires_grad_()\n","    optim_gamma = torch.optim.SGD([g], lr= glr)\n","    ll_traj = []\n","\n","    for ii in range(max_iter): # EM iterations\n","        \"E-step\"\n","        W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","        shat = W.permute(2,0,1,3,4) @ x[...,None]\n","        Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","        Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","        Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","        \"M-step\"\n","        Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","        Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","            Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","        Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","        Rb.imag = Rb.imag - Rb.imag\n","\n","        # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","        # vj.imag = vj.imag - vj.imag\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out, ceiling=1)\n","        loss = loss_func(vhat, Rsshatnf.cuda())\n","        optim_gamma.zero_grad()   \n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","        optim_gamma.step()\n","        torch.cuda.empty_cache()\n","        \n","        \"compute log-likelyhood\"\n","        vhat = vhat.detach()\n","        ll, Rs, Rx, Rcj = log_likelihood(x, vhat, Hhat, Rb)\n","        ll_traj.append(ll.item())\n","        if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","        if ii > 20 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <5e-4:\n","            print(f'EM early stop at iter {ii}')\n","            break\n","\n","    return (shat.cpu(), Hhat.cpu(), vhat.cpu().squeeze()), g.detach().cpu(), Rb.cpu(), ll_traj\n","\n","#%%\n","for db in [30,20,10,5,0]:\n","    res_s, res_h = [], []\n","    for ii in range(100):\n","        which_class, ind = [0,1,2,3,4,5], ii\n","        M, J = 3, len(which_class)\n","        for i, v in enumerate(which_class):\n","            if i == 0 : d = 0\n","            d = d + h[:M, v, None] @ s[ind, v].reshape(1, N*F)\n","        r = d.abs().max()\n","        d = d.reshape(M, N, F).permute(1,2,0)/r\n","        data = awgn(d, db, seed=0)\n","\n","        # rid = 160100\n","        # model = f'../data/nem_ss/models/rid1+/rid{rid}/model_rid{rid}_33.pt'\n","        # shv, g, Rb, loss = nem_func_less(data, J=J, seed=10, model=model, max_iter=301)\n","\n","        rid = 2014\n","        model = f'../data/nem_ss/models/rid{rid}/model_rid{rid}_70.pt'\n","        shv, g, Rb, loss = nem_hci(data, J=J, seed=10, model=model, max_iter=301)\n","\n","        shat, Hhat, vhat = shv\n","        res_s.append(s_corr(shat.squeeze().abs(), s_all[ind].abs()))\n","        res_h.append(h_corr(Hhat.squeeze(), h[:M, which_class]))\n","        print(ii)\n","        print('s_corr', res_s[-1])\n","        print('h_corr', res_h[-1])\n","    print(f's mean at db{db}', sum(res_s)/100)\n","    print(f'h mean at db{db}', sum(res_h)/100)\n","print('done')\n","\n","if False:\n","    for i in range(6):\n","        plt.figure()\n","        plt.imshow((shat.squeeze()[...,i]*Hhat[0,0,i]).abs()*ratio[ind])\n","        plt.colorbar()\n","        # plt.savefig('nem_'+name[i]+'.eps')\n","        plt.show()\n","        # plt.title('plot of s from NEM')\n","\n","\n","    for i, v in enumerate(which_class):\n","        plt.figure()\n","        plt.imshow(s[ind, v].squeeze().abs())\n","        plt.colorbar()\n","        plt.title('GT')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2014_70M6\n","from utils import *\n","\n","import matplotlib\n","matplotlib.rc('font', size=16)\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 150\n","torch.set_printoptions(linewidth=160)\n","from skimage.transform import resize\n","import itertools\n","import time\n","t = time.time()\n","d, s, h = torch.load('../data/nem_ss/test500M6FT100_xsh.pt')\n","h, N, F = torch.tensor(h), s.shape[-1], s.shape[-2] # h is M*J matrix, here 6*6\n","ratio = d.abs().amax(dim=(1,2,3))\n","x_all = (d/ratio[:,None,None,None]).permute(0,2,3,1)\n","s_all = s.abs().permute(0,2,3,1) \n","glr = 0.005\n","\n","#%% loading data and functions\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","def cluster_init(x, J=3, K=14, init=1, Rbscale=1e-3, showfig=False):\n","    \"\"\"psudo code, https://www.saedsayad.com/clustering_hierarchical.htm\n","    Given : A set X of obejects{x1,...,xn}\n","            A cluster distance function dist(c1, c2)\n","    for i=1 to n\n","        ci = {xi}\n","    end for\n","    C = {c1, ..., cn}\n","    I = n+1\n","    While I>1 do\n","        (cmin1, cmin2) = minimum dist(ci, cj) for all ci, cj in C\n","        remove cmin1 and cmin2 from C\n","        add {cmin1, cmin2} to C\n","        I = I - 1\n","    end while\n","\n","    However, this naive algorithm does not fit large samples. \n","    Here we use scipy function for the linkage.\n","    J is how many clusters\n","    \"\"\"   \n","    dtype = x.dtype\n","    N, F, M = x.shape\n","\n","    \"get data and clusters ready\"\n","    x_norm = ((x[:,:,None,:]@x[..., None].conj())**0.5)[:,:,0]\n","    if init==1: x_ = x/x_norm * (-1j*x[...,0:1].angle()).exp() # shape of [N, F, M] x_bar\n","    else: x_ = x * (-1j*x[...,0:1].angle()).exp() # the x_tilde in Duong's paper\n","    data = x_.reshape(N*F, M)\n","    I = data.shape[0]\n","    C = [[i] for i in range(I)]  # initial cluster\n","\n","    \"calc. affinity matrix and linkage\"\n","    perms = torch.combinations(torch.arange(len(C)))\n","    d = data[perms]\n","    table = ((d[:,0] - d[:,1]).abs()**2).sum(dim=-1)**0.5\n","    from scipy.cluster.hierarchy import dendrogram, linkage\n","    z = linkage(table, method='average')\n","    if showfig: dn = dendrogram(z, p=3, truncate_mode='level')\n","\n","    \"find the max J cluster and sample index\"\n","    zind = torch.tensor(z).to(torch.int)\n","    flag = torch.cat((torch.ones(I), torch.zeros(I)))\n","    c = C + [[] for i in range(I)]\n","    for i in range(z.shape[0]-K): # threshold of K level to stop\n","        c[i+I] = c[zind[i][0]] + c[zind[i][1]]\n","        flag[i+I], flag[zind[i][0]], flag[zind[i][1]] = 1, 0, 0\n","    ind = (flag == 1).nonzero(as_tuple=True)[0]\n","    dict_c = {}  # which_cluster: how_many_nodes\n","    for i in range(ind.shape[0]):\n","        dict_c[ind[i].item()] = len(c[ind[i]])\n","    dict_c_sorted = {k:v for k,v in sorted(dict_c.items(), key=lambda x: -x[1])}\n","    cs = []\n","    for i, (k,v) in enumerate(dict_c_sorted.items()):\n","        if i == J:\n","            break\n","        cs.append(c[k])\n","\n","    \"initil the EM variables\"\n","    Hhat = torch.rand(M, J, dtype=dtype)\n","    Rj = torch.rand(J, M, M, dtype=dtype)\n","    for i in range(J):\n","        d = data[torch.tensor(cs[i])] # shape of [I_cj, M]\n","        Hhat[:,i] = d.mean(0)\n","        Rj[i] = (d[..., None] @ d[:,None,:].conj()).mean(0)\n","    vhat = torch.ones(N, F, J).abs().to(dtype)\n","    Rb = torch.eye(M).to(dtype)*Rbscale\n","\n","    return vhat, Hhat, Rb, Rj\n","\n","def nem_hci(x, J=6, Hscale=1, Rbscale=1, max_iter=501, seed=1, model=''):\n","\n","    def log_likelihood(x, vhat, Hhat, Rb, ):\n","        \"\"\" Hhat shape of [I, M, J] # I is NO. of samples, M is NO. of antennas, J is NO. of sources\n","            vhat shape of [I, N, F, J]\n","            Rb shape of [I, M, M]\n","            x shape of [I, N, F, M]\n","        \"\"\"\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rcj = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj()\n","        Rxperm = Rcj + Rb \n","        Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","        l = -(np.pi*torch.linalg.det(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","        return l.sum().real, Rs, Rxperm, Rcj\n","\n","    torch.manual_seed(seed) \n","    if model == '':\n","        print('A model is needed')\n","\n","    model = torch.load(model)\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    #%% EM part\n","    \"initial\"        \n","    N, F, M = x.shape\n","    NF= N*F\n","    graw = torch.tensor(resize(x[...,0].abs().numpy(), [8,8], order=1, preserve_range=True))\n","    graw = (graw/graw.max())[None,...]  #standardization shape of [1, 8, 8]\n","    g = torch.stack([graw[:,None] for j in range(J)], dim=1)  # shape of [1,J,8,8]\n","    noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","    g = (g + noise/10).to(torch.float).cuda() \n","    # g = torch.rand(1, J, 1, 8, 8).cuda()\n","    x = x.cuda()\n","\n","    vhat = torch.randn(1, N, F, J).abs().to(torch.cdouble).cuda()\n","    outs = []\n","    for j in range(J):\n","        outs.append(model(g[:,j]))\n","    out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","    vhat.real = threshold(out)\n","    # Hhat = torch.randn(1, M, J).to(torch.cdouble).cuda()*Hscale\n","    Hhat = cluster_init(x.cpu(), J=J)[1].cuda()\n","    Rb = torch.ones(1, M).diag_embed().cuda().to(torch.cdouble)*Rbscale\n","    Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((0,1))/NF\n","    Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","    Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","    g.requires_grad_()\n","    optim_gamma = torch.optim.SGD([g], lr= glr)\n","    ll_traj = []\n","\n","    for ii in range(max_iter): # EM iterations\n","        \"E-step\"\n","        W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","        shat = W.permute(2,0,1,3,4) @ x[...,None]\n","        Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","        Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","        Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","        \"M-step\"\n","        Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","        Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","            Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","        Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","        Rb.imag = Rb.imag - Rb.imag\n","\n","        # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","        # vj.imag = vj.imag - vj.imag\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out, ceiling=1)\n","        loss = loss_func(vhat, Rsshatnf.cuda())\n","        optim_gamma.zero_grad()   \n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","        optim_gamma.step()\n","        torch.cuda.empty_cache()\n","        \n","        \"compute log-likelyhood\"\n","        vhat = vhat.detach()\n","        ll, Rs, Rx, Rcj = log_likelihood(x, vhat, Hhat, Rb)\n","        ll_traj.append(ll.item())\n","        if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","        if ii > 20 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <5e-4:\n","            print(f'EM early stop at iter {ii}')\n","            break\n","\n","    return (shat.cpu(), Hhat.cpu(), vhat.cpu().squeeze()), g.detach().cpu(), Rb.cpu(), ll_traj\n","\n","#%%\n","for db in [30,20,10,5,0]:\n","    res_s, res_h = [], []\n","    for ii in range(100):\n","        which_class, ind = [0,1,2,3,4,5], ii\n","        M, J = 6, len(which_class)\n","        for i, v in enumerate(which_class):\n","            if i == 0 : d = 0\n","            d = d + h[:M, v, None] @ s[ind, v].reshape(1, N*F)\n","        r = d.abs().max()\n","        d = d.reshape(M, N, F).permute(1,2,0)/r\n","        data = awgn(d, db, seed=0)\n","\n","        # rid = 160100\n","        # model = f'../data/nem_ss/models/rid1+/rid{rid}/model_rid{rid}_33.pt'\n","        # shv, g, Rb, loss = nem_func_less(data, J=J, seed=10, model=model, max_iter=301)\n","\n","        rid = 2014\n","        model = f'../data/nem_ss/models/rid{rid}/model_rid{rid}_70.pt'\n","        shv, g, Rb, loss = nem_hci(data, J=J, seed=10, model=model, max_iter=301)\n","\n","        shat, Hhat, vhat = shv\n","        res_s.append(s_corr(shat.squeeze().abs(), s_all[ind].abs()))\n","        res_h.append(h_corr(Hhat.squeeze(), h[:M, which_class]))\n","        print(ii)\n","        print('s_corr', res_s[-1])\n","        print('h_corr', res_h[-1])\n","    print(f's mean at db{db}', sum(res_s)/100)\n","    print(f'h mean at db{db}', sum(res_h)/100)\n","print('done')\n","\n","if False:\n","    for i in range(6):\n","        plt.figure()\n","        plt.imshow((shat.squeeze()[...,i]*Hhat[0,0,i]).abs()*ratio[ind])\n","        plt.colorbar()\n","        # plt.savefig('nem_'+name[i]+'.eps')\n","        plt.show()\n","        # plt.title('plot of s from NEM')\n","\n","\n","    for i, v in enumerate(which_class):\n","        plt.figure()\n","        plt.imshow(s[ind, v].squeeze().abs())\n","        plt.colorbar()\n","        plt.title('GT')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2015_80M3\n","from utils import *\n","\n","import matplotlib\n","matplotlib.rc('font', size=16)\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 150\n","torch.set_printoptions(linewidth=160)\n","from skimage.transform import resize\n","import itertools\n","import time\n","t = time.time()\n","d, s, h = torch.load('../data/nem_ss/test500M6FT100_xsh.pt')\n","h, N, F = torch.tensor(h), s.shape[-1], s.shape[-2] # h is M*J matrix, here 6*6\n","ratio = d.abs().amax(dim=(1,2,3))\n","x_all = (d/ratio[:,None,None,None]).permute(0,2,3,1)\n","s_all = s.abs().permute(0,2,3,1) \n","glr = 0.005\n","\n","#%% loading data and functions\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","def cluster_init(x, J=3, K=14, init=1, Rbscale=1e-3, showfig=False):\n","    \"\"\"psudo code, https://www.saedsayad.com/clustering_hierarchical.htm\n","    Given : A set X of obejects{x1,...,xn}\n","            A cluster distance function dist(c1, c2)\n","    for i=1 to n\n","        ci = {xi}\n","    end for\n","    C = {c1, ..., cn}\n","    I = n+1\n","    While I>1 do\n","        (cmin1, cmin2) = minimum dist(ci, cj) for all ci, cj in C\n","        remove cmin1 and cmin2 from C\n","        add {cmin1, cmin2} to C\n","        I = I - 1\n","    end while\n","\n","    However, this naive algorithm does not fit large samples. \n","    Here we use scipy function for the linkage.\n","    J is how many clusters\n","    \"\"\"   \n","    dtype = x.dtype\n","    N, F, M = x.shape\n","\n","    \"get data and clusters ready\"\n","    x_norm = ((x[:,:,None,:]@x[..., None].conj())**0.5)[:,:,0]\n","    if init==1: x_ = x/x_norm * (-1j*x[...,0:1].angle()).exp() # shape of [N, F, M] x_bar\n","    else: x_ = x * (-1j*x[...,0:1].angle()).exp() # the x_tilde in Duong's paper\n","    data = x_.reshape(N*F, M)\n","    I = data.shape[0]\n","    C = [[i] for i in range(I)]  # initial cluster\n","\n","    \"calc. affinity matrix and linkage\"\n","    perms = torch.combinations(torch.arange(len(C)))\n","    d = data[perms]\n","    table = ((d[:,0] - d[:,1]).abs()**2).sum(dim=-1)**0.5\n","    from scipy.cluster.hierarchy import dendrogram, linkage\n","    z = linkage(table, method='average')\n","    if showfig: dn = dendrogram(z, p=3, truncate_mode='level')\n","\n","    \"find the max J cluster and sample index\"\n","    zind = torch.tensor(z).to(torch.int)\n","    flag = torch.cat((torch.ones(I), torch.zeros(I)))\n","    c = C + [[] for i in range(I)]\n","    for i in range(z.shape[0]-K): # threshold of K level to stop\n","        c[i+I] = c[zind[i][0]] + c[zind[i][1]]\n","        flag[i+I], flag[zind[i][0]], flag[zind[i][1]] = 1, 0, 0\n","    ind = (flag == 1).nonzero(as_tuple=True)[0]\n","    dict_c = {}  # which_cluster: how_many_nodes\n","    for i in range(ind.shape[0]):\n","        dict_c[ind[i].item()] = len(c[ind[i]])\n","    dict_c_sorted = {k:v for k,v in sorted(dict_c.items(), key=lambda x: -x[1])}\n","    cs = []\n","    for i, (k,v) in enumerate(dict_c_sorted.items()):\n","        if i == J:\n","            break\n","        cs.append(c[k])\n","\n","    \"initil the EM variables\"\n","    Hhat = torch.rand(M, J, dtype=dtype)\n","    Rj = torch.rand(J, M, M, dtype=dtype)\n","    for i in range(J):\n","        d = data[torch.tensor(cs[i])] # shape of [I_cj, M]\n","        Hhat[:,i] = d.mean(0)\n","        Rj[i] = (d[..., None] @ d[:,None,:].conj()).mean(0)\n","    vhat = torch.ones(N, F, J).abs().to(dtype)\n","    Rb = torch.eye(M).to(dtype)*Rbscale\n","\n","    return vhat, Hhat, Rb, Rj\n","\n","def nem_hci(x, J=6, Hscale=1, Rbscale=1, max_iter=501, seed=1, model=''):\n","\n","    def log_likelihood(x, vhat, Hhat, Rb, ):\n","        \"\"\" Hhat shape of [I, M, J] # I is NO. of samples, M is NO. of antennas, J is NO. of sources\n","            vhat shape of [I, N, F, J]\n","            Rb shape of [I, M, M]\n","            x shape of [I, N, F, M]\n","        \"\"\"\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rcj = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj()\n","        Rxperm = Rcj + Rb \n","        Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","        l = -(np.pi*torch.linalg.det(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","        return l.sum().real, Rs, Rxperm, Rcj\n","\n","    torch.manual_seed(seed) \n","    if model == '':\n","        print('A model is needed')\n","\n","    model = torch.load(model)\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    #%% EM part\n","    \"initial\"        \n","    N, F, M = x.shape\n","    NF= N*F\n","    graw = torch.tensor(resize(x[...,0].abs().numpy(), [8,8], order=1, preserve_range=True))\n","    graw = (graw/graw.max())[None,...]  #standardization shape of [1, 8, 8]\n","    g = torch.stack([graw[:,None] for j in range(J)], dim=1)  # shape of [1,J,8,8]\n","    # noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","    noise = torch.rand(1, J, 1, 8, 8)\n","    g = (g + noise/10).to(torch.float).cuda() \n","    # g = torch.rand(1, J, 1, 8, 8).cuda()\n","    x = x.cuda()\n","\n","    vhat = torch.randn(1, N, F, J).abs().to(torch.cdouble).cuda()\n","    outs = []\n","    for j in range(J):\n","        outs.append(model(g[:,j]))\n","    out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","    vhat.real = threshold(out)\n","    # Hhat = torch.randn(1, M, J).to(torch.cdouble).cuda()*Hscale\n","    Hhat = cluster_init(x.cpu(), J=J)[1].cuda()\n","    Rb = torch.ones(1, M).diag_embed().cuda().to(torch.cdouble)*Rbscale\n","    Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((0,1))/NF\n","    Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","    Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","    g.requires_grad_()\n","    optim_gamma = torch.optim.SGD([g], lr= glr)\n","    ll_traj = []\n","\n","    for ii in range(max_iter): # EM iterations\n","        \"E-step\"\n","        W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","        shat = W.permute(2,0,1,3,4) @ x[...,None]\n","        Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","        Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","        Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","        \"M-step\"\n","        Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","        Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","            Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","        Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","        Rb.imag = Rb.imag - Rb.imag\n","\n","        # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","        # vj.imag = vj.imag - vj.imag\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out, ceiling=1)\n","        loss = loss_func(vhat, Rsshatnf.cuda())\n","        optim_gamma.zero_grad()   \n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","        optim_gamma.step()\n","        torch.cuda.empty_cache()\n","        \n","        \"compute log-likelyhood\"\n","        vhat = vhat.detach()\n","        ll, Rs, Rx, Rcj = log_likelihood(x, vhat, Hhat, Rb)\n","        ll_traj.append(ll.item())\n","        if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","        if ii > 20 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <5e-4:\n","            print(f'EM early stop at iter {ii}')\n","            break\n","\n","    return (shat.cpu(), Hhat.cpu(), vhat.cpu().squeeze()), g.detach().cpu(), Rb.cpu(), ll_traj\n","\n","#%%\n","for db in [30,20,10,5,0]:\n","    res_s, res_h = [], []\n","    for ii in range(100):\n","        which_class, ind = [0,1,2,3,4,5], ii\n","        M, J = 3, len(which_class)\n","        for i, v in enumerate(which_class):\n","            if i == 0 : d = 0\n","            d = d + h[:M, v, None] @ s[ind, v].reshape(1, N*F)\n","        r = d.abs().max()\n","        d = d.reshape(M, N, F).permute(1,2,0)/r\n","        data = awgn(d, db, seed=0)\n","\n","        # rid = 160100\n","        # model = f'../data/nem_ss/models/rid1+/rid{rid}/model_rid{rid}_33.pt'\n","        # shv, g, Rb, loss = nem_func_less(data, J=J, seed=10, model=model, max_iter=301)\n","\n","        rid = 2015\n","        model = f'../data/nem_ss/models/rid{rid}/model_rid{rid}_80.pt'\n","        shv, g, Rb, loss = nem_hci(data, J=J, seed=10, model=model, max_iter=301)\n","\n","        shat, Hhat, vhat = shv\n","        res_s.append(s_corr(shat.squeeze().abs(), s_all[ind].abs()))\n","        res_h.append(h_corr(Hhat.squeeze(), h[:M, which_class]))\n","        print(ii)\n","        print('s_corr', res_s[-1])\n","        print('h_corr', res_h[-1])\n","    print(f's mean at db{db}', sum(res_s)/100)\n","    print(f'h mean at db{db}', sum(res_h)/100)\n","print('done')\n","\n","if False:\n","    for i in range(6):\n","        plt.figure()\n","        plt.imshow((shat.squeeze()[...,i]*Hhat[0,0,i]).abs()*ratio[ind])\n","        plt.colorbar()\n","        # plt.savefig('nem_'+name[i]+'.eps')\n","        plt.show()\n","        # plt.title('plot of s from NEM')\n","\n","\n","    for i, v in enumerate(which_class):\n","        plt.figure()\n","        plt.imshow(s[ind, v].squeeze().abs())\n","        plt.colorbar()\n","        plt.title('GT')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2015_80M6\n","from utils import *\n","\n","import matplotlib\n","matplotlib.rc('font', size=16)\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 150\n","torch.set_printoptions(linewidth=160)\n","from skimage.transform import resize\n","import itertools\n","import time\n","t = time.time()\n","d, s, h = torch.load('../data/nem_ss/test500M6FT100_xsh.pt')\n","h, N, F = torch.tensor(h), s.shape[-1], s.shape[-2] # h is M*J matrix, here 6*6\n","ratio = d.abs().amax(dim=(1,2,3))\n","x_all = (d/ratio[:,None,None,None]).permute(0,2,3,1)\n","s_all = s.abs().permute(0,2,3,1) \n","glr = 0.005\n","\n","#%% loading data and functions\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","def cluster_init(x, J=3, K=14, init=1, Rbscale=1e-3, showfig=False):\n","    \"\"\"psudo code, https://www.saedsayad.com/clustering_hierarchical.htm\n","    Given : A set X of obejects{x1,...,xn}\n","            A cluster distance function dist(c1, c2)\n","    for i=1 to n\n","        ci = {xi}\n","    end for\n","    C = {c1, ..., cn}\n","    I = n+1\n","    While I>1 do\n","        (cmin1, cmin2) = minimum dist(ci, cj) for all ci, cj in C\n","        remove cmin1 and cmin2 from C\n","        add {cmin1, cmin2} to C\n","        I = I - 1\n","    end while\n","\n","    However, this naive algorithm does not fit large samples. \n","    Here we use scipy function for the linkage.\n","    J is how many clusters\n","    \"\"\"   \n","    dtype = x.dtype\n","    N, F, M = x.shape\n","\n","    \"get data and clusters ready\"\n","    x_norm = ((x[:,:,None,:]@x[..., None].conj())**0.5)[:,:,0]\n","    if init==1: x_ = x/x_norm * (-1j*x[...,0:1].angle()).exp() # shape of [N, F, M] x_bar\n","    else: x_ = x * (-1j*x[...,0:1].angle()).exp() # the x_tilde in Duong's paper\n","    data = x_.reshape(N*F, M)\n","    I = data.shape[0]\n","    C = [[i] for i in range(I)]  # initial cluster\n","\n","    \"calc. affinity matrix and linkage\"\n","    perms = torch.combinations(torch.arange(len(C)))\n","    d = data[perms]\n","    table = ((d[:,0] - d[:,1]).abs()**2).sum(dim=-1)**0.5\n","    from scipy.cluster.hierarchy import dendrogram, linkage\n","    z = linkage(table, method='average')\n","    if showfig: dn = dendrogram(z, p=3, truncate_mode='level')\n","\n","    \"find the max J cluster and sample index\"\n","    zind = torch.tensor(z).to(torch.int)\n","    flag = torch.cat((torch.ones(I), torch.zeros(I)))\n","    c = C + [[] for i in range(I)]\n","    for i in range(z.shape[0]-K): # threshold of K level to stop\n","        c[i+I] = c[zind[i][0]] + c[zind[i][1]]\n","        flag[i+I], flag[zind[i][0]], flag[zind[i][1]] = 1, 0, 0\n","    ind = (flag == 1).nonzero(as_tuple=True)[0]\n","    dict_c = {}  # which_cluster: how_many_nodes\n","    for i in range(ind.shape[0]):\n","        dict_c[ind[i].item()] = len(c[ind[i]])\n","    dict_c_sorted = {k:v for k,v in sorted(dict_c.items(), key=lambda x: -x[1])}\n","    cs = []\n","    for i, (k,v) in enumerate(dict_c_sorted.items()):\n","        if i == J:\n","            break\n","        cs.append(c[k])\n","\n","    \"initil the EM variables\"\n","    Hhat = torch.rand(M, J, dtype=dtype)\n","    Rj = torch.rand(J, M, M, dtype=dtype)\n","    for i in range(J):\n","        d = data[torch.tensor(cs[i])] # shape of [I_cj, M]\n","        Hhat[:,i] = d.mean(0)\n","        Rj[i] = (d[..., None] @ d[:,None,:].conj()).mean(0)\n","    vhat = torch.ones(N, F, J).abs().to(dtype)\n","    Rb = torch.eye(M).to(dtype)*Rbscale\n","\n","    return vhat, Hhat, Rb, Rj\n","\n","def nem_hci(x, J=6, Hscale=1, Rbscale=1, max_iter=501, seed=1, model=''):\n","\n","    def log_likelihood(x, vhat, Hhat, Rb, ):\n","        \"\"\" Hhat shape of [I, M, J] # I is NO. of samples, M is NO. of antennas, J is NO. of sources\n","            vhat shape of [I, N, F, J]\n","            Rb shape of [I, M, M]\n","            x shape of [I, N, F, M]\n","        \"\"\"\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rcj = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj()\n","        Rxperm = Rcj + Rb \n","        Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","        l = -(np.pi*torch.linalg.det(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","        return l.sum().real, Rs, Rxperm, Rcj\n","\n","    torch.manual_seed(seed) \n","    if model == '':\n","        print('A model is needed')\n","\n","    model = torch.load(model)\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    #%% EM part\n","    \"initial\"        \n","    N, F, M = x.shape\n","    NF= N*F\n","    graw = torch.tensor(resize(x[...,0].abs().numpy(), [8,8], order=1, preserve_range=True))\n","    graw = (graw/graw.max())[None,...]  #standardization shape of [1, 8, 8]\n","    g = torch.stack([graw[:,None] for j in range(J)], dim=1)  # shape of [1,J,8,8]\n","    # noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","    noise = torch.rand(1, J, 1, 8, 8)\n","    g = (g + noise/10).to(torch.float).cuda() \n","    # g = torch.rand(1, J, 1, 8, 8).cuda()\n","    x = x.cuda()\n","\n","    vhat = torch.randn(1, N, F, J).abs().to(torch.cdouble).cuda()\n","    outs = []\n","    for j in range(J):\n","        outs.append(model(g[:,j]))\n","    out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","    vhat.real = threshold(out)\n","    # Hhat = torch.randn(1, M, J).to(torch.cdouble).cuda()*Hscale\n","    Hhat = cluster_init(x.cpu(), J=J)[1].cuda()\n","    Rb = torch.ones(1, M).diag_embed().cuda().to(torch.cdouble)*Rbscale\n","    Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((0,1))/NF\n","    Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","    Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","    g.requires_grad_()\n","    optim_gamma = torch.optim.SGD([g], lr= glr)\n","    ll_traj = []\n","\n","    for ii in range(max_iter): # EM iterations\n","        \"E-step\"\n","        W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","        shat = W.permute(2,0,1,3,4) @ x[...,None]\n","        Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","        Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","        Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","        \"M-step\"\n","        Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","        Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","            Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","        Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","        Rb.imag = Rb.imag - Rb.imag\n","\n","        # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","        # vj.imag = vj.imag - vj.imag\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out, ceiling=1)\n","        loss = loss_func(vhat, Rsshatnf.cuda())\n","        optim_gamma.zero_grad()   \n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","        optim_gamma.step()\n","        torch.cuda.empty_cache()\n","        \n","        \"compute log-likelyhood\"\n","        vhat = vhat.detach()\n","        ll, Rs, Rx, Rcj = log_likelihood(x, vhat, Hhat, Rb)\n","        ll_traj.append(ll.item())\n","        if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","        if ii > 20 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <5e-4:\n","            print(f'EM early stop at iter {ii}')\n","            break\n","\n","    return (shat.cpu(), Hhat.cpu(), vhat.cpu().squeeze()), g.detach().cpu(), Rb.cpu(), ll_traj\n","\n","#%%\n","for db in [30,20,10,5,0]:\n","    res_s, res_h = [], []\n","    for ii in range(100):\n","        which_class, ind = [0,1,2,3,4,5], ii\n","        M, J = 6, len(which_class)\n","        for i, v in enumerate(which_class):\n","            if i == 0 : d = 0\n","            d = d + h[:M, v, None] @ s[ind, v].reshape(1, N*F)\n","        r = d.abs().max()\n","        d = d.reshape(M, N, F).permute(1,2,0)/r\n","        data = awgn(d, db, seed=0)\n","\n","        # rid = 160100\n","        # model = f'../data/nem_ss/models/rid1+/rid{rid}/model_rid{rid}_33.pt'\n","        # shv, g, Rb, loss = nem_func_less(data, J=J, seed=10, model=model, max_iter=301)\n","\n","        rid = 2015\n","        model = f'../data/nem_ss/models/rid{rid}/model_rid{rid}_80.pt'\n","        shv, g, Rb, loss = nem_hci(data, J=J, seed=10, model=model, max_iter=301)\n","\n","        shat, Hhat, vhat = shv\n","        res_s.append(s_corr(shat.squeeze().abs(), s_all[ind].abs()))\n","        res_h.append(h_corr(Hhat.squeeze(), h[:M, which_class]))\n","        print(ii)\n","        print('s_corr', res_s[-1])\n","        print('h_corr', res_h[-1])\n","    print(f's mean at db{db}', sum(res_s)/100)\n","    print(f'h mean at db{db}', sum(res_h)/100)\n","print('done')\n","\n","if False:\n","    for i in range(6):\n","        plt.figure()\n","        plt.imshow((shat.squeeze()[...,i]*Hhat[0,0,i]).abs()*ratio[ind])\n","        plt.colorbar()\n","        # plt.savefig('nem_'+name[i]+'.eps')\n","        plt.show()\n","        # plt.title('plot of s from NEM')\n","\n","\n","    for i, v in enumerate(which_class):\n","        plt.figure()\n","        plt.imshow(s[ind, v].squeeze().abs())\n","        plt.colorbar()\n","        plt.title('GT')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2016_48M3\n","from utils import *\n","\n","import matplotlib\n","matplotlib.rc('font', size=16)\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 150\n","torch.set_printoptions(linewidth=160)\n","from skimage.transform import resize\n","import itertools\n","import time\n","t = time.time()\n","d, s, h = torch.load('../data/nem_ss/test500M6FT100_xsh.pt')\n","h, N, F = torch.tensor(h), s.shape[-1], s.shape[-2] # h is M*J matrix, here 6*6\n","ratio = d.abs().amax(dim=(1,2,3))\n","x_all = (d/ratio[:,None,None,None]).permute(0,2,3,1)\n","s_all = s.abs().permute(0,2,3,1) \n","glr = 0.005\n","\n","#%% loading data and functions\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","def cluster_init(x, J=3, K=14, init=1, Rbscale=1e-3, showfig=False):\n","    \"\"\"psudo code, https://www.saedsayad.com/clustering_hierarchical.htm\n","    Given : A set X of obejects{x1,...,xn}\n","            A cluster distance function dist(c1, c2)\n","    for i=1 to n\n","        ci = {xi}\n","    end for\n","    C = {c1, ..., cn}\n","    I = n+1\n","    While I>1 do\n","        (cmin1, cmin2) = minimum dist(ci, cj) for all ci, cj in C\n","        remove cmin1 and cmin2 from C\n","        add {cmin1, cmin2} to C\n","        I = I - 1\n","    end while\n","\n","    However, this naive algorithm does not fit large samples. \n","    Here we use scipy function for the linkage.\n","    J is how many clusters\n","    \"\"\"   \n","    dtype = x.dtype\n","    N, F, M = x.shape\n","\n","    \"get data and clusters ready\"\n","    x_norm = ((x[:,:,None,:]@x[..., None].conj())**0.5)[:,:,0]\n","    if init==1: x_ = x/x_norm * (-1j*x[...,0:1].angle()).exp() # shape of [N, F, M] x_bar\n","    else: x_ = x * (-1j*x[...,0:1].angle()).exp() # the x_tilde in Duong's paper\n","    data = x_.reshape(N*F, M)\n","    I = data.shape[0]\n","    C = [[i] for i in range(I)]  # initial cluster\n","\n","    \"calc. affinity matrix and linkage\"\n","    perms = torch.combinations(torch.arange(len(C)))\n","    d = data[perms]\n","    table = ((d[:,0] - d[:,1]).abs()**2).sum(dim=-1)**0.5\n","    from scipy.cluster.hierarchy import dendrogram, linkage\n","    z = linkage(table, method='average')\n","    if showfig: dn = dendrogram(z, p=3, truncate_mode='level')\n","\n","    \"find the max J cluster and sample index\"\n","    zind = torch.tensor(z).to(torch.int)\n","    flag = torch.cat((torch.ones(I), torch.zeros(I)))\n","    c = C + [[] for i in range(I)]\n","    for i in range(z.shape[0]-K): # threshold of K level to stop\n","        c[i+I] = c[zind[i][0]] + c[zind[i][1]]\n","        flag[i+I], flag[zind[i][0]], flag[zind[i][1]] = 1, 0, 0\n","    ind = (flag == 1).nonzero(as_tuple=True)[0]\n","    dict_c = {}  # which_cluster: how_many_nodes\n","    for i in range(ind.shape[0]):\n","        dict_c[ind[i].item()] = len(c[ind[i]])\n","    dict_c_sorted = {k:v for k,v in sorted(dict_c.items(), key=lambda x: -x[1])}\n","    cs = []\n","    for i, (k,v) in enumerate(dict_c_sorted.items()):\n","        if i == J:\n","            break\n","        cs.append(c[k])\n","\n","    \"initil the EM variables\"\n","    Hhat = torch.rand(M, J, dtype=dtype)\n","    Rj = torch.rand(J, M, M, dtype=dtype)\n","    for i in range(J):\n","        d = data[torch.tensor(cs[i])] # shape of [I_cj, M]\n","        Hhat[:,i] = d.mean(0)\n","        Rj[i] = (d[..., None] @ d[:,None,:].conj()).mean(0)\n","    vhat = torch.ones(N, F, J).abs().to(dtype)\n","    Rb = torch.eye(M).to(dtype)*Rbscale\n","\n","    return vhat, Hhat, Rb, Rj\n","\n","def nem_hci(x, J=6, Hscale=1, Rbscale=1, max_iter=501, seed=1, model=''):\n","\n","    def log_likelihood(x, vhat, Hhat, Rb, ):\n","        \"\"\" Hhat shape of [I, M, J] # I is NO. of samples, M is NO. of antennas, J is NO. of sources\n","            vhat shape of [I, N, F, J]\n","            Rb shape of [I, M, M]\n","            x shape of [I, N, F, M]\n","        \"\"\"\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rcj = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj()\n","        Rxperm = Rcj + Rb \n","        Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","        l = -(np.pi*torch.linalg.det(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","        return l.sum().real, Rs, Rxperm, Rcj\n","\n","    torch.manual_seed(seed) \n","    if model == '':\n","        print('A model is needed')\n","\n","    model = torch.load(model)\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    #%% EM part\n","    \"initial\"        \n","    N, F, M = x.shape\n","    NF= N*F\n","    graw = torch.tensor(resize(x[...,0].abs().numpy(), [8,8], order=1, preserve_range=True))\n","    graw = (graw/graw.max())[None,...]  #standardization shape of [1, 8, 8]\n","    g = torch.stack([graw[:,None] for j in range(J)], dim=1)  # shape of [1,J,8,8]\n","    # noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","    noise = torch.rand(1, J, 1, 8, 8)\n","    g = (g + noise/10).to(torch.float).cuda() \n","    # g = torch.rand(1, J, 1, 8, 8).cuda()\n","    x = x.cuda()\n","\n","    vhat = torch.randn(1, N, F, J).abs().to(torch.cdouble).cuda()\n","    outs = []\n","    for j in range(J):\n","        outs.append(model(g[:,j]))\n","    out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","    vhat.real = threshold(out)\n","    # Hhat = torch.randn(1, M, J).to(torch.cdouble).cuda()*Hscale\n","    Hhat = cluster_init(x.cpu(), J=J)[1].cuda()\n","    Rb = torch.ones(1, M).diag_embed().cuda().to(torch.cdouble)*Rbscale\n","    Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((0,1))/NF\n","    Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","    Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","    g.requires_grad_()\n","    optim_gamma = optim.RAdam([g],\n","            lr= glr,\n","            betas=(0.9, 0.999), \n","            eps=1e-8,\n","            weight_decay=0)\n","    ll_traj = []\n","\n","    for ii in range(max_iter): # EM iterations\n","        \"E-step\"\n","        W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","        shat = W.permute(2,0,1,3,4) @ x[...,None]\n","        Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","        Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","        Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","        \"M-step\"\n","        Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","        Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","            Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","        Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","        Rb.imag = Rb.imag - Rb.imag\n","\n","        # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","        # vj.imag = vj.imag - vj.imag\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out, ceiling=1)\n","        loss = loss_func(vhat, Rsshatnf.cuda())\n","        optim_gamma.zero_grad()   \n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","        optim_gamma.step()\n","        torch.cuda.empty_cache()\n","        \n","        \"compute log-likelyhood\"\n","        vhat = vhat.detach()\n","        ll, Rs, Rx, Rcj = log_likelihood(x, vhat, Hhat, Rb)\n","        ll_traj.append(ll.item())\n","        if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","        if ii > 20 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <5e-4:\n","            print(f'EM early stop at iter {ii}')\n","            break\n","\n","    return (shat.cpu(), Hhat.cpu(), vhat.cpu().squeeze()), g.detach().cpu(), Rb.cpu(), ll_traj\n","\n","#%%\n","for db in [30,20,10,5,0]:\n","    res_s, res_h = [], []\n","    for ii in range(100):\n","        which_class, ind = [0,1,2,3,4,5], ii\n","        M, J = 3, len(which_class)\n","        for i, v in enumerate(which_class):\n","            if i == 0 : d = 0\n","            d = d + h[:M, v, None] @ s[ind, v].reshape(1, N*F)\n","        r = d.abs().max()\n","        d = d.reshape(M, N, F).permute(1,2,0)/r\n","        data = awgn(d, db, seed=0)\n","\n","        # rid = 160100\n","        # model = f'../data/nem_ss/models/rid1+/rid{rid}/model_rid{rid}_33.pt'\n","        # shv, g, Rb, loss = nem_func_less(data, J=J, seed=10, model=model, max_iter=301)\n","\n","        rid = 2016\n","        model = f'../data/nem_ss/models/rid{rid}/model_rid{rid}_48.pt'\n","        shv, g, Rb, loss = nem_hci(data, J=J, seed=10, model=model, max_iter=301)\n","\n","        shat, Hhat, vhat = shv\n","        res_s.append(s_corr(shat.squeeze().abs(), s_all[ind].abs()))\n","        res_h.append(h_corr(Hhat.squeeze(), h[:M, which_class]))\n","        print(ii)\n","        print('s_corr', res_s[-1])\n","        print('h_corr', res_h[-1])\n","    print(f's mean at db{db}', sum(res_s)/100)\n","    print(f'h mean at db{db}', sum(res_h)/100)\n","print('done')\n","\n","if False:\n","    for i in range(6):\n","        plt.figure()\n","        plt.imshow((shat.squeeze()[...,i]*Hhat[0,0,i]).abs()*ratio[ind])\n","        plt.colorbar()\n","        # plt.savefig('nem_'+name[i]+'.eps')\n","        plt.show()\n","        # plt.title('plot of s from NEM')\n","\n","\n","    for i, v in enumerate(which_class):\n","        plt.figure()\n","        plt.imshow(s[ind, v].squeeze().abs())\n","        plt.colorbar()\n","        plt.title('GT')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#%% r2016_48M6\n","from utils import *\n","\n","import matplotlib\n","matplotlib.rc('font', size=16)\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 150\n","torch.set_printoptions(linewidth=160)\n","from skimage.transform import resize\n","import itertools\n","import time\n","t = time.time()\n","d, s, h = torch.load('../data/nem_ss/test500M6FT100_xsh.pt')\n","h, N, F = torch.tensor(h), s.shape[-1], s.shape[-2] # h is M*J matrix, here 6*6\n","ratio = d.abs().amax(dim=(1,2,3))\n","x_all = (d/ratio[:,None,None,None]).permute(0,2,3,1)\n","s_all = s.abs().permute(0,2,3,1) \n","glr = 0.005\n","\n","#%% loading data and functions\n","from unet.unet_model import *\n","class NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        n_channels = 1\n","        n_classes = 1\n","        bilinear = False\n","        n_ch = 384\n","        self.inc = DoubleConv(n_channels, n_ch)\n","        self.up1 = MyUp(n_ch, n_ch//2)\n","        self.up2 = MyUp(n_ch//2, n_ch//4)\n","        self.up3 = MyUp(n_ch//4, n_ch//8)\n","        self.up4 = MyUp(n_ch//8, n_ch//16)\n","        self.reshape = nn.Sequential(\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//16, kernel_size=5, dilation=3),\n","            nn.BatchNorm2d(n_ch//16),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//16, n_ch//8, kernel_size=3, dilation=2),\n","            nn.BatchNorm2d(n_ch//8),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(n_ch//8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(inplace=True))\n","        self.outc = OutConv(32, n_classes)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.inc(x)\n","        x = self.up1(x)\n","        x = self.up2(x)\n","        x = self.up3(x)\n","        x = self.up4(x)\n","        x = self.reshape(x) \n","        x = self.outc(x)\n","        x = self.sig(x)\n","        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n","        return out\n","\n","def cluster_init(x, J=3, K=14, init=1, Rbscale=1e-3, showfig=False):\n","    \"\"\"psudo code, https://www.saedsayad.com/clustering_hierarchical.htm\n","    Given : A set X of obejects{x1,...,xn}\n","            A cluster distance function dist(c1, c2)\n","    for i=1 to n\n","        ci = {xi}\n","    end for\n","    C = {c1, ..., cn}\n","    I = n+1\n","    While I>1 do\n","        (cmin1, cmin2) = minimum dist(ci, cj) for all ci, cj in C\n","        remove cmin1 and cmin2 from C\n","        add {cmin1, cmin2} to C\n","        I = I - 1\n","    end while\n","\n","    However, this naive algorithm does not fit large samples. \n","    Here we use scipy function for the linkage.\n","    J is how many clusters\n","    \"\"\"   \n","    dtype = x.dtype\n","    N, F, M = x.shape\n","\n","    \"get data and clusters ready\"\n","    x_norm = ((x[:,:,None,:]@x[..., None].conj())**0.5)[:,:,0]\n","    if init==1: x_ = x/x_norm * (-1j*x[...,0:1].angle()).exp() # shape of [N, F, M] x_bar\n","    else: x_ = x * (-1j*x[...,0:1].angle()).exp() # the x_tilde in Duong's paper\n","    data = x_.reshape(N*F, M)\n","    I = data.shape[0]\n","    C = [[i] for i in range(I)]  # initial cluster\n","\n","    \"calc. affinity matrix and linkage\"\n","    perms = torch.combinations(torch.arange(len(C)))\n","    d = data[perms]\n","    table = ((d[:,0] - d[:,1]).abs()**2).sum(dim=-1)**0.5\n","    from scipy.cluster.hierarchy import dendrogram, linkage\n","    z = linkage(table, method='average')\n","    if showfig: dn = dendrogram(z, p=3, truncate_mode='level')\n","\n","    \"find the max J cluster and sample index\"\n","    zind = torch.tensor(z).to(torch.int)\n","    flag = torch.cat((torch.ones(I), torch.zeros(I)))\n","    c = C + [[] for i in range(I)]\n","    for i in range(z.shape[0]-K): # threshold of K level to stop\n","        c[i+I] = c[zind[i][0]] + c[zind[i][1]]\n","        flag[i+I], flag[zind[i][0]], flag[zind[i][1]] = 1, 0, 0\n","    ind = (flag == 1).nonzero(as_tuple=True)[0]\n","    dict_c = {}  # which_cluster: how_many_nodes\n","    for i in range(ind.shape[0]):\n","        dict_c[ind[i].item()] = len(c[ind[i]])\n","    dict_c_sorted = {k:v for k,v in sorted(dict_c.items(), key=lambda x: -x[1])}\n","    cs = []\n","    for i, (k,v) in enumerate(dict_c_sorted.items()):\n","        if i == J:\n","            break\n","        cs.append(c[k])\n","\n","    \"initil the EM variables\"\n","    Hhat = torch.rand(M, J, dtype=dtype)\n","    Rj = torch.rand(J, M, M, dtype=dtype)\n","    for i in range(J):\n","        d = data[torch.tensor(cs[i])] # shape of [I_cj, M]\n","        Hhat[:,i] = d.mean(0)\n","        Rj[i] = (d[..., None] @ d[:,None,:].conj()).mean(0)\n","    vhat = torch.ones(N, F, J).abs().to(dtype)\n","    Rb = torch.eye(M).to(dtype)*Rbscale\n","\n","    return vhat, Hhat, Rb, Rj\n","\n","def nem_hci(x, J=6, Hscale=1, Rbscale=1, max_iter=501, seed=1, model=''):\n","\n","    def log_likelihood(x, vhat, Hhat, Rb, ):\n","        \"\"\" Hhat shape of [I, M, J] # I is NO. of samples, M is NO. of antennas, J is NO. of sources\n","            vhat shape of [I, N, F, J]\n","            Rb shape of [I, M, M]\n","            x shape of [I, N, F, M]\n","        \"\"\"\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rcj = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj()\n","        Rxperm = Rcj + Rb \n","        Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n","        l = -(np.pi*torch.linalg.det(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n","        return l.sum().real, Rs, Rxperm, Rcj\n","\n","    torch.manual_seed(seed) \n","    if model == '':\n","        print('A model is needed')\n","\n","    model = torch.load(model)\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    #%% EM part\n","    \"initial\"        \n","    N, F, M = x.shape\n","    NF= N*F\n","    graw = torch.tensor(resize(x[...,0].abs().numpy(), [8,8], order=1, preserve_range=True))\n","    graw = (graw/graw.max())[None,...]  #standardization shape of [1, 8, 8]\n","    g = torch.stack([graw[:,None] for j in range(J)], dim=1)  # shape of [1,J,8,8]\n","    # noise = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","    noise = torch.rand(1, J, 1, 8, 8)\n","    g = (g + noise/10).to(torch.float).cuda() \n","    # g = torch.rand(1, J, 1, 8, 8).cuda()\n","    x = x.cuda()\n","\n","    vhat = torch.randn(1, N, F, J).abs().to(torch.cdouble).cuda()\n","    outs = []\n","    for j in range(J):\n","        outs.append(model(g[:,j]))\n","    out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","    vhat.real = threshold(out)\n","    # Hhat = torch.randn(1, M, J).to(torch.cdouble).cuda()*Hscale\n","    Hhat = cluster_init(x.cpu(), J=J)[1].cuda()\n","    Rb = torch.ones(1, M).diag_embed().cuda().to(torch.cdouble)*Rbscale\n","    Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((0,1))/NF\n","    Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","    Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","    g.requires_grad_()\n","    optim_gamma = optim.RAdam([g],\n","            lr= glr,\n","            betas=(0.9, 0.999), \n","            eps=1e-8,\n","            weight_decay=0)\n","    ll_traj = []\n","\n","    for ii in range(max_iter): # EM iterations\n","        \"E-step\"\n","        W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","        shat = W.permute(2,0,1,3,4) @ x[...,None]\n","        Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","        Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","        Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","        \"M-step\"\n","        Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","        Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","            Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","        Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","        Rb.imag = Rb.imag - Rb.imag\n","\n","        # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","        # vj.imag = vj.imag - vj.imag\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(g[:,j]))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out, ceiling=1)\n","        loss = loss_func(vhat, Rsshatnf.cuda())\n","        optim_gamma.zero_grad()   \n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","        optim_gamma.step()\n","        torch.cuda.empty_cache()\n","        \n","        \"compute log-likelyhood\"\n","        vhat = vhat.detach()\n","        ll, Rs, Rx, Rcj = log_likelihood(x, vhat, Hhat, Rb)\n","        ll_traj.append(ll.item())\n","        if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","        if ii > 20 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <5e-4:\n","            print(f'EM early stop at iter {ii}')\n","            break\n","\n","    return (shat.cpu(), Hhat.cpu(), vhat.cpu().squeeze()), g.detach().cpu(), Rb.cpu(), ll_traj\n","\n","#%%\n","for db in [30,20,10,5,0]:\n","    res_s, res_h = [], []\n","    for ii in range(100):\n","        which_class, ind = [0,1,2,3,4,5], ii\n","        M, J = 6, len(which_class)\n","        for i, v in enumerate(which_class):\n","            if i == 0 : d = 0\n","            d = d + h[:M, v, None] @ s[ind, v].reshape(1, N*F)\n","        r = d.abs().max()\n","        d = d.reshape(M, N, F).permute(1,2,0)/r\n","        data = awgn(d, db, seed=0)\n","\n","        # rid = 160100\n","        # model = f'../data/nem_ss/models/rid1+/rid{rid}/model_rid{rid}_33.pt'\n","        # shv, g, Rb, loss = nem_func_less(data, J=J, seed=10, model=model, max_iter=301)\n","\n","        rid = 2016\n","        model = f'../data/nem_ss/models/rid{rid}/model_rid{rid}_48.pt'\n","        shv, g, Rb, loss = nem_hci(data, J=J, seed=10, model=model, max_iter=301)\n","\n","        shat, Hhat, vhat = shv\n","        res_s.append(s_corr(shat.squeeze().abs(), s_all[ind].abs()))\n","        res_h.append(h_corr(Hhat.squeeze(), h[:M, which_class]))\n","        print(ii)\n","        print('s_corr', res_s[-1])\n","        print('h_corr', res_h[-1])\n","    print(f's mean at db{db}', sum(res_s)/100)\n","    print(f'h mean at db{db}', sum(res_h)/100)\n","print('done')\n","\n","if False:\n","    for i in range(6):\n","        plt.figure()\n","        plt.imshow((shat.squeeze()[...,i]*Hhat[0,0,i]).abs()*ratio[ind])\n","        plt.colorbar()\n","        # plt.savefig('nem_'+name[i]+'.eps')\n","        plt.show()\n","        # plt.title('plot of s from NEM')\n","\n","\n","    for i, v in enumerate(which_class):\n","        plt.figure()\n","        plt.imshow(s[ind, v].squeeze().abs())\n","        plt.colorbar()\n","        plt.title('GT')\n","\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPpmgFYLsgSaMlipMa8R5/N","collapsed_sections":["NVnL-nCeBA3t","3jW2Qw0aymjk","uPI2xhUkPW6M","BqCh4vrkSS2C","w_il05ERz3u0","hyhzT1CIcERI","RsvZxNctBe2D"],"name":"[]nem_realdata_dynamic_record.ipynb","provenance":[{"file_id":"1rEeDE0AIi8lEjzIdIoiysB5PVijO7f1Z","timestamp":1627578797711},{"file_id":"1sL0NuzGnbFzk2Y5XY_n4woZ0c4lJBoEy","timestamp":1627388962384},{"file_id":"1QdweiBwMNieeGBdDFCzphgwJoB7PW0Xo","timestamp":1619782442433},{"file_id":"1W6TyLRUPhBNNN7bLUJ2u4rX4K7CBEjCU","timestamp":1619749565297},{"file_id":"1N5hj6oGZ8XxOExPPtxDCML9bchrleb2O","timestamp":1619749345089},{"file_id":"1vI54wk15P3rTcruo_pRVWX2GldhPqU1D","timestamp":1619744297962},{"file_id":"1HyayE3FINrKpEPXQ5hCl3yQ6S_gaJRF7","timestamp":1619187752515},{"file_id":"137mHbM4Vq4YtcOF9la2sOul4aNWZwRzn","timestamp":1619037563775}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
