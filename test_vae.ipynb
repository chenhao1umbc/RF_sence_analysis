{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"3class best is s71 epoch 400\"\"\"\n",
    "#%%\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx12):\n",
    "    ind = torch.tril_indices(3,3)\n",
    "    indx = np.diag_indices(3)\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet_s10(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(42, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 64),\n",
    "            nn.Linear(64, 42)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, padding=(1,2)),\n",
    "            nn.GroupNorm(num_groups=max(8//4,1), num_channels=8),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(M,M)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-3\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 3, 64, 66, 3\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 701\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/test1kM3FT64_xsh_data3.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "# model = NNet_s10(M,J,N).cuda()\n",
    "model = torch.load('../data/data_ss/models/s71/model_epoch400.pt')\n",
    "\n",
    "#%%\n",
    "print('Start date time ', datetime.now())\n",
    "model.eval()\n",
    "hall, sall = [], []\n",
    "with torch.no_grad():\n",
    "    for snr in ['inf', 20, 10, 5, 0]:\n",
    "        av_hcorr, av_scorr = [], []\n",
    "        for i, (x, s, h) in enumerate(dval):\n",
    "            print(snr, i)\n",
    "            if snr != 'inf':\n",
    "                x = awgn_batch(x, snr)\n",
    "            xval_cuda = x.cuda()\n",
    "            Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)                \n",
    "            Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "            shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                    @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "            shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "            for ind in range(x.shape[0]):\n",
    "                hh = Hhat_val[ind]\n",
    "                av_hcorr.append(h_corr_cuda(hh, h[ind].cuda()).cpu().item())\n",
    "                av_scorr.append(s_corr_cuda(s[ind:ind+1].abs().cuda(), \\\n",
    "                    shat[ind:ind+1].cuda()).cpu().item())\n",
    "        hall.append(sum(av_hcorr)/len(av_hcorr))\n",
    "        sall.append(sum(av_scorr)/len(av_scorr))\n",
    "print(hall, sall)\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenhao1/.miniconda3/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading\n",
      "starting date time  2022-09-30 16:54:20.243067\n",
      "Start date time  2022-09-30 16:54:25.081644\n",
      "inf 0\n",
      "inf 1\n",
      "inf 2\n",
      "inf 3\n",
      "inf 4\n",
      "20 0\n",
      "20 1\n",
      "20 2\n",
      "20 3\n",
      "20 4\n",
      "10 0\n",
      "10 1\n",
      "10 2\n",
      "10 3\n",
      "10 4\n",
      "5 0\n",
      "5 1\n",
      "5 2\n",
      "5 3\n",
      "5 4\n",
      "0 0\n",
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "0 4\n",
      "[0.9317586681842804, 0.9141999424695969, 0.8536779200434684, 0.8209835113584996, 0.7884571696221828] [0.8303824057281017, 0.7857984720468522, 0.714805782109499, 0.6411964830458164, 0.5110661704540252]\n",
      "done\n",
      "End date time  2022-09-30 16:56:43.616660\n"
     ]
    }
   ],
   "source": [
    "\"\"\"6class best is s76ca epoch 660\"\"\"\n",
    "#%%\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx42):\n",
    "    ind = torch.tril_indices(6,6)\n",
    "    indx = np.diag_indices(6)\n",
    "    rx_inv_hat = torch.zeros(rx42.shape[0], 6, 6, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx42[:, :21] + 1j*rx42[:,21:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet_s10(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(42, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 64),\n",
    "            nn.Linear(64, 42)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, padding=(1,2)),\n",
    "            nn.GroupNorm(num_groups=max(8//4,1), num_channels=8),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(M,M)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-3\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 6, 64, 66, 6\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 701\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/test1kM6FT64_xsh_data4.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "# model = NNet_s10(M,J,N).cuda()\n",
    "model = torch.load('../data/data_ss/models/s76ca/model_epoch660.pt')\n",
    "\n",
    "#%%\n",
    "print('Start date time ', datetime.now())\n",
    "model.eval()\n",
    "hall, sall = [], []\n",
    "with torch.no_grad():\n",
    "    for snr in ['inf', 20, 10, 5, 0]:\n",
    "        av_hcorr, av_scorr = [], []\n",
    "        for i, (x, s, h) in enumerate(dval):\n",
    "            print(snr, i)\n",
    "            if snr != 'inf':\n",
    "                x = awgn_batch(x, snr)\n",
    "            xval_cuda = x.cuda()\n",
    "            Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)                \n",
    "            Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "            shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                    @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "            shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "            for ind in range(x.shape[0]):\n",
    "                hh = Hhat_val[ind]\n",
    "                av_hcorr.append(h_corr_cuda(hh, h[ind].cuda()).cpu().item())\n",
    "                av_scorr.append(s_corr_cuda(s[ind:ind+1].abs().cuda(), \\\n",
    "                    shat[ind:ind+1].cuda()).cpu().item())\n",
    "        hall.append(sum(av_hcorr)/len(av_hcorr))\n",
    "        sall.append(sum(av_scorr)/len(av_scorr))\n",
    "print(hall, sall)\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c728c5ad72b5fd3c6e083c4badca00ca04470578383c0e9d983163c40aa43e1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
