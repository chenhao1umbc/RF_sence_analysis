{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s30\n",
    "rid = 's30' # running id\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from vae_model import *\n",
    "class NN_s5(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        self.hnet = nn.Sequential(\n",
    "            nn.Linear(18,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64,M*2)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Up_(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        Is = torch.stack([torch.eye(3,3)]*btsize, dim=0).to(x.dtype).cuda()\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        eps0 = 1e-5\n",
    "        x0 = x.permute(0,2,3,1)[...,None]\n",
    "        Rx = x0 @ x.permute(0,2,3,1).conj()[...,None,:]\n",
    "        rx = Rx.mean(dim=(1,2)) # shape of [I,M,M]\n",
    "        rx_inv0 = rx.pinverse()\n",
    "        for i in range(self.J):\n",
    "            if i == 0:\n",
    "                rx_inv = rx_inv0\n",
    "            else:\n",
    "                w = rx_inv@hhat[...,None] / \\\n",
    "                        (eps0+ hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "                p = Is - (hhat[...,None]@w.permute(0,2,1).conj()).detach()\n",
    "                rx = p@rx@p.permute(0,2,1).conj()\n",
    "                rx_inv = rx.pinverse()\n",
    "            temp = self.hnet(torch.stack((rx.real, rx.imag), dim=1).reshape(btsize,-1))\n",
    "            hhat = temp[:,:M] +1j*temp[:,M:]\n",
    "            hhat = hhat/hhat.detach()[:,0:1]\n",
    "            h_all.append(hhat)\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "\n",
    "        b = x0\n",
    "        for i in range(self.J):\n",
    "            hhat = Hhat[:,:,i]\n",
    "            w = rx_inv0@hhat[...,None] / \\\n",
    "                    (hhat[:,None,:].conj()@rx_inv0@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@x0\n",
    "            b = b - shat*hhat[:, None,None,:,None]\n",
    "        \n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, beta=0.1):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "\n",
    "    try:\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "    except:\n",
    "        torch.save((x, Rx, Rs, Hhat, Rb), f'rid{rid}x_Rx_Rs_Hhat_Rb.pt')\n",
    "        print('error happpened, data saved and stop')\n",
    "        ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze()\n",
    "    return -ll.mean().real, beta*kl \n",
    "\n",
    "#%% load data\n",
    "I = 3000 # how many samples\n",
    "M, N, F, J = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 2001\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh_ang6915-30.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat).cuda()\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_s5\n",
    "model = NN(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "    loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "    loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "\n",
    "            hh = Hhat[0].detach()\n",
    "            rs0 = Rs[0].detach() \n",
    "            Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[0]\n",
    "            shat = (rs0 @ hh.conj().t() @ Rx.inverse()@xval_cuda.permute(0,2,3,1)[0,:,:,:, None]).cpu() \n",
    "            for ii in range(J):\n",
    "                plt.figure()\n",
    "                plt.imshow(shat[:,:,ii,0].abs())\n",
    "                plt.title(f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources-{ii}')\n",
    "                plt.show()\n",
    "\n",
    "                # plt.figure()\n",
    "                # plt.imshow(rs0[:,:,ii, ii].abs().cpu())\n",
    "                # plt.title(f'Epoch{epoch}_estimated V-{ii}')\n",
    "                # plt.savefig(fig_loc + f'Epoch{epoch}_estimated V-{ii}')\n",
    "                # plt.show()\n",
    "                plt.close('all')\n",
    "            print('h_corr', h_corr(hh, hgt[0]))\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s31\n",
    "rid = 's31' # running id\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "if torch.__version__[:5] != '1.8.1':\n",
    "    def mydet(x):\n",
    "        return x.det()\n",
    "    RAdam = torch.optim.RAdam\n",
    "else:\n",
    "    RAdam = optim.RAdam\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "from vae_model import *\n",
    "class NN_s6(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, K=3, im_size=100):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        self.hnet = nn.Sequential(\n",
    "            nn.Linear(18,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64,1)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(25*25, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, 25*25),\n",
    "            Reshape(-1, 1, 25, 25),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Up_(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=4),\n",
    "            OutConv(in_channels=4, out_channels=1),\n",
    "            ) \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, _, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        x0 = x.permute(0,2,3,1)[...,None]\n",
    "        Rx = x0 @ x.permute(0,2,3,1).conj()[...,None,:]\n",
    "        rx = Rx.mean(dim=(1,2)) # shape of [I,M,M]\n",
    "        rx_inv0 = rx.pinverse()\n",
    "        b = x0\n",
    "        for i in range(self.J):\n",
    "            if i > 0 :\n",
    "                rx = (b@b.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            ang = self.hnet(torch.stack((rx.real, rx.imag), dim=1).reshape(btsize,-1))\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()\n",
    "            h_all.append(hhat)\n",
    "        \n",
    "            w = rx_inv0@hhat[...,None] / \\\n",
    "                    (hhat[:,None,:].conj()@rx_inv0@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@x0\n",
    "            b = b - shat*hhat[:, None,None,:,None]\n",
    "        \n",
    "            \"Encoder\"\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_all.append(z)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square()\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, K]\n",
    "        Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        vhat = torch.stack(v_all, 4).squeeze().to(torch.cfloat) # shape:[I, N, F, K]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1e-3):\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*mydet(Rx)).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    # 'fisher'\n",
    "    # _, J, D = zall.shape\n",
    "    # m = zall.mean(dim=(0,1))\n",
    "    # sw, sb = 0, 0\n",
    "    # for j in range(J):\n",
    "    #     z = zall[:,j]\n",
    "    #     mj = z.mean(0)\n",
    "    #     sw = sw + ((z - mj)[...,None]@(z - mj)[:,None]).sum(0)\n",
    "    #     sb = sb + D*(mj-m)[...,None]@(mj-m)[None,:]\n",
    "    # term = 1e-3*(sw-sb).trace()\n",
    "\n",
    "    return -ll.mean().real, beta*kl\n",
    "\n",
    "#%% load data\n",
    "I = 6000 # how many samples\n",
    "M, N, F, J = 3, 100, 100, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['lr'] = 1e-3\n",
    "opts['n_epochs'] = 501\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr9kM3FT100_ang6915-30.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, _ , hgt0 = torch.load('../data/nem_ss/val500M3FT100_xsh_ang6915-30.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "NN = NN_s6\n",
    "model = NN(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "\n",
    "            av_hcorr = []\n",
    "            for ind in range(128):\n",
    "                hh = Hhat[ind].detach()\n",
    "                av_hcorr.append(h_corr(hh.cpu(), hgt[ind]))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged128:', sum(av_hcorr)/128)\n",
    "\n",
    "            plt.figure()\n",
    "            for ind in range(3):\n",
    "                hh = Hhat[ind].detach()\n",
    "                rs0 = Rs[ind].detach() \n",
    "                Rx = hh @ rs0 @ hh.conj().t() + Rb.detach()[ind]\n",
    "                shat = (rs0 @ hh.conj().t() @ Rx.inverse()@xval_cuda.permute(0,2,3,1)[ind,:,:,:, None]).cpu() \n",
    "                for ii in range(J):\n",
    "                    plt.subplot(3,3,ii+1+ind*3)\n",
    "                    plt.imshow(shat[:,:,ii,0].abs())\n",
    "                    # plt.tight_layout(pad=1.1)\n",
    "                    # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "            plt.show()\n",
    "            plt.close('all')\n",
    "            \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s35\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's35' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx12):\n",
    "    ind = torch.tril_indices(3,3)\n",
    "    indx = np.diag_indices(3)\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(12, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 32),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(3,3)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-3\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()/I\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl  #+ 10*loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 6000 # how many samples\n",
    "M, N, F, J = 3, 64, 64, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['n_epochs'] = 301\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr9kM3FT64_data0.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, sval, hgt0 = torch.load('../data/nem_ss/val500M3FT64_xsh_data0.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat)\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "\n",
    "            av_hcorr, av_scorr = [], []\n",
    "            Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "            shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                    @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "            shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "            for ind in range(128):\n",
    "                hh = Hhat_val[ind]\n",
    "                av_hcorr.append(h_corr(hh.cpu(), hgt[ind]))\n",
    "                av_scorr.append(s_corr(sval[ind].abs(), shat[ind]))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged128:', sum(av_hcorr)/128)\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged128:', sum(av_scorr)/128)\n",
    "\n",
    "            plt.figure()\n",
    "            for ind in range(3):\n",
    "                for ii in range(J):\n",
    "                    plt.subplot(3,3,ii+1+ind*3)\n",
    "                    plt.imshow(shat[ind,:,:,ii])\n",
    "                    # plt.tight_layout(pad=1.1)\n",
    "                    # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "            plt.show()\n",
    "            plt.close('all')\n",
    "            \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s52\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's52' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx12):\n",
    "    ind = torch.tril_indices(3,3)\n",
    "    indx = np.diag_indices(3)\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(12, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 32),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(3,3)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-3 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-5\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()/I\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl  #+ 10*loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 6000 # how many samples\n",
    "M, N, F, J = 3, 48, 48, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['n_epochs'] = 301\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr9kM3FT48_data0.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, sval, hgt0 = torch.load('../data/nem_ss/val500M3FT48_xsh_data0.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat)\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "\n",
    "            av_hcorr, av_scorr = [], []\n",
    "            Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "            shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                    @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "            shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "            for ind in range(128):\n",
    "                hh = Hhat_val[ind]\n",
    "                av_hcorr.append(h_corr(hh.cpu(), hgt[ind]))\n",
    "                av_scorr.append(s_corr(sval[ind].abs(), shat[ind]))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged128:', sum(av_hcorr)/128)\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged128:', sum(av_scorr)/128)\n",
    "\n",
    "            plt.figure()\n",
    "            for ind in range(3):\n",
    "                for ii in range(J):\n",
    "                    plt.subplot(3,3,ii+1+ind*3)\n",
    "                    plt.imshow(shat[ind,:,:,ii])\n",
    "                    # plt.tight_layout(pad=1.1)\n",
    "                    # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "            plt.show()\n",
    "            plt.close('all')\n",
    "            \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s54\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's54' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx12):\n",
    "    ind = torch.tril_indices(3,3)\n",
    "    indx = np.diag_indices(3)\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(12, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 32),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(3,3)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-3\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    \"Slot contrastive loss\"\n",
    "    inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl + loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 6000 # how many samples\n",
    "M, N, F, J = 3, 64, 64, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['n_epochs'] = 301\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr9kM3FT64_data0.pt')\n",
    "d = awgn_batch(d, snr=30, seed=1)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval, sval, hgt0 = torch.load('../data/nem_ss/val500M3FT64_xsh_data0.pt')\n",
    "hgt = torch.tensor(hgt0).to(torch.cfloat)\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "\n",
    "            av_hcorr, av_scorr = [], []\n",
    "            Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "            shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                    @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "            shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "            for ind in range(128):\n",
    "                hh = Hhat_val[ind]\n",
    "                av_hcorr.append(h_corr(hh.cpu(), hgt[ind]))\n",
    "                av_scorr.append(s_corr(sval[ind].abs(), shat[ind]))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged128:', sum(av_hcorr)/128)\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged128:', sum(av_scorr)/128)\n",
    "\n",
    "            plt.figure()\n",
    "            for ind in range(3):\n",
    "                for ii in range(J):\n",
    "                    plt.subplot(3,3,ii+1+ind*3)\n",
    "                    plt.imshow(shat[ind,:,:,ii])\n",
    "                    # plt.tight_layout(pad=1.1)\n",
    "                    # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "            plt.show()\n",
    "            plt.close('all')\n",
    "            \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s59\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's59' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx12):\n",
    "    ind = torch.tril_indices(3,3)\n",
    "    indx = np.diag_indices(3)\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(12, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 32),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(3,3)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-3\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl #+ loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 3, 64, 64, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 201\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM3FT64_data1.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/val1kM3FT64_xsh_data1.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s, h) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                     \n",
    "                Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "                shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                        @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "                shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "                for ind in range(x.shape[0]):\n",
    "                    hh = Hhat_val[ind]\n",
    "                    av_hcorr.append(h_corr(hh.cpu(), h[ind]))\n",
    "                    av_scorr.append(s_corr(s[ind].abs(), shat[ind]))\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,3,ii+1+ind*3)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged:', sum(av_hcorr)/len(av_hcorr))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_hcorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s61_1\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's61_1' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx12):\n",
    "    ind = torch.tril_indices(3,3)\n",
    "    indx = np.diag_indices(3)\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(12, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 32),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(3,3)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-3\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl #+ loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 3, 64, 64, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 501\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM3FT64_data1.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/val1kM3FT64_xsh_data1.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s, h) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                     \n",
    "                Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "                shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                        @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "                shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "                for ind in range(x.shape[0]):\n",
    "                    hh = Hhat_val[ind]\n",
    "                    av_hcorr.append(h_corr(hh.cpu(), h[ind]))\n",
    "                    av_scorr.append(s_corr(s[ind].abs(), shat[ind]))\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,3,ii+1+ind*3)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged:', sum(av_hcorr)/len(av_hcorr))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_hcorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s62\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's62' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx12):\n",
    "    ind = torch.tril_indices(3,3)\n",
    "    indx = np.diag_indices(3)\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(12, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 32),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(3,3)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-4\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl #+ loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 3, 64, 64, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 501\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM3FT64_data1.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/val1kM3FT64_xsh_data1.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s, h) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                     \n",
    "                Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "                shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                        @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "                shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "                for ind in range(x.shape[0]):\n",
    "                    hh = Hhat_val[ind]\n",
    "                    av_hcorr.append(h_corr(hh.cpu(), h[ind]))\n",
    "                    av_scorr.append(s_corr(s[ind].abs(), shat[ind]))\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,3,ii+1+ind*3)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged:', sum(av_hcorr)/len(av_hcorr))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_hcorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s65\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's65' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx12):\n",
    "    ind = torch.tril_indices(3,3)\n",
    "    indx = np.diag_indices(3)\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(12, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 32),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(3,3)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-3\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl #+ loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 3, 64, 64, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 701\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM3FT64_data2.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/val1kM3FT64_xsh_data2.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s, h) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                     \n",
    "                Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "                shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                        @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "                shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "                for ind in range(x.shape[0]):\n",
    "                    hh = Hhat_val[ind]\n",
    "                    av_hcorr.append(h_corr(hh.cpu(), h[ind]))\n",
    "                    av_scorr.append(s_corr(s[ind].abs(), shat[ind]))\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,3,ii+1+ind*3)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged:', sum(av_hcorr)/len(av_hcorr))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_hcorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s67\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's67' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx12):\n",
    "    ind = torch.tril_indices(3,3)\n",
    "    indx = np.diag_indices(3)\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(12, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 32),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(3,3)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-3\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl #+ loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 3, 64, 64, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 701\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM3FT64_data0.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/val1kM3FT64_xsh_data0.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s, h) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                     \n",
    "                Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "                shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                        @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "                shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "                for ind in range(x.shape[0]):\n",
    "                    hh = Hhat_val[ind]\n",
    "                    av_hcorr.append(h_corr(hh.cpu(), h[ind]))\n",
    "                    av_scorr.append(s_corr(s[ind].abs(), shat[ind]))\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,3,ii+1+ind*3)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged:', sum(av_hcorr)/len(av_hcorr))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_hcorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s69\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's69' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx12):\n",
    "    ind = torch.tril_indices(3,3)\n",
    "    indx = np.diag_indices(3)\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet_s10(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(12, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 32),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, padding=(1,2)),\n",
    "            nn.GroupNorm(num_groups=max(8//4,1), num_channels=8),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(3,3)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-3\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl #+ loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 3, 64, 66, 3\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 701\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM3FT64_data3.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/val1kM3FT64_xsh_data3.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet_s10(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s, h) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                     \n",
    "                Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "                shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                        @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "                shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "                for ind in range(x.shape[0]):\n",
    "                    hh = Hhat_val[ind]\n",
    "                    av_hcorr.append(h_corr(hh.cpu(), h[ind]))\n",
    "                    av_scorr.append(s_corr(s[ind].abs(), shat[ind]))\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,3,ii+1+ind*3)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged:', sum(av_hcorr)/len(av_hcorr))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_hcorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s71\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's71' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx12):\n",
    "    ind = torch.tril_indices(3,3)\n",
    "    indx = np.diag_indices(3)\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet_s10(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(12, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 32),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, padding=(1,2)),\n",
    "            nn.GroupNorm(num_groups=max(8//4,1), num_channels=8),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(3,3)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-4\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl #+ loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 3, 64, 66, 3\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 701\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM3FT64_data3.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/val1kM3FT64_xsh_data3.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet_s10(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s, h) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                     \n",
    "                Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "                shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                        @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "                shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "                for ind in range(x.shape[0]):\n",
    "                    hh = Hhat_val[ind]\n",
    "                    av_hcorr.append(h_corr(hh.cpu(), h[ind]))\n",
    "                    av_scorr.append(s_corr(s[ind].abs(), shat[ind]))\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,3,ii+1+ind*3)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged:', sum(av_hcorr)/len(av_hcorr))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_hcorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s71_\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's71_' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx12):\n",
    "    ind = torch.tril_indices(3,3)\n",
    "    indx = np.diag_indices(3)\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet_s11(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(12, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 32),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=(1,2)),\n",
    "            nn.GroupNorm(num_groups=max(16//4,1), num_channels=16),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(3,3)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-4\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl #+ loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 3, 64, 66, 3\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 701\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM3FT64_data3.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/val1kM3FT64_xsh_data3.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet_s11(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s, h) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                     \n",
    "                Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "                shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                        @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "                shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "                for ind in range(x.shape[0]):\n",
    "                    hh = Hhat_val[ind]\n",
    "                    av_hcorr.append(h_corr(hh.cpu(), h[ind]))\n",
    "                    av_scorr.append(s_corr(s[ind].abs(), shat[ind]))\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,3,ii+1+ind*3)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged:', sum(av_hcorr)/len(av_hcorr))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_hcorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s75\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's75' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx42):\n",
    "    ind = torch.tril_indices(6,6)\n",
    "    indx = np.diag_indices(6)\n",
    "    rx_inv_hat = torch.zeros(rx42.shape[0], 6, 6, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx42[:, :21] + 1j*rx42[:,21:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet_s10(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(42, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 64),\n",
    "            nn.Linear(64, 42)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, padding=(1,2)),\n",
    "            nn.GroupNorm(num_groups=max(8//4,1), num_channels=8),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(M,M)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-3\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl #+ loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 6, 64, 66, 6\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 701\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM6FT64_data3.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/val1kM6FT64_xsh_data3.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet_s10(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s, h) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                     \n",
    "                Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "                shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                        @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "                shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "                for ind in range(x.shape[0]):\n",
    "                    hh = Hhat_val[ind]\n",
    "                    av_hcorr.append(h_corr(hh.cpu(), h[ind]))\n",
    "                    av_scorr.append(s_corr(s[ind].abs(), shat[ind]))\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,J,ii+1+ind*J)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged:', sum(av_hcorr)/len(av_hcorr))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_hcorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s76ca\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's76ca' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx42):\n",
    "    ind = torch.tril_indices(6,6)\n",
    "    indx = np.diag_indices(6)\n",
    "    rx_inv_hat = torch.zeros(rx42.shape[0], 6, 6, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx42[:, :21] + 1j*rx42[:,21:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet_s10(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(42, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 64),\n",
    "            nn.Linear(64, 42)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, padding=(1,2)),\n",
    "            nn.GroupNorm(num_groups=max(8//4,1), num_channels=8),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(M,M)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-3\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl #+ loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 6, 64, 66, 6\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 701\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM6FT64_data3.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/val1kM6FT64_xsh_data3.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "# model = NNet_s10(M,J,N).cuda()\n",
    "model = torch.load('../data/data_ss/models/s75/model_epoch400.pt')\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s, h) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                     \n",
    "                Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "                shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                        @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "                shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "                for ind in range(x.shape[0]):\n",
    "                    hh = Hhat_val[ind]\n",
    "                    av_hcorr.append(h_corr_cuda(hh, h[ind].cuda()).cpu())\n",
    "                    av_scorr.append(s_corr_cuda(s[ind:ind+1].abs().cuda(), shat[ind:ind+1].cuda()).cpu())\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,J,ii+1+ind*J)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged:', sum(av_hcorr)/len(av_hcorr))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_hcorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s76s\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's76s' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx42):\n",
    "    ind = torch.tril_indices(6,6)\n",
    "    indx = np.diag_indices(6)\n",
    "    rx_inv_hat = torch.zeros(rx42.shape[0], 6, 6, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx42[:, :21] + 1j*rx42[:,21:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet_s10(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(42, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 64),\n",
    "            nn.Linear(64, 42)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, padding=(1,2)),\n",
    "            nn.GroupNorm(num_groups=max(8//4,1), num_channels=8),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(M,M)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-3\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    \"Slot contrastive loss\"\n",
    "    inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl + loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 6, 64, 66, 6\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 701\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM6FT64_data3.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/val1kM6FT64_xsh_data3.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet_s10(M,J,N).cuda()\n",
    "# model = torch.load('../data/data_ss/models/s75/model_epoch400.pt')\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s, h) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                     \n",
    "                Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "                shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                        @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "                shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "                for ind in range(x.shape[0]):\n",
    "                    hh = Hhat_val[ind]\n",
    "                    av_hcorr.append(h_corr_cuda(hh, h[ind].cuda()).cpu().item())\n",
    "                    av_scorr.append(s_corr_cuda(s[ind:ind+1].abs().cuda(), \\\n",
    "                        shat[ind:ind+1].cuda()).cpu().item())\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,J,ii+1+ind*J)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged:', sum(av_hcorr)/len(av_hcorr))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_hcorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s76s5\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's76s5' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx42):\n",
    "    ind = torch.tril_indices(6,6)\n",
    "    indx = np.diag_indices(6)\n",
    "    rx_inv_hat = torch.zeros(rx42.shape[0], 6, 6, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx42[:, :21] + 1j*rx42[:,21:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet_s10(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(42, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 64),\n",
    "            nn.Linear(64, 42)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, padding=(1,2)),\n",
    "            nn.GroupNorm(num_groups=max(8//4,1), num_channels=8),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(M,M)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-3\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    \"Slot contrastive loss\"\n",
    "    inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl + loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 6, 64, 66, 6\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 701\n",
    "opts['lr'] = 1e-4\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM6FT64_data3.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/val1kM6FT64_xsh_data3.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet_s10(M,J,N).cuda()\n",
    "# model = torch.load('../data/data_ss/models/s75/model_epoch400.pt')\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s, h) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                     \n",
    "                Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "                shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                        @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "                shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "                for ind in range(x.shape[0]):\n",
    "                    hh = Hhat_val[ind]\n",
    "                    av_hcorr.append(h_corr_cuda(hh, h[ind].cuda()).cpu().item())\n",
    "                    av_scorr.append(s_corr_cuda(s[ind:ind+1].abs().cuda(), \\\n",
    "                        shat[ind:ind+1].cuda()).cpu().item())\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,J,ii+1+ind*J)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged:', sum(av_hcorr)/len(av_hcorr))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_hcorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s76sc\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's76sc' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx42):\n",
    "    ind = torch.tril_indices(6,6)\n",
    "    indx = np.diag_indices(6)\n",
    "    rx_inv_hat = torch.zeros(rx42.shape[0], 6, 6, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx42[:, :21] + 1j*rx42[:,21:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet_s10(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(42, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 64),\n",
    "            nn.Linear(64, 42)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, padding=(1,2)),\n",
    "            nn.GroupNorm(num_groups=max(8//4,1), num_channels=8),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(M,M)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-3\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    \"Slot contrastive loss\"\n",
    "    inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl + loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 6, 64, 66, 6\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 701\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM6FT64_data3.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/val1kM6FT64_xsh_data3.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet_s10(M,J,N).cuda()\n",
    "model = torch.load('../data/data_ss/models/s75/model_epoch400.pt')\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s, h) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                     \n",
    "                Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "                shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                        @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "                shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "                for ind in range(x.shape[0]):\n",
    "                    hh = Hhat_val[ind]\n",
    "                    av_hcorr.append(h_corr_cuda(hh, h[ind].cuda()).cpu().item())\n",
    "                    av_scorr.append(s_corr_cuda(s[ind:ind+1].abs().cuda(), \\\n",
    "                        shat[ind:ind+1].cuda()).cpu().item())\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,J,ii+1+ind*J)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged:', sum(av_hcorr)/len(av_hcorr))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_hcorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s77\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's77' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx12):\n",
    "    ind = torch.tril_indices(3,3)\n",
    "    indx = np.diag_indices(3)\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet_s10(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(12, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 32),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, padding=(1,2)),\n",
    "            nn.GroupNorm(num_groups=max(8//4,1), num_channels=8),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(3,3)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "        \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        b = xj\n",
    "        Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        # eye = torch.eye(M, device='cuda')\n",
    "        # Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-4\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl #+ loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 3, 64, 66, 3\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 701\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM3FT64_data3.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/val1kM3FT64_xsh_data3.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet_s10(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s, h) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                     \n",
    "                Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "                shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                        @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "                shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "                for ind in range(x.shape[0]):\n",
    "                    hh = Hhat_val[ind]\n",
    "                    av_hcorr.append(h_corr(hh.cpu(), h[ind]))\n",
    "                    av_scorr.append(s_corr(s[ind].abs(), shat[ind]))\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,3,ii+1+ind*3)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged:', sum(av_hcorr)/len(av_hcorr))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_hcorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s79\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's79' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx12):\n",
    "    ind = torch.tril_indices(3,3)\n",
    "    indx = np.diag_indices(3)\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet_s11(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(12, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 32),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=(1,2)),\n",
    "            nn.GroupNorm(num_groups=max(16//4,1), num_channels=16),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(3,3)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        b = xj\n",
    "        Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        # eye = torch.eye(M, device='cuda')\n",
    "        # Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-4\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl #+ loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 3, 64, 66, 3\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 701\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM3FT64_data3.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/val1kM3FT64_xsh_data3.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet_s11(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s, h) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                     \n",
    "                Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "                shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                        @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "                shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "                for ind in range(x.shape[0]):\n",
    "                    hh = Hhat_val[ind]\n",
    "                    av_hcorr.append(h_corr(hh.cpu(), h[ind]))\n",
    "                    av_scorr.append(s_corr(s[ind].abs(), shat[ind]))\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,3,ii+1+ind*3)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged:', sum(av_hcorr)/len(av_hcorr))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_hcorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s81\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's81' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx42):\n",
    "    ind = torch.tril_indices(6,6)\n",
    "    indx = np.diag_indices(6)\n",
    "    rx_inv_hat = torch.zeros(rx42.shape[0], 6, 6, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx42[:, :21] + 1j*rx42[:,21:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet_s10(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(42, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 64),\n",
    "            nn.Linear(64, 42)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, padding=(1,2)),\n",
    "            nn.GroupNorm(num_groups=max(8//4,1), num_channels=8),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(M,M)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        b = xj\n",
    "        Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        # eye = torch.eye(M, device='cuda')\n",
    "        # Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-4\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -Rx.det().real.log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze().real \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl #+ loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 6, 64, 66, 6\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 16\n",
    "opts['n_epochs'] = 901\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM6FT64_data3.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/val1kM6FT64_xsh_data3.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet_s10(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s, h) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                 \n",
    "                Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "\n",
    "                shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                        @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "                shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "\n",
    "                for ind in range(x.shape[0]):\n",
    "                    hh = Hhat_val[ind]\n",
    "                    av_hcorr.append(h_corr(hh.cpu(), h[ind]))\n",
    "                    av_scorr.append(s_corr(s[ind].abs(), shat[ind]))\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,J,ii+1+ind*J)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged:', sum(av_hcorr)/len(av_hcorr))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_hcorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s81_\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's81_' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx42):\n",
    "    ind = torch.tril_indices(6,6)\n",
    "    indx = np.diag_indices(6)\n",
    "    rx_inv_hat = torch.zeros(rx42.shape[0], 6, 6, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx42[:, :21] + 1j*rx42[:,21:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet_s10_(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        n_feat, n_channel = 192, 96\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(42, n_feat),\n",
    "            FC_layer_g(n_feat, n_feat),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(n_feat, n_feat),\n",
    "            FC_layer_g(n_feat, n_feat),\n",
    "            FC_layer_g(n_feat, n_feat//2),\n",
    "            nn.Linear(n_feat//2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(n_feat, n_feat),\n",
    "            FC_layer_g(n_feat, n_feat),\n",
    "            FC_layer_g(n_feat, n_feat//2),\n",
    "            FC_layer_g(n_feat//2, n_feat//2),\n",
    "            nn.Linear(n_feat//2, 42)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=n_channel),\n",
    "            DoubleConv_g(in_channels=n_channel, out_channels=n_channel//2),\n",
    "            Down_g(in_channels=n_channel//2, out_channels=n_channel//4),\n",
    "            DoubleConv_g(in_channels=n_channel//4, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=n_channel),\n",
    "            DoubleConv_g(in_channels=n_channel, out_channels=n_channel//2),\n",
    "            Up_g(in_channels=n_channel//2, out_channels=n_channel//4),\n",
    "            DoubleConv_g(in_channels=n_channel//4, out_channels=n_channel//8),\n",
    "            nn.Conv2d(n_channel//8, n_channel//8, kernel_size=3, padding=(1,2)),\n",
    "            nn.GroupNorm(num_groups=max(n_channel//32,1), num_channels=n_channel//8),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            OutConv(in_channels=n_channel//8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(M,M)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        b = xj\n",
    "        Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        # eye = torch.eye(M, device='cuda')\n",
    "        # Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-4\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -Rx.det().real.log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze().real \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl #+ loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 6, 64, 66, 6\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 901\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM6FT64_data3.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/val1kM6FT64_xsh_data3.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet_s10_(M,J,N).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s, h) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                     \n",
    "                Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "                shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                        @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "                shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "                for ind in range(x.shape[0]):\n",
    "                    hh = Hhat_val[ind]\n",
    "                    av_hcorr.append(h_corr(hh.cpu(), h[ind]))\n",
    "                    av_scorr.append(s_corr(s[ind].abs(), shat[ind]))\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,J,ii+1+ind*J)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged:', sum(av_hcorr)/len(av_hcorr))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_hcorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s82\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's82' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx42):\n",
    "    ind = torch.tril_indices(6,6)\n",
    "    indx = np.diag_indices(6)\n",
    "    rx_inv_hat = torch.zeros(rx42.shape[0], 6, 6, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx42[:, :21] + 1j*rx42[:,21:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet_s10(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(42, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 64),\n",
    "            nn.Linear(64, 42)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, padding=(1,2)),\n",
    "            nn.GroupNorm(num_groups=max(8//4,1), num_channels=8),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(M,M)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        b = xj\n",
    "        Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        # eye = torch.eye(M, device='cuda')\n",
    "        # Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-4\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -Rx.det().real.log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze().real  \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl #+ loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 6, 64, 66, 6\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 901\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM6FT64_data3.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/val1kM6FT64_xsh_data3.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet_s10(M,J,N).cuda()\n",
    "model = torch.load('../data/data_ss/models/s75/model_epoch400.pt')\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s, h) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                     \n",
    "                Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "                shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                        @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "                shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "                for ind in range(x.shape[0]):\n",
    "                    hh = Hhat_val[ind]\n",
    "                    av_hcorr.append(h_corr_cuda(hh, h[ind].cuda()).cpu().item())\n",
    "                    av_scorr.append(s_corr_cuda(s[ind:ind+1].abs().cuda(), \\\n",
    "                        shat[ind:ind+1].cuda()).cpu().item())\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,J,ii+1+ind*J)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged:', sum(av_hcorr)/len(av_hcorr))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_hcorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s82c\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's82c' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx42):\n",
    "    ind = torch.tril_indices(6,6)\n",
    "    indx = np.diag_indices(6)\n",
    "    rx_inv_hat = torch.zeros(rx42.shape[0], 6, 6, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx42[:, :21] + 1j*rx42[:,21:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet_s10(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(42, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 64),\n",
    "            nn.Linear(64, 42)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, padding=(1,2)),\n",
    "            nn.GroupNorm(num_groups=max(8//4,1), num_channels=8),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(M,M)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        b = xj\n",
    "        Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        # eye = torch.eye(M, device='cuda')\n",
    "        # Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-4\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -Rx.det().real.log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze().real  \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl #+ loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 6, 64, 66, 6\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 901\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM6FT64_data3.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/val1kM6FT64_xsh_data3.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet_s10(M,J,N).cuda()\n",
    "model = torch.load('../data/data_ss/models/s75/model_epoch400.pt')\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s, h) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                     \n",
    "                Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "                shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                        @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "                shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "                for ind in range(x.shape[0]):\n",
    "                    hh = Hhat_val[ind]\n",
    "                    av_hcorr.append(h_corr_cuda(hh, h[ind].cuda()).cpu().item())\n",
    "                    av_scorr.append(s_corr_cuda(s[ind:ind+1].abs().cuda(), \\\n",
    "                        shat[ind:ind+1].cuda()).cpu().item())\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,J,ii+1+ind*J)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged:', sum(av_hcorr)/len(av_hcorr))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_hcorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s87\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's87' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx42):\n",
    "    ind = torch.tril_indices(6,6)\n",
    "    indx = np.diag_indices(6)\n",
    "    rx_inv_hat = torch.zeros(rx42.shape[0], 6, 6, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx42[:, :21] + 1j*rx42[:,21:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet_s10(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(42, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 64),\n",
    "            nn.Linear(64, 42)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, padding=(1,2)),\n",
    "            nn.GroupNorm(num_groups=max(8//4,1), num_channels=8),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(M,M)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-3\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl #+ loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 6, 64, 66, 6\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 701\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM6FT64_data3.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/val1kM6FT64_xsh_data3.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet_s10(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s, h) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                     \n",
    "                Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "                shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                        @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "                shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "                for ind in range(x.shape[0]):\n",
    "                    hh = Hhat_val[ind]\n",
    "                    av_hcorr.append(h_corr_cuda(hh, h[ind].cuda()).cpu().item())\n",
    "                    av_scorr.append(s_corr_cuda(s[ind:ind+1].abs().cuda(), \\\n",
    "                        shat[ind:ind+1].cuda()).cpu().item())\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,J,ii+1+ind*J)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged:', sum(av_hcorr)/len(av_hcorr))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_hcorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s91\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's91' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx42):\n",
    "    ind = torch.tril_indices(6,6)\n",
    "    indx = np.diag_indices(6)\n",
    "    rx_inv_hat = torch.zeros(rx42.shape[0], 6, 6, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx42[:, :21] + 1j*rx42[:,21:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet_s12(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(42, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 64),\n",
    "            nn.Linear(64, 42)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, padding=(1,2)),\n",
    "            nn.GroupNorm(num_groups=max(8//4,1), num_channels=8),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(M,M)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-3\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl #+ loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 6, 64, 66, 6\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 701\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM6FT64_data3.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/val1kM6FT64_xsh_data3.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet_s12(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s, h) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                     \n",
    "                Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "                shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                        @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "                shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "                for ind in range(x.shape[0]):\n",
    "                    hh = Hhat_val[ind]\n",
    "                    av_hcorr.append(h_corr_cuda(hh, h[ind].cuda()).cpu().item())\n",
    "                    av_scorr.append(s_corr_cuda(s[ind:ind+1].abs().cuda(), \\\n",
    "                        shat[ind:ind+1].cuda()).cpu().item())\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,J,ii+1+ind*J)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged:', sum(av_hcorr)/len(av_hcorr))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_hcorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% s95\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 's95' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx42):\n",
    "    ind = torch.tril_indices(6,6)\n",
    "    indx = np.diag_indices(6)\n",
    "    rx_inv_hat = torch.zeros(rx42.shape[0], 6, 6, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx42[:, :21] + 1j*rx42[:,21:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet_s10_1(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        feat = 192\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(42, feat),\n",
    "            FC_layer_g(feat, feat),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(feat, feat),\n",
    "            FC_layer_g(feat, feat),\n",
    "            FC_layer_g(feat, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(feat, feat),\n",
    "            FC_layer_g(feat, feat),\n",
    "            FC_layer_g(feat, 64),\n",
    "            FC_layer_g(64, 64),\n",
    "            nn.Linear(64, 42)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, padding=(1,2)),\n",
    "            nn.GroupNorm(num_groups=max(8//4,1), num_channels=8),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(M,M)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-3\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl #+ loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 6, 64, 66, 6\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 701\n",
    "opts['lr'] = 1e-4\n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM6FT64_data3.pt')\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "xval, sval, hgt = torch.load('../data/nem_ss/val1kM6FT64_xsh_data3.pt')\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval, hgt)\n",
    "dval = Data.DataLoader(data, batch_size=200, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet_s10_1(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s, h) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                     \n",
    "                Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "                shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                        @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "                shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "                for ind in range(x.shape[0]):\n",
    "                    hh = Hhat_val[ind]\n",
    "                    av_hcorr.append(h_corr_cuda(hh, h[ind].cuda()).cpu().item())\n",
    "                    av_scorr.append(s_corr_cuda(s[ind:ind+1].abs().cuda(), \\\n",
    "                        shat[ind:ind+1].cuda()).cpu().item())\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,J,ii+1+ind*J)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged:', sum(av_hcorr)/len(av_hcorr))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_hcorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v79\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "rid = 'v79'\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% generate data\n",
    "if True:\n",
    "    M, n_data= 3, int(9e3)\n",
    "    dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "    v0 = dicts['v'][..., 0]\n",
    "    v1 = dicts['v'][..., 1]\n",
    "    v2 = dicts['v'][..., 2]\n",
    "    from skimage.transform import resize\n",
    "    NandF = 48\n",
    "    v0 = resize(v0, (50, 50), preserve_range=True)\n",
    "    v0 = torch.tensor(resize(v0, (NandF, NandF), preserve_range=True))\n",
    "    v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "    plt.imshow(v0.abs())\n",
    "    plt.colorbar()\n",
    "    v1 = resize(v1, (50, 50), preserve_range=True)\n",
    "    v1 = torch.tensor(resize(v1, (NandF, NandF), preserve_range=True))\n",
    "    v1 = awgn(v1, snr=30, seed=1).abs().to(torch.cfloat)\n",
    "    v2 = resize(v2, (50, 50), preserve_range=True)\n",
    "    v2 = torch.tensor(resize(v2, (NandF, NandF), preserve_range=True))\n",
    "    v2 = awgn(v2, snr=30, seed=2).abs().to(torch.cfloat)\n",
    "\n",
    "    angs = (torch.rand(n_data,1)*180 -0)/180*np.pi  # signal aoa [0, 180]\n",
    "    h_1 = (1j*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "    angs_n1 = (torch.rand(n_data,1)*180 -0)/180*np.pi  # noise aoa [0, 180]\n",
    "    h_2 = (1j*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "    angs_n2 = (torch.rand(n_data,1)*180 -0)/180*np.pi  # noise aoa [0, 180]\n",
    "    h_3 = (1j*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    sig1 = (h_1[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:])\n",
    "    sig2 = (h_2[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:])\n",
    "    sig3 = (h_3[..., None]@(torch.randn(v2.shape, dtype=torch.cfloat)*(v2**0.5)).flatten()[None,:])\n",
    "    sig1 = sig1.reshape(n_data, M, NandF, NandF)\n",
    "    sig2 = sig2.reshape(n_data, M, NandF, NandF)\n",
    "    sig3 = sig3.reshape(n_data, M, NandF, NandF)\n",
    "    for i in range(n_data):\n",
    "        a = torch.randint(0,50,(2,))\n",
    "        sig1[i] = sig1[i].roll((a[0], a[1]),dims=(1,2))\n",
    "        a = torch.randint(0,50,(2,))\n",
    "        sig2[i] = sig2[i].roll((a[0], a[1]),dims=(1,2))\n",
    "        a = torch.randint(0,50,(2,))\n",
    "        sig3[i] = sig3[i].roll((a[0], a[1]),dims=(1,2))\n",
    "    s = torch.stack((sig1[:,0], sig2[:,0], sig3[:,0]), dim=3)\n",
    "    mix = sig1 + sig2 + sig3\n",
    "    mix_all = mix.permute(0,2,3,1)\n",
    "    mixn = awgn_batch(mix_all,seed=0).permute(0,3,1,2) # shape of [I, M, N, F]\n",
    "    H_all = torch.stack((h_1, h_2, h_3), dim=-1)\n",
    "\n",
    "    # torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "    plt.figure()\n",
    "    plt.imshow(mix_all[0,:,:,0].abs())\n",
    "    plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(NandF**2, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx12):\n",
    "    ind = torch.tril_indices(3,3)\n",
    "    indx = np.diag_indices(3)\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer(12, 128),\n",
    "            FC_layer(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer(128, 128),\n",
    "            FC_layer(128, 128),\n",
    "            FC_layer(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer(128, 128),\n",
    "            FC_layer(128, 128),\n",
    "            FC_layer(128, 64),\n",
    "            FC_layer(64, 32),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Down(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up(in_channels=1, out_channels=64),\n",
    "            DoubleConv(in_channels=64, out_channels=32),\n",
    "            Up(in_channels=32, out_channels=16),\n",
    "            DoubleConv(in_channels=16, out_channels=8),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(3,3)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-4\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()/I\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl  #+ 10*loss_slotCEL\n",
    "\n",
    "I = 6000 # how many samples\n",
    "M, N, F, J = 3, NandF, NandF, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['n_epochs'] = 201\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "xtr = mixn[:I]/mix[:I].abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xtr)\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval = mixn[I:I+128]\n",
    "hgt = H_all[I:I+128]\n",
    "sval = s[I:I+128]\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "\n",
    "            av_hcorr, av_scorr = [], []\n",
    "            Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "            shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                    @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "            shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "            for ind in range(128):\n",
    "                hh = Hhat_val[ind]\n",
    "                av_hcorr.append(h_corr(hh.cpu(), hgt[ind]))\n",
    "                av_scorr.append(s_corr(sval[ind].abs(), shat[ind]))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged128:', sum(av_hcorr)/128)\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged128:', sum(av_scorr)/128)\n",
    "\n",
    "            plt.figure()\n",
    "            for ind in range(3):\n",
    "                for ii in range(J):\n",
    "                    plt.subplot(3,3,ii+1+ind*3)\n",
    "                    plt.imshow(shat[ind,:,:,ii])\n",
    "                    # plt.tight_layout(pad=1.1)\n",
    "                    # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "            plt.show()\n",
    "            plt.close('all')\n",
    "            \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "\n",
    "\n",
    "# #%%\n",
    "# model = torch.load('../data/data_ss/models/v76/model_epoch200.pt')\n",
    "\n",
    "\n",
    "\n",
    "# #%%\n",
    "# rr = xval.permute(0,2,3,1)[...,None]@xval.permute(0,2,3,1)[:,:,:,None,:].conj()\n",
    "# rxval = rr.mean(dim=(1,2))\n",
    "# for i in range(5):\n",
    "#     r0 = rxval[i].pinverse()\n",
    "#     hgt0 = hgt[i]\n",
    "#     for j in range(3):\n",
    "#         w = r0@hgt0[:,j:j+1] / \\\n",
    "#                 (eps + hgt0[:,j:j+1].t().conj()@r0@hgt0[:,j:j+1])\n",
    "#         shat = w.t().conj()@xval.permute(0,2,3,1)[i,..., None]\n",
    "#         plt.figure()\n",
    "#         plt.imshow(shat.squeeze().abs())\n",
    "#         plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v80\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 'v80' # standard\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% generate data\n",
    "if True:\n",
    "    M, n_data= 3, int(9e3)\n",
    "    dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "    v0 = dicts['v'][..., 0]\n",
    "    v1 = dicts['v'][..., 1]\n",
    "    v2 = dicts['v'][..., 2]\n",
    "    from skimage.transform import resize\n",
    "    NandF = 48\n",
    "    v0 = resize(v0, (50, 50), preserve_range=True)\n",
    "    v0 = torch.tensor(resize(v0, (NandF, NandF), preserve_range=True))\n",
    "    v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "    plt.imshow(v0.abs())\n",
    "    plt.colorbar()\n",
    "    v1 = resize(v1, (50, 50), preserve_range=True)\n",
    "    v1 = torch.tensor(resize(v1, (NandF, NandF), preserve_range=True))\n",
    "    v1 = awgn(v1, snr=30, seed=1).abs().to(torch.cfloat)\n",
    "    v2 = resize(v2, (50, 50), preserve_range=True)\n",
    "    v2 = torch.tensor(resize(v2, (NandF, NandF), preserve_range=True))\n",
    "    v2 = awgn(v2, snr=30, seed=2).abs().to(torch.cfloat)\n",
    "\n",
    "    angs = (torch.rand(n_data,1)*180 -0)/180*np.pi  # signal aoa [0, 180]\n",
    "    h_1 = (1j*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "    angs_n1 = (torch.rand(n_data,1)*180 -0)/180*np.pi  # noise aoa [0, 180]\n",
    "    h_2 = (1j*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "    angs_n2 = (torch.rand(n_data,1)*180 -0)/180*np.pi  # noise aoa [0, 180]\n",
    "    h_3 = (1j*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    sig1 = (h_1[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:])\n",
    "    sig2 = (h_2[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:])\n",
    "    sig3 = (h_3[..., None]@(torch.randn(v2.shape, dtype=torch.cfloat)*(v2**0.5)).flatten()[None,:])\n",
    "    sig1 = sig1.reshape(n_data, M, NandF, NandF)\n",
    "    sig2 = sig2.reshape(n_data, M, NandF, NandF)\n",
    "    sig3 = sig3.reshape(n_data, M, NandF, NandF)\n",
    "    for i in range(n_data):\n",
    "        a = torch.randint(0,50,(2,))\n",
    "        sig1[i] = sig1[i].roll((a[0], a[1]),dims=(1,2))\n",
    "        a = torch.randint(0,50,(2,))\n",
    "        sig2[i] = sig2[i].roll((a[0], a[1]),dims=(1,2))\n",
    "        a = torch.randint(0,50,(2,))\n",
    "        sig3[i] = sig3[i].roll((a[0], a[1]),dims=(1,2))\n",
    "    s = torch.stack((sig1[:,0], sig2[:,0], sig3[:,0]), dim=3)\n",
    "    mix = sig1 + sig2 + sig3\n",
    "    mix_all = mix.permute(0,2,3,1)\n",
    "    mixn = awgn_batch(mix_all,seed=0).permute(0,3,1,2) # shape of [I, M, N, F]\n",
    "    H_all = torch.stack((h_1, h_2, h_3), dim=-1)\n",
    "\n",
    "    # torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "    plt.figure()\n",
    "    plt.imshow(mix_all[0,:,:,0].abs())\n",
    "    plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(NandF**2, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx12):\n",
    "    ind = torch.tril_indices(3,3)\n",
    "    indx = np.diag_indices(3)\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_(12, 128),\n",
    "            FC_layer_(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_(128, 128),\n",
    "            FC_layer_(128, 128),\n",
    "            FC_layer_(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_(128, 128),\n",
    "            FC_layer_(128, 128),\n",
    "            FC_layer_(128, 64),\n",
    "            FC_layer_(64, 32),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_(in_channels=1, out_channels=64),\n",
    "            DoubleConv_(in_channels=64, out_channels=32),\n",
    "            Down_(in_channels=32, out_channels=16),\n",
    "            DoubleConv_(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_(in_channels=1, out_channels=64),\n",
    "            DoubleConv_(in_channels=64, out_channels=32),\n",
    "            Up_(in_channels=32, out_channels=16),\n",
    "            DoubleConv_(in_channels=16, out_channels=8),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(3,3)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-3\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()/I\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl  #+ 10*loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 6000 # how many samples\n",
    "M, N, F, J = 3, NandF, NandF, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['n_epochs'] = 201\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "xtr = mixn[:I]/mix[:I].abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xtr)\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval = mixn[I:I+128]\n",
    "hgt = H_all[I:I+128]\n",
    "sval = s[I:I+128]\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "\n",
    "            av_hcorr, av_scorr = [], []\n",
    "            Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "            shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                    @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "            shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "            for ind in range(128):\n",
    "                hh = Hhat_val[ind]\n",
    "                av_hcorr.append(h_corr(hh.cpu(), hgt[ind]))\n",
    "                av_scorr.append(s_corr(sval[ind].abs(), shat[ind]))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged128:', sum(av_hcorr)/128)\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged128:', sum(av_scorr)/128)\n",
    "\n",
    "            plt.figure()\n",
    "            for ind in range(3):\n",
    "                for ii in range(J):\n",
    "                    plt.subplot(3,3,ii+1+ind*3)\n",
    "                    plt.imshow(shat[ind,:,:,ii])\n",
    "                    # plt.tight_layout(pad=1.1)\n",
    "                    # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "            plt.show()\n",
    "            plt.close('all')\n",
    "            \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "\n",
    "# #%%\n",
    "# model = torch.load('../data/data_ss/models/v76/model_epoch200.pt')\n",
    "\n",
    "# #%%\n",
    "# rr = xval.permute(0,2,3,1)[...,None]@xval.permute(0,2,3,1)[:,:,:,None,:].conj()\n",
    "# rxval = rr.mean(dim=(1,2))\n",
    "# for i in range(5):\n",
    "    # r0 = rxval[i].pinverse()\n",
    "    # hgt0 = hgt[i]\n",
    "    # for j in range(3):\n",
    "    #     w = r0@hgt0[:,j:j+1] / \\\n",
    "    #             (eps + hgt0[:,j:j+1].t().conj()@r0@hgt0[:,j:j+1])\n",
    "    #     shat = w.t().conj()@xval.permute(0,2,3,1)[i,..., None]\n",
    "    #     plt.figure()\n",
    "    #     plt.imshow(shat.squeeze().abs())\n",
    "    #     plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v84\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 'v84' # standard\n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% generate data\n",
    "if True:\n",
    "    M, n_data= 3, int(9e3)\n",
    "    dicts = sio.loadmat('../data/nem_ss/v2.mat')\n",
    "    v0 = dicts['v'][..., 0]\n",
    "    v1 = dicts['v'][..., 1]\n",
    "    v2 = dicts['v'][..., 2]\n",
    "    from skimage.transform import resize\n",
    "    NandF = 48\n",
    "    v0 = resize(v0, (50, 50), preserve_range=True)\n",
    "    v0 = torch.tensor(resize(v0, (NandF, NandF), preserve_range=True))\n",
    "    v0 = awgn(v0, snr=30, seed=0).abs().to(torch.cfloat)\n",
    "    plt.imshow(v0.abs())\n",
    "    plt.colorbar()\n",
    "    v1 = resize(v1, (50, 50), preserve_range=True)\n",
    "    v1 = torch.tensor(resize(v1, (NandF, NandF), preserve_range=True))\n",
    "    v1 = awgn(v1, snr=30, seed=1).abs().to(torch.cfloat)\n",
    "    v2 = resize(v2, (50, 50), preserve_range=True)\n",
    "    v2 = torch.tensor(resize(v2, (NandF, NandF), preserve_range=True))\n",
    "    v2 = awgn(v2, snr=30, seed=2).abs().to(torch.cfloat)\n",
    "\n",
    "    angs = (torch.rand(n_data,1)*180 -0)/180*np.pi  # signal aoa [0, 180]\n",
    "    h_1 = (1j*np.pi*angs.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "    angs_n1 = (torch.rand(n_data,1)*180 -0)/180*np.pi  # noise aoa [0, 180]\n",
    "    h_2 = (1j*np.pi*angs_n1.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "    angs_n2 = (torch.rand(n_data,1)*180 -0)/180*np.pi  # noise aoa [0, 180]\n",
    "    h_3 = (1j*np.pi*angs_n2.sin()@torch.arange(M).to(torch.cfloat)[None,:]).exp() #shape of [n,M]\n",
    "\n",
    "    sig1 = (h_1[..., None]@(torch.randn(v0.shape, dtype=torch.cfloat)*(v0**0.5)).flatten()[None,:])\n",
    "    sig2 = (h_2[..., None]@(torch.randn(v1.shape, dtype=torch.cfloat)*(v1**0.5)).flatten()[None,:])\n",
    "    sig3 = (h_3[..., None]@(torch.randn(v2.shape, dtype=torch.cfloat)*(v2**0.5)).flatten()[None,:])\n",
    "    sig1 = sig1.reshape(n_data, M, NandF, NandF)\n",
    "    sig2 = sig2.reshape(n_data, M, NandF, NandF)\n",
    "    sig3 = sig3.reshape(n_data, M, NandF, NandF)\n",
    "    for i in range(n_data):\n",
    "        a = torch.randint(0,50,(2,))\n",
    "        sig1[i] = sig1[i].roll((a[0], a[1]),dims=(1,2))\n",
    "        a = torch.randint(0,50,(2,))\n",
    "        sig2[i] = sig2[i].roll((a[0], a[1]),dims=(1,2))\n",
    "        a = torch.randint(0,50,(2,))\n",
    "        sig3[i] = sig3[i].roll((a[0], a[1]),dims=(1,2))\n",
    "    s = torch.stack((sig1[:,0], sig2[:,0], sig3[:,0]), dim=3)\n",
    "    mix = sig1 + sig2 + sig3\n",
    "    mix_all = mix.permute(0,2,3,1)\n",
    "    mixn = awgn_batch(mix_all,seed=0).permute(0,3,1,2) # shape of [I, M, N, F]\n",
    "    H_all = torch.stack((h_1, h_2, h_3), dim=-1)\n",
    "\n",
    "    # torch.save((mix, sig, h), 'toy_matrix_inv.pt') # generate data is faster than loading it...\n",
    "    plt.figure()\n",
    "    plt.imshow(mix_all[0,:,:,0].abs())\n",
    "    plt.colorbar()\n",
    "\n",
    "if False: # check data low rank or not\n",
    "    for i in range(n_data):\n",
    "        x = mix[i,:,:].reshape(NandF**2, 3)\n",
    "        xbar = x - x.mean(0)\n",
    "        cov = x.conj().t() @ x\n",
    "        r = torch.linalg.matrix_rank(cov)\n",
    "        if r != 3:\n",
    "            print('low rank', i, 'rank is ', r)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx12):\n",
    "    ind = torch.tril_indices(3,3)\n",
    "    indx = np.diag_indices(3)\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.mainnet = nn.Sequential(\n",
    "            FC_layer_g(12, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "        )\n",
    "        self.hnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rxnet = nn.Sequential(\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 128),\n",
    "            FC_layer_g(128, 64),\n",
    "            FC_layer_g(64, 32),\n",
    "            nn.Linear(32, 12)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "        for i in range(self.J):\n",
    "            \"Get H estimation\"\n",
    "            ind = torch.tril_indices(3,3)\n",
    "            rx = (xj@xj.transpose(-1,-2).conj()).mean(dim=(1,2))\n",
    "            rx_lower = rx[:, ind[0], ind[1]]\n",
    "            mid =self.mainnet(torch.stack((rx_lower.real,rx_lower.imag),\\\n",
    "                 dim=1).reshape(btsize,-1))\n",
    "            ang = self.hnet(mid)\n",
    "            temp = ang@ch[None,:]\n",
    "            hhat = (1j*temp).exp()  # shape of [I, M]\n",
    "            h_all.append(hhat)\n",
    "\n",
    "            \"Get Rx inverse\"\n",
    "            rx_index = self.rxnet(mid)\n",
    "            rx_inv = lower2matrix(rx_index) # shape of [I, M, M]\n",
    "        \n",
    "            \"Encoder part\"\n",
    "            w = rx_inv@hhat[...,None] / \\\n",
    "                (hhat[:,None,:].conj()@rx_inv@hhat[...,None])\n",
    "            shat = w.permute(0,2,1).conj()[:,None,None]@xj\n",
    "            xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "            \"Get latent variable\"\n",
    "            zz = self.fc1(xx.reshape(btsize,-1))\n",
    "            mu = zz[:,::2]\n",
    "            logvar = zz[:,1::2]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            wz = self.bilinear(z)\n",
    "            z_all.append(z)\n",
    "            z_all.append(wz)\n",
    "            \n",
    "            \"Decoder to get V\"\n",
    "            v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "            v_all.append(threshold(v, floor=1e-3, ceiling=1e2)) # 1e-3 to 1e2\n",
    "\n",
    "            \"Remove the current component\"\n",
    "            rxinvh = rx_inv@hhat[...,None]  # shape of [I, M, 1]\n",
    "            v_rxinv_h_herm = (v[...,None, None]*rxinvh[:,None, None]).transpose(-1,-2).conj() \n",
    "            cj = hhat[:,None,None,:,None] * (v_rxinv_h_herm @ xj) # shape of [I,N,F,M,1]\n",
    "            xj = xj - cj\n",
    "       \n",
    "        Hhat = torch.stack(h_all, 2) # shape:[I, M, J]\n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda')\n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-3\n",
    "\n",
    "        return vhat.diag_embed(), Hhat, Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.permute(0,2,3,1)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rxperm = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb\n",
    "    Rx = Rxperm.permute(2,0,1,3,4) # shape of [I, N, F, M, M]\n",
    "    ll = -(np.pi*Rx.det()).log() - (x[...,None,:].conj()@Rx.inverse()@x[...,None]).squeeze() \n",
    "\n",
    "    # \"Slot contrastive loss\"\n",
    "    # inp = (zall[:,0::2]@zall[:,1::2].permute(0,2,1)).reshape(I*J, J) # shape of [N,J,J]\n",
    "    # target = torch.cat([torch.arange(J) for i in range(I)]).cuda()\n",
    "    # loss_slotCEL = nn.CrossEntropyLoss(reduction='none')(inp, target).sum()/I\n",
    "\n",
    "    # \"My own loss for H\"\n",
    "    # HHt = Hhat@Hhat.permute(0,2,1).conj() \n",
    "    # temp = x[...,None]@ x[:,:,:,None].conj()\n",
    "    # rx = temp.mean(dim=(1,2))\n",
    "    # term = (((rx- HHt/100).abs())**2).mean()\n",
    "\n",
    "    return -ll.sum(), beta*kl  #+ 10*loss_slotCEL\n",
    "\n",
    "#%%\n",
    "I = 6000 # how many samples\n",
    "M, N, F, J = 3, NandF, NandF, 3\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 64\n",
    "opts['n_epochs'] = 201\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "xtr = mixn[:I]/mix[:I].abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xtr)\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "xval = mixn[I:I+128]\n",
    "hgt = H_all[I:I+128]\n",
    "sval = s[I:I+128]\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "xval_cuda = xval[:128].to(torch.cfloat).cuda()\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Hhat, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Hhat, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Rs, Hhat_val, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "            l1, l2 = loss_fun(xval_cuda, Rs, Hhat_val, Rb, mu, logvar, zall)\n",
    "            loss_eval.append((l1+l2).cpu().item()/128)\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all')           \n",
    "\n",
    "            av_hcorr, av_scorr = [], []\n",
    "            Rxperm = Hhat_val@Rs.permute(1,2,0,3,4)@Hhat_val.transpose(-1,-2).conj() + Rb\n",
    "            shatperm = Rs.permute(1,2,0,3,4)@Hhat_val.conj().transpose(-1,-2)\\\n",
    "                    @Rxperm.inverse()@xval_cuda.permute(2,3,0,1)[...,None]\n",
    "            shat = shatperm.permute(2,0,1,3,4).squeeze().cpu().abs()\n",
    "            for ind in range(128):\n",
    "                hh = Hhat_val[ind]\n",
    "                av_hcorr.append(h_corr(hh.cpu(), hgt[ind]))\n",
    "                av_scorr.append(s_corr(sval[ind].abs(), shat[ind]))\n",
    "            print('first 3 h_corr',av_hcorr[:3],' averaged128:', sum(av_hcorr)/128)\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged128:', sum(av_scorr)/128)\n",
    "\n",
    "            plt.figure()\n",
    "            for ind in range(3):\n",
    "                for ii in range(J):\n",
    "                    plt.subplot(3,3,ii+1+ind*3)\n",
    "                    plt.imshow(shat[ind,:,:,ii])\n",
    "                    # plt.tight_layout(pad=1.1)\n",
    "                    # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "            plt.show()\n",
    "            plt.close('all')\n",
    "            \n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "\n",
    "\n",
    "# #%%\n",
    "# model = torch.load('../data/data_ss/models/v76/model_epoch200.pt')\n",
    "\n",
    "# #%%\n",
    "# rr = xval.permute(0,2,3,1)[...,None]@xval.permute(0,2,3,1)[:,:,:,None,:].conj()\n",
    "# rxval = rr.mean(dim=(1,2))\n",
    "# for i in range(5):\n",
    "    # r0 = rxval[i].pinverse()\n",
    "    # hgt0 = hgt[i]\n",
    "    # for j in range(3):\n",
    "    #     w = r0@hgt0[:,j:j+1] / \\\n",
    "    #             (eps + hgt0[:,j:j+1].t().conj()@r0@hgt0[:,j:j+1])\n",
    "    #     shat = w.t().conj()@xval.permute(0,2,3,1)[i,..., None]\n",
    "    #     plt.figure()\n",
    "    #     plt.imshow(shat.squeeze().abs())\n",
    "    #     plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% v86  # 1 channel, 3 classes, see if the VAE capacity is enough\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "\n",
    "#%%\n",
    "\"make the result reproducible\"\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)       # current GPU seed\n",
    "torch.cuda.manual_seed_all(seed)   # all GPUs seed\n",
    "torch.backends.cudnn.deterministic = True  #True uses deterministic alg. for cuda\n",
    "torch.backends.cudnn.benchmark = False  #False cuda use the fixed alg. for conv, may slower\n",
    "\n",
    "rid = 'v86' \n",
    "fig_loc = '../data/data_ss/figures/'\n",
    "mod_loc = '../data/data_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#%% define models and functions\n",
    "from vae_modules import *\n",
    "def lower2matrix(rx12):\n",
    "    ind = torch.tril_indices(3,3)\n",
    "    indx = np.diag_indices(3)\n",
    "    rx_inv_hat = torch.zeros(rx12.shape[0], 3, 3, dtype=torch.cfloat).cuda()\n",
    "    rx_inv_hat[:, ind[0], ind[1]] = rx12[:, :6] + 1j*rx12[:,6:]\n",
    "    rx_inv_hat = rx_inv_hat + rx_inv_hat.permute(0,2,1).conj()\n",
    "    rx_inv_hat[:, indx[0], indx[1]] = rx_inv_hat[:, indx[0], indx[1]]/2\n",
    "    return rx_inv_hat\n",
    "\n",
    "class NNet(nn.Module):\n",
    "    \"\"\"This is recursive Wiener filter version, with Rb threshold of [1e-3, 1e2]\n",
    "    Input shape [I,M,N,F], e.g.[32,3,100,100]\n",
    "    J <=K\n",
    "    \"\"\"\n",
    "    def __init__(self, M, K, im_size):\n",
    "        super().__init__()\n",
    "        self.dz = 32\n",
    "        self.J, self.M = K, M\n",
    "        down_size = int(im_size/4)\n",
    "        self.encoder = nn.Sequential(\n",
    "            Down_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Down_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=1),\n",
    "            )\n",
    "        self.fc1 = nn.Linear(down_size*down_size, 2*self.dz)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dz, down_size*down_size),\n",
    "            Reshape(-1, 1, down_size, down_size),\n",
    "            Up_g(in_channels=1, out_channels=64),\n",
    "            DoubleConv_g(in_channels=64, out_channels=32),\n",
    "            Up_g(in_channels=32, out_channels=16),\n",
    "            DoubleConv_g(in_channels=16, out_channels=8),\n",
    "            OutConv(in_channels=8, out_channels=1),\n",
    "            ) \n",
    "        self.bilinear = nn.Linear(self.dz, self.dz, bias=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):     \n",
    "        btsize, M, N, F = x.shape\n",
    "        z_all, v_all, h_all = [], [], []\n",
    "        ch = np.pi*torch.arange(self.M, device=x.device)\n",
    "        xj = x.permute(0,2,3,1)[...,None]  # shape of [I,N,F,M,1]\n",
    "\n",
    "        \n",
    "        \"Encoder part\"\n",
    "        shat = xj\n",
    "        xx = self.encoder(shat.squeeze()[:,None].abs())\n",
    "\n",
    "        \"Get latent variable\"\n",
    "        zz = self.fc1(xx.reshape(btsize,-1))\n",
    "        mu = zz[:,::2]\n",
    "        logvar = zz[:,1::2]\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        wz = self.bilinear(z)\n",
    "        z_all.append(z)\n",
    "        z_all.append(wz)\n",
    "        \n",
    "        \"Decoder to get V\"\n",
    "        v = self.decoder(z).square().squeeze()  # shape of [I,N,F]\n",
    "        v_all.append(threshold(v, floor=1e-6, ceiling=1e2)) # 1e-6 to 1e2 \n",
    "       \n",
    "        vhat = torch.stack(v_all, 3).to(torch.cfloat) # shape:[I, N, F, J]\n",
    "        zall = torch.stack(z_all, dim=1)\n",
    "\n",
    "        # Rb = (b@b.conj().permute(0,1,2,4,3)).mean(dim=(1,2)).squeeze()\n",
    "        eye = torch.eye(M, device='cuda') \n",
    "        Rb = torch.stack(tuple(eye for ii in range(btsize)), 0)*1e-3\n",
    "\n",
    "        return vhat.squeeze(), Rb, mu, logvar, zall\n",
    "\n",
    "def loss_fun(x, Rs, Rb, mu, logvar, zall, beta=1):\n",
    "    I, M, J = x.shape[0], x.shape[1], Rs.shape[-1]\n",
    "    x = x.squeeze()\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    Rx = Rs.squeeze() + Rb # shape of [I, N, F]\n",
    "    ll = -(np.pi*Rx.abs()).log() - (x.conj()*(1/Rx)*x)\n",
    "    return -ll.sum(), beta*kl \n",
    "\n",
    "#%%\n",
    "\"raw data processing\"\n",
    "FT = 64  #48, 64, 80, 100, 128, 200, 256\n",
    "var_name = ['ble', 'bt', 'fhss1', 'fhss2', 'wifi1', 'wifi2']\n",
    "data = {}\n",
    "\n",
    "def get_ftdata(data_pool):\n",
    "    *_, Z = stft(data_pool, fs=4e7, nperseg=FT, boundary=None)\n",
    "    x = torch.tensor(np.roll(Z, FT//2, axis=1))  # roll nperseg//2\n",
    "    return x.to(torch.cfloat)\n",
    "\n",
    "for i in range(6):\n",
    "    temp = sio.loadmat('/home/chenhao1/Matlab/LMdata/compressed/'+var_name[i]+f'_{FT}_2k.mat')\n",
    "    x = torch.tensor(temp['x'])\n",
    "    x =  x/((x.abs()**2).sum(dim=(1),keepdim=True)**0.5)# normalize\n",
    "    data[i] = x\n",
    "s1 = get_ftdata(data[0]) # ble [2000,F,T]\n",
    "s2 = get_ftdata(data[2]) # fhss1\n",
    "s3 = get_ftdata(data[5]) # wifi2\n",
    "s = [s1, s2, s3]\n",
    "\n",
    "torch.manual_seed(1)\n",
    "J, M = 3, 1\n",
    "\"training data\"\n",
    "x = []\n",
    "for i in range(4):\n",
    "    temp = 0\n",
    "    for j in range(J):\n",
    "        idx = torch.randperm(2000)\n",
    "        temp =s[j][idx]\n",
    "        x.append(temp)\n",
    "x = torch.stack(x, dim=1)\n",
    "xtr = x[:,:9].reshape(-1,1,FT,FT)\n",
    "d = awgn_batch(xtr, snr=40, seed=1) # added white noise\n",
    "\n",
    "xvt = x[:,9:].reshape(-1,1,FT,FT)\n",
    "sval = xvt[:1000]\n",
    "xval = awgn_batch(sval, snr=40, seed=10)\n",
    "\n",
    "#%%\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 1, 64, 64, 1\n",
    "NF = N*F\n",
    "eps = 5e-4\n",
    "opts = {}\n",
    "opts['batch_size'] = 128\n",
    "opts['n_epochs'] = 301\n",
    "opts['lr'] = 1e-3\n",
    "\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3), keepdim=True)) # [sample,M,N,F]\n",
    "xtr = xtr.to(torch.cfloat)\n",
    "data = Data.TensorDataset(xtr[:I])\n",
    "tr = Data.DataLoader(data, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "sval= sval.permute(0,2,3,1)\n",
    "xval = xval/xval.abs().amax(dim=(1,2,3), keepdim=True)\n",
    "data = Data.TensorDataset(xval, sval)\n",
    "dval = Data.DataLoader(data, batch_size=1000, drop_last=True)\n",
    "\n",
    "#%%\n",
    "loss_iter, loss_tr, loss1, loss2, loss_eval = [], [], [], [], []\n",
    "model = NNet(M,J,N).cuda()\n",
    "# for w in model.parameters():\n",
    "#     nn.init.normal_(w, mean=0., std=0.01)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "for epoch in range(opts['n_epochs']):\n",
    "    model.train()\n",
    "    for i, (x,) in enumerate(tr): \n",
    "        x = x.cuda()\n",
    "        optimizer.zero_grad()         \n",
    "        Rs, Rb, mu, logvar, zall= model(x)\n",
    "        l1, l2 = loss_fun(x, Rs, Rb, mu, logvar, zall)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i%30 == 0:\n",
    "            loss_tr.append(loss.detach().cpu().item()/opts['batch_size'])\n",
    "            loss1.append(l1.detach().cpu().item()/opts['batch_size'])\n",
    "            loss2.append(l2.detach().cpu().item()/opts['batch_size'])\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(epoch)\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr, '-or')\n",
    "        plt.title(f'Loss fuction at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_LossFunAll')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss1, '-og')\n",
    "        plt.title(f'Reconstruction loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss1')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss2, '-og')\n",
    "        plt.title(f'KL loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_Loss2')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_tr[-50:], '-or')\n",
    "        plt.title(f'Last 50 of loss at epoch{epoch}')\n",
    "        plt.savefig(fig_loc + f'Epoch{epoch}_last50')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            av_hcorr, av_scorr, temp = [], [], []\n",
    "            for i, (x, s) in enumerate(dval):\n",
    "                xval_cuda = x.cuda()\n",
    "                Rs, Rb, mu, logvar, zall= model(xval_cuda)\n",
    "                l1, l2 = loss_fun(xval_cuda, Rs, Rb, mu, logvar, zall)\n",
    "                temp.append((l1+l2).cpu().item()/x.shape[0])\n",
    "                     \n",
    "                Rx = Rs + Rb\n",
    "                shatperm = Rs/Rx*xval_cuda.squeeze()\n",
    "                shat = shatperm[...,None].cpu().abs()\n",
    "                for ind in range(x.shape[0]):\n",
    "                    av_scorr.append(s_corr(s[ind].abs(), shat[ind]))\n",
    "                \n",
    "                if i == 0:\n",
    "                    plt.figure()\n",
    "                    for ind in range(3):\n",
    "                        for ii in range(J):\n",
    "                            plt.subplot(3,3,ii+1+ind*3)\n",
    "                            plt.imshow(shat[ind,:,:,ii])\n",
    "                            # plt.tight_layout(pad=1.1)\n",
    "                            # if ii == 0 : plt.title(f'Epoch{epoch}_sample{ind}')\n",
    "                    plt.savefig(fig_loc + f'Epoch{epoch}_estimated sources')\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "\n",
    "            loss_eval.append(sum(temp)/len(temp))\n",
    "            print('first 3 s_corr',av_scorr[:3],' averaged:', sum(av_scorr)/len(av_scorr))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(loss_eval[-50:], '-xb')\n",
    "            plt.title(f'last 50 validation loss at epoch{epoch}')\n",
    "            plt.savefig(fig_loc + f'Epoch{epoch}_val') \n",
    "            plt.close('all') \n",
    "\n",
    "        torch.save(model, mod_loc+f'model_epoch{epoch}.pt')\n",
    "print('done')\n",
    "print('End date time ', datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c728c5ad72b5fd3c6e083c4badca00ca04470578383c0e9d983163c40aa43e1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
