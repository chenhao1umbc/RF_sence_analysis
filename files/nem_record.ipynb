{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This file is record of NEM to compare with VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%@title 'n1'\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#%%\n",
    "rid = 'n1' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'rid{rid}/')\n",
    "    os.mkdir(mod_loc + f'rid{rid}/')\n",
    "fig_loc = fig_loc + f'rid{rid}/'\n",
    "mod_loc = mod_loc + f'rid{rid}/'\n",
    "\n",
    "from unet.unet_model import *\n",
    "class UNetHalf(nn.Module):\n",
    "    \"16 layers here\"\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        \"\"\"Only the up part of the unet\n",
    "        Args:\n",
    "            n_channels ([type]): [how many input channels=n_sources]\n",
    "            n_classes ([type]): [how many output classes=n_sources]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_ch = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.n_ch = 128\n",
    "\n",
    "        self.up = nn.Sequential(DoubleConv(n_channels, self.n_ch),\n",
    "                    MyUp(self.n_ch, self.n_ch//2),\n",
    "                    MyUp(self.n_ch//2, self.n_ch//4),\n",
    "                    MyUp(self.n_ch//4, self.n_ch//4))\n",
    "        self.reshape = nn.Sequential(\n",
    "                    nn.Conv2d(self.n_ch//4, self.n_ch//4, kernel_size=3, padding=(1,2)),\n",
    "                    nn.BatchNorm2d(self.n_ch//4),\n",
    "                    nn.LeakyReLU(inplace=True),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4))\n",
    "        self.outc = OutConv(32, n_classes)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.reshape(x) \n",
    "        x = self.outc(x)\n",
    "        x = self.sig(x)\n",
    "        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n",
    "        return out\n",
    "\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 3, 64, 66, 3\n",
    "NF = N*F\n",
    "eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n",
    "opts = {}\n",
    "opts['n_ch'] = [1,1]  \n",
    "opts['batch_size'] = 256\n",
    "opts['EM_iter'] = 201\n",
    "opts['lr'] = 0.001\n",
    "opts['n_epochs'] = 71\n",
    "opts['d_gamma'] = 8 \n",
    "n = 5  # for stopping \n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM3FT64_data3.pt').to(torch.cdouble)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n",
    "loss_iter, loss_tr = [], []\n",
    "model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "\"initial\"\n",
    "xtr = xtr[:I]\n",
    "Htr = torch.load('../data/nem_ss/tr18kHCI10M3_data3.pt').to(torch.cdouble)[:I]\n",
    "Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n",
    "gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma']) # shape of [J,1,8,8], cpu()\n",
    "\n",
    "#%%\n",
    "for epoch in range(opts['n_epochs']):  \n",
    "    xtr, Rbtr, gtr, Htr = myshuffle((xtr, Rbtr, gtr, Htr))  \n",
    "    res = xtr, Rbtr, gtr, Htr\n",
    "    data = Data.TensorDataset(*res)\n",
    "    tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n",
    "    for i, (x, Rb, g, Hhat) in enumerate(tr): # gamma [n_batch, 4, 4]\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        model.eval()\n",
    "\n",
    "        \"EM part\"\n",
    "        Rb, g, Hhat = Rb.cuda(), g.cuda(), Hhat.cuda()\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(g[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n",
    "\n",
    "        x = x.cuda()\n",
    "        g.requires_grad_()\n",
    "        optim_gamma = torch.optim.RAdam([g],\n",
    "                lr= glr,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n",
    "        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n",
    "        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n",
    "        ll_traj = []\n",
    "\n",
    "        for ii in range(opts['EM_iter']):\n",
    "            \"E-step\"\n",
    "            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n",
    "            shat = W.permute(2,0,1,3,4) @ x[...,None]\n",
    "            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n",
    "            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n",
    "            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n",
    "\n",
    "            \"M-step\"\n",
    "            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n",
    "            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n",
    "                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n",
    "            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n",
    "            Rb.imag = Rb.imag - Rb.imag\n",
    "\n",
    "            outs = []\n",
    "            for j in range(J):\n",
    "                outs.append(model(g[:,j]))\n",
    "            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "            vhat.real = threshold(out)\n",
    "            loss = loss_func(vhat, Rsshatnf.cuda())\n",
    "            optim_gamma.zero_grad()   \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n",
    "            optim_gamma.step()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            \"compute log-likelyhood\"\n",
    "            vhat = vhat.detach()\n",
    "            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n",
    "            ll_traj.append(ll.item())\n",
    "            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n",
    "            # if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n",
    "            #     print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n",
    "            #     break\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print(f'batch {i} is done. ', 'Current time--', datetime.now())\n",
    "    \n",
    "        if i == 0 :\n",
    "            plt.figure()\n",
    "            plt.plot(ll_traj, '-x')\n",
    "            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n",
    "\n",
    "            for ii in range(J):\n",
    "                plt.figure()\n",
    "                plt.imshow(vhat[0,...,ii].real.cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'source of v{ii} in 1st sample from the 1st batch at epoch {epoch}')\n",
    "                plt.savefig(fig_loc + f'id{rid}_vj{ii}_epoch{epoch}')      \n",
    "\n",
    "        \"update variable\"\n",
    "        with torch.no_grad():\n",
    "            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n",
    "            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n",
    "            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n",
    "        g.requires_grad_(False)\n",
    "        model.train()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(g[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat.real = threshold(out)\n",
    "        optimizer.zero_grad()         \n",
    "        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n",
    "        loss = -ll\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        loss_iter.append(loss.detach().cpu().item())\n",
    "\n",
    "    print(f'done with epoch{epoch}')\n",
    "    plt.figure()\n",
    "    plt.plot(loss_iter, '-xr')\n",
    "    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n",
    "    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n",
    "\n",
    "    loss_tr.append(loss.detach().cpu().item())\n",
    "    plt.figure()\n",
    "    plt.plot(loss_tr, '-or')\n",
    "    plt.title(f'Loss fuction at epoch{epoch}')\n",
    "    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n",
    "\n",
    "    plt.close('all')  # to avoid warnings\n",
    "    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n",
    "    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n",
    "    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n",
    "\n",
    "    # if epoch >10 :\n",
    "    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n",
    "    #     if s1 - s2 < 0 :\n",
    "    #         print('break-1')\n",
    "    #         break\n",
    "    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n",
    "    #     if abs((s1-s2)/s1) < 5e-4 :\n",
    "    #         print('break-2')\n",
    "    #         break\n",
    "print('done')\n",
    "print('starting date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%@title 'n4'\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#%%\n",
    "rid = 'n4' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "from unet.unet_model import *\n",
    "class UNetHalf(nn.Module):\n",
    "    \"16 layers here\"\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        \"\"\"Only the up part of the unet\n",
    "        Args:\n",
    "            n_channels ([type]): [how many input channels=n_sources]\n",
    "            n_classes ([type]): [how many output classes=n_sources]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_ch = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.n_ch = 128\n",
    "\n",
    "        self.up = nn.Sequential(DoubleConv(n_channels, self.n_ch),\n",
    "                    MyUp(self.n_ch, self.n_ch//2),\n",
    "                    MyUp(self.n_ch//2, self.n_ch//4),\n",
    "                    MyUp(self.n_ch//4, self.n_ch//4))\n",
    "        self.reshape = nn.Sequential(\n",
    "                    nn.Conv2d(self.n_ch//4, self.n_ch//4, kernel_size=3, padding=(1,2)),\n",
    "                    nn.BatchNorm2d(self.n_ch//4),\n",
    "                    nn.LeakyReLU(inplace=True),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4))\n",
    "        self.outc = OutConv(32, n_classes)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.reshape(x) \n",
    "        x = self.outc(x)\n",
    "        x = self.sig(x)\n",
    "        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n",
    "        return out\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, J = 3, 64, 66, 3\n",
    "NF = N*F\n",
    "eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n",
    "opts = {}\n",
    "opts['n_ch'] = [1,1]  \n",
    "opts['batch_size'] = 128\n",
    "opts['EM_iter'] = 201\n",
    "opts['lr'] = 0.001\n",
    "opts['n_epochs'] = 71\n",
    "opts['d_gamma'] = 8 \n",
    "n = 5  # for stopping \n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM3FT64_data3.pt').to(torch.cdouble)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n",
    "loss_iter, loss_tr = [], []\n",
    "model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "\"initial\"\n",
    "xtr = xtr[:I]\n",
    "Htr = torch.load('../data/nem_ss/tr18kHCI10M3_data3.pt').to(torch.cdouble)[:I]\n",
    "Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n",
    "gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma']) # shape of [J,1,8,8], cpu()\n",
    "\n",
    "#%%\n",
    "for epoch in range(opts['n_epochs']):  \n",
    "    xtr, Rbtr, gtr, Htr = myshuffle((xtr, Rbtr, gtr, Htr))  \n",
    "    res = xtr, Rbtr, gtr, Htr\n",
    "    data = Data.TensorDataset(*res)\n",
    "    tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n",
    "    for i, (x, Rb, g, Hhat) in enumerate(tr): # gamma [n_batch, 4, 4]\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        model.eval()\n",
    "\n",
    "        \"EM part\"\n",
    "        Rb, g, Hhat = Rb.cuda(), g.cuda(), Hhat.cuda()\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(g[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n",
    "\n",
    "        x = x.cuda()\n",
    "        g.requires_grad_()\n",
    "        optim_gamma = torch.optim.RAdam([g],\n",
    "                lr= glr,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n",
    "        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n",
    "        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n",
    "        ll_traj = []\n",
    "\n",
    "        for ii in range(opts['EM_iter']):\n",
    "            \"E-step\"\n",
    "            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n",
    "            shat = W.permute(2,0,1,3,4) @ x[...,None]\n",
    "            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n",
    "            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n",
    "            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n",
    "\n",
    "            \"M-step\"\n",
    "            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n",
    "            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n",
    "                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n",
    "            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n",
    "            Rb.imag = Rb.imag - Rb.imag\n",
    "\n",
    "            outs = []\n",
    "            for j in range(J):\n",
    "                outs.append(model(g[:,j]))\n",
    "            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "            vhat.real = threshold(out)\n",
    "            loss = loss_func(vhat, Rsshatnf.cuda())\n",
    "            optim_gamma.zero_grad()   \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n",
    "            optim_gamma.step()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            \"compute log-likelyhood\"\n",
    "            vhat = vhat.detach()\n",
    "            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n",
    "            ll_traj.append(ll.item())\n",
    "            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n",
    "            # if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n",
    "            #     print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n",
    "            #     break\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print(f'batch {i} is done. ', 'Current time--', datetime.now())\n",
    "    \n",
    "        if i == 0 :\n",
    "            plt.figure()\n",
    "            plt.plot(ll_traj, '-x')\n",
    "            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n",
    "\n",
    "            for ii in range(J):\n",
    "                plt.figure()\n",
    "                plt.imshow(vhat[0,...,ii].real.cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'source of v{ii} in 1st sample from the 1st batch at epoch {epoch}')\n",
    "                plt.savefig(fig_loc + f'id{rid}_vj{ii}_epoch{epoch}')      \n",
    "\n",
    "        \"update variable\"\n",
    "        with torch.no_grad():\n",
    "            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n",
    "            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n",
    "            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n",
    "        g.requires_grad_(False)\n",
    "        model.train()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(g[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat.real = threshold(out)\n",
    "        optimizer.zero_grad()         \n",
    "        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n",
    "        loss = -ll\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        loss_iter.append(loss.detach().cpu().item())\n",
    "\n",
    "    print(f'done with epoch{epoch}')\n",
    "    plt.figure()\n",
    "    plt.plot(loss_iter, '-xr')\n",
    "    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n",
    "    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n",
    "\n",
    "    loss_tr.append(loss.detach().cpu().item())\n",
    "    plt.figure()\n",
    "    plt.plot(loss_tr, '-or')\n",
    "    plt.title(f'Loss fuction at epoch{epoch}')\n",
    "    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n",
    "\n",
    "    plt.close('all')  # to avoid warnings\n",
    "    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n",
    "    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n",
    "    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n",
    "\n",
    "    # if epoch >10 :\n",
    "    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n",
    "    #     if s1 - s2 < 0 :\n",
    "    #         print('break-1')\n",
    "    #         break\n",
    "    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n",
    "    #     if abs((s1-s2)/s1) < 5e-4 :\n",
    "    #         print('break-2')\n",
    "    #         break\n",
    "print('done')\n",
    "print('starting date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%@title 'n5'\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#%%\n",
    "rid = 'n5' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "from unet.unet_model import *\n",
    "class UNetHalf(nn.Module):\n",
    "    \"16 layers here\"\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        \"\"\"Only the up part of the unet\n",
    "        Args:\n",
    "            n_channels ([type]): [how many input channels=n_sources]\n",
    "            n_classes ([type]): [how many output classes=n_sources]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_ch = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.n_ch = 128\n",
    "\n",
    "        self.up = nn.Sequential(DoubleConv(n_channels, self.n_ch),\n",
    "                    MyUp(self.n_ch, self.n_ch//2),\n",
    "                    MyUp(self.n_ch//2, self.n_ch//4),\n",
    "                    MyUp(self.n_ch//4, self.n_ch//4))\n",
    "        self.reshape = nn.Sequential(\n",
    "                    nn.Conv2d(self.n_ch//4, self.n_ch//4, kernel_size=3, padding=(1,2)),\n",
    "                    nn.BatchNorm2d(self.n_ch//4),\n",
    "                    nn.LeakyReLU(inplace=True),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4))\n",
    "        self.outc = OutConv(32, n_classes)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.reshape(x) \n",
    "        x = self.outc(x)\n",
    "        x = self.sig(x)\n",
    "        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n",
    "        return out\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, J = 3, 64, 66, 3\n",
    "NF = N*F\n",
    "eps, delta, glr = 5e-4, 1, 0.001 # delta is scale for Rb, glr is gamma learning rate\n",
    "opts = {}\n",
    "opts['n_ch'] = [1,1]  \n",
    "opts['batch_size'] = 128\n",
    "opts['EM_iter'] = 201\n",
    "opts['lr'] = 0.001\n",
    "opts['n_epochs'] = 71\n",
    "opts['d_gamma'] = 8 \n",
    "n = 5  # for stopping \n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM3FT64_data3.pt').to(torch.cdouble)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n",
    "loss_iter, loss_tr = [], []\n",
    "model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "\"initial\"\n",
    "xtr = xtr[:I]\n",
    "Htr = torch.load('../data/nem_ss/tr18kHCI10M3_data3.pt').to(torch.cdouble)[:I]\n",
    "Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n",
    "gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma']) # shape of [J,1,8,8], cpu()\n",
    "\n",
    "#%%\n",
    "for epoch in range(opts['n_epochs']):  \n",
    "    xtr, Rbtr, gtr, Htr = myshuffle((xtr, Rbtr, gtr, Htr))  \n",
    "    res = xtr, Rbtr, gtr, Htr\n",
    "    data = Data.TensorDataset(*res)\n",
    "    tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n",
    "    for i, (x, Rb, g, Hhat) in enumerate(tr): # gamma [n_batch, 4, 4]\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        model.eval()\n",
    "\n",
    "        \"EM part\"\n",
    "        Rb, g, Hhat = Rb.cuda(), g.cuda(), Hhat.cuda()\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(g[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n",
    "\n",
    "        x = x.cuda()\n",
    "        g.requires_grad_()\n",
    "        optim_gamma = torch.optim.RAdam([g],\n",
    "                lr= glr,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n",
    "        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n",
    "        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n",
    "        ll_traj = []\n",
    "\n",
    "        for ii in range(opts['EM_iter']):\n",
    "            \"E-step\"\n",
    "            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n",
    "            shat = W.permute(2,0,1,3,4) @ x[...,None]\n",
    "            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n",
    "            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n",
    "            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n",
    "\n",
    "            \"M-step\"\n",
    "            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n",
    "            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n",
    "                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n",
    "            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n",
    "            Rb.imag = Rb.imag - Rb.imag\n",
    "\n",
    "            outs = []\n",
    "            for j in range(J):\n",
    "                outs.append(model(g[:,j]))\n",
    "            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "            vhat.real = threshold(out)\n",
    "            loss = loss_func(vhat, Rsshatnf.cuda())\n",
    "            optim_gamma.zero_grad()   \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n",
    "            optim_gamma.step()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            \"compute log-likelyhood\"\n",
    "            vhat = vhat.detach()\n",
    "            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n",
    "            ll_traj.append(ll.item())\n",
    "            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n",
    "            # if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n",
    "            #     print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n",
    "            #     break\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print(f'batch {i} is done. ', 'Current time--', datetime.now())\n",
    "    \n",
    "        if i == 0 :\n",
    "            plt.figure()\n",
    "            plt.plot(ll_traj, '-x')\n",
    "            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n",
    "\n",
    "            for ii in range(J):\n",
    "                plt.figure()\n",
    "                plt.imshow(vhat[0,...,ii].real.cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'source of v{ii} in 1st sample from the 1st batch at epoch {epoch}')\n",
    "                plt.savefig(fig_loc + f'id{rid}_vj{ii}_epoch{epoch}')      \n",
    "\n",
    "        \"update variable\"\n",
    "        with torch.no_grad():\n",
    "            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n",
    "            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n",
    "            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n",
    "        g.requires_grad_(False)\n",
    "        model.train()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(g[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat.real = threshold(out)\n",
    "        optimizer.zero_grad()         \n",
    "        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n",
    "        loss = -ll\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        loss_iter.append(loss.detach().cpu().item())\n",
    "\n",
    "    print(f'done with epoch{epoch}')\n",
    "    plt.figure()\n",
    "    plt.plot(loss_iter, '-xr')\n",
    "    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n",
    "    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n",
    "\n",
    "    loss_tr.append(loss.detach().cpu().item())\n",
    "    plt.figure()\n",
    "    plt.plot(loss_tr, '-or')\n",
    "    plt.title(f'Loss fuction at epoch{epoch}')\n",
    "    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n",
    "\n",
    "    plt.close('all')  # to avoid warnings\n",
    "    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n",
    "    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n",
    "    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n",
    "\n",
    "    # if epoch >10 :\n",
    "    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n",
    "    #     if s1 - s2 < 0 :\n",
    "    #         print('break-1')\n",
    "    #         break\n",
    "    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n",
    "    #     if abs((s1-s2)/s1) < 5e-4 :\n",
    "    #         print('break-2')\n",
    "    #         break\n",
    "print('done')\n",
    "print('starting date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%@title 'n7'\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='1'\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#%%\n",
    "rid = 'n7' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "from unet.unet_model import *\n",
    "class UNetHalf(nn.Module):\n",
    "    \"16 layers here\"\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        \"\"\"Only the up part of the unet\n",
    "        Args:\n",
    "            n_channels ([type]): [how many input channels=n_sources]\n",
    "            n_classes ([type]): [how many output classes=n_sources]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_ch = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.n_ch = 128\n",
    "\n",
    "        self.up = nn.Sequential(DoubleConv(n_channels, self.n_ch),\n",
    "                    MyUp(self.n_ch, self.n_ch//2),\n",
    "                    MyUp(self.n_ch//2, self.n_ch//4),\n",
    "                    MyUp(self.n_ch//4, self.n_ch//4))\n",
    "        self.reshape = nn.Sequential(\n",
    "                    nn.Conv2d(self.n_ch//4, self.n_ch//4, kernel_size=3, padding=(1,2)),\n",
    "                    nn.BatchNorm2d(self.n_ch//4),\n",
    "                    nn.LeakyReLU(inplace=True),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4))\n",
    "        self.outc = OutConv(32, n_classes)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.reshape(x) \n",
    "        x = self.outc(x)\n",
    "        x = self.sig(x)\n",
    "        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n",
    "        return out\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, J = 3, 64, 66, 3\n",
    "NF = N*F\n",
    "eps, delta, glr = 5e-4, 1, 0.001 # delta is scale for Rb, glr is gamma learning rate\n",
    "opts = {}\n",
    "opts['n_ch'] = [1,1]  \n",
    "opts['batch_size'] = 128\n",
    "opts['EM_iter'] = 201\n",
    "opts['lr'] = 0.001\n",
    "opts['n_epochs'] = 71\n",
    "opts['d_gamma'] = 8 \n",
    "n = 5  # for stopping \n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM3FT64_data3.pt').to(torch.cdouble)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n",
    "loss_iter, loss_tr = [], []\n",
    "model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "\"initial\"\n",
    "xtr = xtr[:I]\n",
    "Htr = torch.load('../data/nem_ss/tr18kHCI10M3_data3.pt').to(torch.cdouble)[:I]\n",
    "Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n",
    "gamma = torch.rand(opts['batch_size'],J,1,opts['d_gamma'], opts['d_gamma']).cuda() # shape of[I,J,1,8,8]\n",
    "\n",
    "#%%\n",
    "for epoch in range(opts['n_epochs']):  \n",
    "    xtr, Rbtr, Htr = myshuffle((xtr, Rbtr, Htr))  \n",
    "    res = xtr, Rbtr, Htr\n",
    "    data = Data.TensorDataset(*res)\n",
    "    tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n",
    "    for i, (x, Rb, Hhat) in enumerate(tr): # gamma [n_batch, 4, 4]\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        model.eval()\n",
    "\n",
    "        \"EM part\"\n",
    "        Rb, Hhat = Rb.cuda(), Hhat.cuda()\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(gamma[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n",
    "\n",
    "        x = x.cuda()\n",
    "        gamma.requires_grad_()\n",
    "        optim_gamma = torch.optim.RAdam([gamma],\n",
    "                lr= glr,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n",
    "        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n",
    "        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n",
    "        ll_traj = []\n",
    "\n",
    "        for ii in range(opts['EM_iter']):\n",
    "            \"E-step\"\n",
    "            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n",
    "            shat = W.permute(2,0,1,3,4) @ x[...,None]\n",
    "            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n",
    "            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n",
    "            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n",
    "\n",
    "            \"M-step\"\n",
    "            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n",
    "            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n",
    "                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n",
    "            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n",
    "            Rb.imag = Rb.imag - Rb.imag\n",
    "\n",
    "            outs = []\n",
    "            for j in range(J):\n",
    "                outs.append(model(gamma[:,j]))\n",
    "            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "            vhat.real = threshold(out)\n",
    "            loss = loss_func(vhat, Rsshatnf.cuda())\n",
    "            optim_gamma.zero_grad()   \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_([gamma], max_norm=10)\n",
    "            optim_gamma.step()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            \"compute log-likelyhood\"\n",
    "            vhat = vhat.detach()\n",
    "            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n",
    "            ll_traj.append(ll.item())\n",
    "            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n",
    "            # if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n",
    "            #     print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n",
    "            #     break\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print(f'batch {i} is done. ', 'Current time--', datetime.now())\n",
    "    \n",
    "        if i == 0 :\n",
    "            plt.figure()\n",
    "            plt.plot(ll_traj, '-x')\n",
    "            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n",
    "\n",
    "            for ii in range(J):\n",
    "                plt.figure()\n",
    "                plt.imshow(vhat[0,...,ii].real.cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'source of v{ii} in 1st sample from the 1st batch at epoch {epoch}')\n",
    "                plt.savefig(fig_loc + f'id{rid}_vj{ii}_epoch{epoch}')      \n",
    "\n",
    "        \"update variable\"\n",
    "        with torch.no_grad():\n",
    "            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n",
    "            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n",
    "        gamma.requires_grad_(False)\n",
    "        model.train()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(gamma[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat.real = threshold(out)\n",
    "        optimizer.zero_grad()         \n",
    "        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n",
    "        loss = -ll\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        loss_iter.append(loss.detach().cpu().item())\n",
    "\n",
    "    print(f'done with epoch{epoch}')\n",
    "    plt.figure()\n",
    "    plt.plot(loss_iter, '-xr')\n",
    "    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n",
    "    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n",
    "\n",
    "    loss_tr.append(loss.detach().cpu().item())\n",
    "    plt.figure()\n",
    "    plt.plot(loss_tr, '-or')\n",
    "    plt.title(f'Loss fuction at epoch{epoch}')\n",
    "    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n",
    "\n",
    "    plt.close('all')  # to avoid warnings\n",
    "    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n",
    "    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n",
    "    torch.save(gamma.detach().cpu(), mod_loc +f'gamma_rid{rid}_{epoch}.pt')    \n",
    "\n",
    "    # if epoch >10 :\n",
    "    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n",
    "    #     if s1 - s2 < 0 :\n",
    "    #         print('break-1')\n",
    "    #         break\n",
    "    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n",
    "    #     if abs((s1-s2)/s1) < 5e-4 :\n",
    "    #         print('break-2')\n",
    "    #         break\n",
    "print('done')\n",
    "print('starting date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%@title 'n8'\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#%%\n",
    "rid = 'n8' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "from unet.unet_model import *\n",
    "class UNetHalf(nn.Module):\n",
    "    \"16 layers here\"\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        \"\"\"Only the up part of the unet\n",
    "        Args:\n",
    "            n_channels ([type]): [how many input channels=n_sources]\n",
    "            n_classes ([type]): [how many output classes=n_sources]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_ch = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.n_ch = int(128*1.5)\n",
    "\n",
    "        self.up = nn.Sequential(DoubleConv(n_channels, self.n_ch),\n",
    "                    MyUp(self.n_ch, self.n_ch//2),\n",
    "                    MyUp(self.n_ch//2, self.n_ch//4),\n",
    "                    MyUp(self.n_ch//4, self.n_ch//4))\n",
    "        self.reshape = nn.Sequential(\n",
    "                    nn.Conv2d(self.n_ch//4, self.n_ch//4, kernel_size=3, padding=(1,2)),\n",
    "                    nn.BatchNorm2d(self.n_ch//4),\n",
    "                    nn.LeakyReLU(inplace=True),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4))\n",
    "        self.outc = OutConv(self.n_ch//4, n_classes)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.reshape(x) \n",
    "        x = self.outc(x)\n",
    "        x = self.sig(x)\n",
    "        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n",
    "        return out\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, J = 6, 64, 66, 6\n",
    "NF = N*F\n",
    "eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n",
    "opts = {}\n",
    "opts['n_ch'] = [1,1]  \n",
    "opts['batch_size'] = 64\n",
    "opts['EM_iter'] = 201\n",
    "opts['lr'] = 0.01\n",
    "opts['n_epochs'] = 71\n",
    "opts['d_gamma'] = 8 \n",
    "n = 5  # for stopping \n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM6FT64_data3.pt').to(torch.cdouble)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n",
    "loss_iter, loss_tr = [], []\n",
    "model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "\"initial\"\n",
    "xtr = xtr[:I]\n",
    "Htr = torch.load('../data/nem_ss/tr18kHCI10M6_data3.pt').to(torch.cdouble)[:I]\n",
    "Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n",
    "gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma']) # shape of [J,1,8,8], cpu()\n",
    "\n",
    "#%%\n",
    "for epoch in range(opts['n_epochs']):  \n",
    "    xtr, Rbtr, gtr, Htr = myshuffle((xtr, Rbtr, gtr, Htr))  \n",
    "    res = xtr, Rbtr, gtr, Htr\n",
    "    data = Data.TensorDataset(*res)\n",
    "    tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n",
    "    for i, (x, Rb, g, Hhat) in enumerate(tr): # gamma [n_batch, 4, 4]\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        model.eval()\n",
    "\n",
    "        \"EM part\"\n",
    "        Rb, g, Hhat = Rb.cuda(), g.cuda(), Hhat.cuda()\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(g[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n",
    "\n",
    "        x = x.cuda()\n",
    "        g.requires_grad_()\n",
    "        optim_gamma = torch.optim.RAdam([g],\n",
    "                lr= glr,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n",
    "        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n",
    "        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n",
    "        ll_traj = []\n",
    "\n",
    "        for ii in range(opts['EM_iter']):\n",
    "            \"E-step\"\n",
    "            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n",
    "            shat = W.permute(2,0,1,3,4) @ x[...,None]\n",
    "            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n",
    "            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n",
    "            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n",
    "\n",
    "            \"M-step\"\n",
    "            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n",
    "            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n",
    "                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n",
    "            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n",
    "            Rb.imag = Rb.imag - Rb.imag\n",
    "\n",
    "            outs = []\n",
    "            for j in range(J):\n",
    "                outs.append(model(g[:,j]))\n",
    "            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "            vhat.real = threshold(out)\n",
    "            loss = loss_func(vhat, Rsshatnf.cuda())\n",
    "            optim_gamma.zero_grad()   \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n",
    "            optim_gamma.step()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            \"compute log-likelyhood\"\n",
    "            vhat = vhat.detach()\n",
    "            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n",
    "            ll_traj.append(ll.item())\n",
    "            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n",
    "            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n",
    "                # print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print(f'batch {i} is done. ', 'Current time--', datetime.now())\n",
    "    \n",
    "        if i == 0 :\n",
    "            plt.figure()\n",
    "            plt.plot(ll_traj, '-x')\n",
    "            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n",
    "\n",
    "            for ii in range(J):\n",
    "                plt.figure()\n",
    "                plt.imshow(vhat[0,...,ii].real.cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'source of v{ii} in 1st sample from the 1st batch at epoch {epoch}')\n",
    "                plt.savefig(fig_loc + f'id{rid}_vj{ii}_epoch{epoch}')      \n",
    "\n",
    "        \"update variable\"\n",
    "        with torch.no_grad():\n",
    "            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n",
    "            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n",
    "            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n",
    "        g.requires_grad_(False)\n",
    "        model.train()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(g[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat.real = threshold(out)\n",
    "        optimizer.zero_grad()         \n",
    "        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n",
    "        loss = -ll\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        loss_iter.append(loss.detach().cpu().item())\n",
    "\n",
    "    print(f'done with epoch{epoch}')\n",
    "    plt.figure()\n",
    "    plt.plot(loss_iter, '-xr')\n",
    "    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n",
    "    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n",
    "\n",
    "    loss_tr.append(loss.detach().cpu().item())\n",
    "    plt.figure()\n",
    "    plt.plot(loss_tr, '-or')\n",
    "    plt.title(f'Loss fuction at epoch{epoch}')\n",
    "    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n",
    "\n",
    "    plt.close('all')  # to avoid warnings\n",
    "    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n",
    "    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n",
    "    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n",
    "\n",
    "    # if epoch >10 :\n",
    "    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n",
    "    #     if s1 - s2 < 0 :\n",
    "    #         print('break-1')\n",
    "    #         break\n",
    "    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n",
    "    #     if abs((s1-s2)/s1) < 5e-4 :\n",
    "    #         print('break-2')\n",
    "    #         break\n",
    "print('done')\n",
    "print('starting date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%@title 'n9'\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#%%\n",
    "rid = 'n9' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "from unet.unet_model import *\n",
    "class UNetHalf(nn.Module):\n",
    "    \"16 layers here\"\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        \"\"\"Only the up part of the unet\n",
    "        Args:\n",
    "            n_channels ([type]): [how many input channels=n_sources]\n",
    "            n_classes ([type]): [how many output classes=n_sources]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_ch = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.n_ch = int(128*1.5)\n",
    "\n",
    "        self.up = nn.Sequential(DoubleConv(n_channels, self.n_ch),\n",
    "                    MyUp(self.n_ch, self.n_ch//2),\n",
    "                    MyUp(self.n_ch//2, self.n_ch//4),\n",
    "                    MyUp(self.n_ch//4, self.n_ch//4))\n",
    "        self.reshape = nn.Sequential(\n",
    "                    nn.Conv2d(self.n_ch//4, self.n_ch//4, kernel_size=3, padding=(1,2)),\n",
    "                    nn.BatchNorm2d(self.n_ch//4),\n",
    "                    nn.LeakyReLU(inplace=True),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4))\n",
    "        self.outc = OutConv(self.n_ch//4, n_classes)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.reshape(x) \n",
    "        x = self.outc(x)\n",
    "        x = self.sig(x)\n",
    "        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n",
    "        return out\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, J = 6, 64, 66, 6\n",
    "NF = N*F\n",
    "eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n",
    "opts = {}\n",
    "opts['n_ch'] = [1,1]  \n",
    "opts['batch_size'] = 64\n",
    "opts['EM_iter'] = 201\n",
    "opts['lr'] = 0.001\n",
    "opts['n_epochs'] = 71\n",
    "opts['d_gamma'] = 8 \n",
    "n = 5  # for stopping \n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM6FT64_data3.pt').to(torch.cdouble)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n",
    "loss_iter, loss_tr = [], []\n",
    "model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "\"initial\"\n",
    "xtr = xtr[:I]\n",
    "Htr = torch.load('../data/nem_ss/tr18kHCI10M6_data3.pt').to(torch.cdouble)[:I]\n",
    "Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n",
    "gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma']) # shape of [J,1,8,8], cpu()\n",
    "\n",
    "#%%\n",
    "for epoch in range(opts['n_epochs']):  \n",
    "    xtr, Rbtr, gtr, Htr = myshuffle((xtr, Rbtr, gtr, Htr))  \n",
    "    res = xtr, Rbtr, gtr, Htr\n",
    "    data = Data.TensorDataset(*res)\n",
    "    tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n",
    "    for i, (x, Rb, g, Hhat) in enumerate(tr): # gamma [n_batch, 4, 4]\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        model.eval()\n",
    "\n",
    "        \"EM part\"\n",
    "        Rb, g, Hhat = Rb.cuda(), g.cuda(), Hhat.cuda()\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(g[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n",
    "\n",
    "        x = x.cuda()\n",
    "        g.requires_grad_()\n",
    "        optim_gamma = torch.optim.RAdam([g],\n",
    "                lr= glr,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n",
    "        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n",
    "        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n",
    "        ll_traj = []\n",
    "\n",
    "        for ii in range(opts['EM_iter']):\n",
    "            \"E-step\"\n",
    "            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n",
    "            shat = W.permute(2,0,1,3,4) @ x[...,None]\n",
    "            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n",
    "            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n",
    "            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n",
    "\n",
    "            \"M-step\"\n",
    "            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n",
    "            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n",
    "                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n",
    "            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n",
    "            Rb.imag = Rb.imag - Rb.imag\n",
    "\n",
    "            outs = []\n",
    "            for j in range(J):\n",
    "                outs.append(model(g[:,j]))\n",
    "            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "            vhat.real = threshold(out)\n",
    "            loss = loss_func(vhat, Rsshatnf.cuda())\n",
    "            optim_gamma.zero_grad()   \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n",
    "            optim_gamma.step()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            \"compute log-likelyhood\"\n",
    "            vhat = vhat.detach()\n",
    "            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n",
    "            ll_traj.append(ll.item())\n",
    "            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n",
    "            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n",
    "                # print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print(f'batch {i} is done. ', 'Current time--', datetime.now())\n",
    "    \n",
    "        if i == 0 :\n",
    "            plt.figure()\n",
    "            plt.plot(ll_traj, '-x')\n",
    "            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n",
    "\n",
    "            for ii in range(J):\n",
    "                plt.figure()\n",
    "                plt.imshow(vhat[0,...,ii].real.cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'source of v{ii} in 1st sample from the 1st batch at epoch {epoch}')\n",
    "                plt.savefig(fig_loc + f'id{rid}_vj{ii}_epoch{epoch}')      \n",
    "\n",
    "        \"update variable\"\n",
    "        with torch.no_grad():\n",
    "            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n",
    "            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n",
    "            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n",
    "        g.requires_grad_(False)\n",
    "        model.train()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(g[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat.real = threshold(out)\n",
    "        optimizer.zero_grad()         \n",
    "        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n",
    "        loss = -ll\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        loss_iter.append(loss.detach().cpu().item())\n",
    "\n",
    "    print(f'done with epoch{epoch}')\n",
    "    plt.figure()\n",
    "    plt.plot(loss_iter, '-xr')\n",
    "    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n",
    "    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n",
    "\n",
    "    loss_tr.append(loss.detach().cpu().item())\n",
    "    plt.figure()\n",
    "    plt.plot(loss_tr, '-or')\n",
    "    plt.title(f'Loss fuction at epoch{epoch}')\n",
    "    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n",
    "\n",
    "    plt.close('all')  # to avoid warnings\n",
    "    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n",
    "    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n",
    "    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n",
    "\n",
    "    # if epoch >10 :\n",
    "    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n",
    "    #     if s1 - s2 < 0 :\n",
    "    #         print('break-1')\n",
    "    #         break\n",
    "    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n",
    "    #     if abs((s1-s2)/s1) < 5e-4 :\n",
    "    #         print('break-2')\n",
    "    #         break\n",
    "print('done')\n",
    "print('starting date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%@title 'n10'\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#%%\n",
    "rid = 'n10' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "from unet.unet_model import *\n",
    "class UNetHalf(nn.Module):\n",
    "    \"16 layers here\"\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        \"\"\"Only the up part of the unet\n",
    "        Args:\n",
    "            n_channels ([type]): [how many input channels=n_sources]\n",
    "            n_classes ([type]): [how many output classes=n_sources]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_ch = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.n_ch = int(128*1.5)\n",
    "\n",
    "        self.up = nn.Sequential(DoubleConv(n_channels, self.n_ch),\n",
    "                    MyUp(self.n_ch, self.n_ch//2),\n",
    "                    MyUp(self.n_ch//2, self.n_ch//4),\n",
    "                    MyUp(self.n_ch//4, self.n_ch//4))\n",
    "        self.reshape = nn.Sequential(\n",
    "                    nn.Conv2d(self.n_ch//4, self.n_ch//4, kernel_size=3, padding=(1,2)),\n",
    "                    nn.BatchNorm2d(self.n_ch//4),\n",
    "                    nn.LeakyReLU(inplace=True),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4))\n",
    "        self.outc = OutConv(self.n_ch//4, n_classes)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.reshape(x) \n",
    "        x = self.outc(x)\n",
    "        x = self.sig(x)\n",
    "        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n",
    "        return out\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, J = 6, 64, 66, 6\n",
    "NF = N*F\n",
    "eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n",
    "opts = {}\n",
    "opts['n_ch'] = [1,1]  \n",
    "opts['batch_size'] = 64\n",
    "opts['EM_iter'] = 201\n",
    "opts['lr'] = 0.01\n",
    "opts['n_epochs'] = 71\n",
    "opts['d_gamma'] = 8 \n",
    "n = 5  # for stopping \n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM6FT64_data3.pt').to(torch.cdouble)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n",
    "loss_iter, loss_tr = [], []\n",
    "model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "\"initial\"\n",
    "xtr = xtr[:I]\n",
    "Htr = torch.load('../data/nem_ss/tr18kHCI10M6_data3.pt').to(torch.cdouble)[:I]\n",
    "Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n",
    "gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma']) # shape of [J,1,8,8], cpu()\n",
    "\n",
    "#%%\n",
    "for epoch in range(opts['n_epochs']):  \n",
    "    xtr, Rbtr, gtr, Htr = myshuffle((xtr, Rbtr, gtr, Htr))  \n",
    "    res = xtr, Rbtr, gtr, Htr\n",
    "    data = Data.TensorDataset(*res)\n",
    "    tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n",
    "    for i, (x, Rb, g, Hhat) in enumerate(tr): # gamma [n_batch, 4, 4]\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        model.eval()\n",
    "\n",
    "        \"EM part\"\n",
    "        Rb, g, Hhat = Rb.cuda(), g.cuda(), Hhat.cuda()\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(g[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n",
    "\n",
    "        x = x.cuda()\n",
    "        g.requires_grad_()\n",
    "        optim_gamma = torch.optim.RAdam([g],\n",
    "                lr= glr,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n",
    "        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n",
    "        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n",
    "        ll_traj = []\n",
    "\n",
    "        for ii in range(opts['EM_iter']):\n",
    "            \"E-step\"\n",
    "            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n",
    "            shat = W.permute(2,0,1,3,4) @ x[...,None]\n",
    "            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n",
    "            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n",
    "            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n",
    "\n",
    "            \"M-step\"\n",
    "            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n",
    "            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n",
    "                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n",
    "            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n",
    "            Rb.imag = Rb.imag - Rb.imag\n",
    "\n",
    "            outs = []\n",
    "            for j in range(J):\n",
    "                outs.append(model(g[:,j]))\n",
    "            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "            vhat.real = threshold(out)\n",
    "            loss = loss_func(vhat, Rsshatnf.cuda())\n",
    "            optim_gamma.zero_grad()   \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n",
    "            optim_gamma.step()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            \"compute log-likelyhood\"\n",
    "            vhat = vhat.detach()\n",
    "            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n",
    "            ll_traj.append(ll.item())\n",
    "            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n",
    "            # if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n",
    "            #     # print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n",
    "            #     break\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print(f'batch {i} is done. ', 'Current time--', datetime.now())\n",
    "    \n",
    "        if i == 0 :\n",
    "            plt.figure()\n",
    "            plt.plot(ll_traj, '-x')\n",
    "            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n",
    "\n",
    "            for ii in range(J):\n",
    "                plt.figure()\n",
    "                plt.imshow(vhat[0,...,ii].real.cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'source of v{ii} in 1st sample from the 1st batch at epoch {epoch}')\n",
    "                plt.savefig(fig_loc + f'id{rid}_vj{ii}_epoch{epoch}')      \n",
    "\n",
    "        \"update variable\"\n",
    "        with torch.no_grad():\n",
    "            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n",
    "            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n",
    "            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n",
    "        g.requires_grad_(False)\n",
    "        model.train()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(g[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat.real = threshold(out)\n",
    "        optimizer.zero_grad()         \n",
    "        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n",
    "        loss = -ll\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        loss_iter.append(loss.detach().cpu().item())\n",
    "\n",
    "    print(f'done with epoch{epoch}')\n",
    "    plt.figure()\n",
    "    plt.plot(loss_iter, '-xr')\n",
    "    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n",
    "    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n",
    "\n",
    "    loss_tr.append(loss.detach().cpu().item())\n",
    "    plt.figure()\n",
    "    plt.plot(loss_tr, '-or')\n",
    "    plt.title(f'Loss fuction at epoch{epoch}')\n",
    "    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n",
    "\n",
    "    plt.close('all')  # to avoid warnings\n",
    "    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n",
    "    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n",
    "    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n",
    "\n",
    "    # if epoch >10 :\n",
    "    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n",
    "    #     if s1 - s2 < 0 :\n",
    "    #         print('break-1')\n",
    "    #         break\n",
    "    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n",
    "    #     if abs((s1-s2)/s1) < 5e-4 :\n",
    "    #         print('break-2')\n",
    "    #         break\n",
    "print('done')\n",
    "print('starting date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%@title 'n11'\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#%%\n",
    "rid = 'n11' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "from unet.unet_model import *\n",
    "class UNetHalf(nn.Module):\n",
    "    \"16 layers here\"\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        \"\"\"Only the up part of the unet\n",
    "        Args:\n",
    "            n_channels ([type]): [how many input channels=n_sources]\n",
    "            n_classes ([type]): [how many output classes=n_sources]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_ch = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.n_ch = 128\n",
    "\n",
    "        self.up = nn.Sequential(DoubleConv(n_channels, self.n_ch),\n",
    "                    MyUp(self.n_ch, self.n_ch//2),\n",
    "                    MyUp(self.n_ch//2, self.n_ch//4),\n",
    "                    MyUp(self.n_ch//4, self.n_ch//4))\n",
    "        self.reshape = nn.Sequential(\n",
    "                    nn.Conv2d(self.n_ch//4, self.n_ch//4, kernel_size=3, padding=(1,2)),\n",
    "                    nn.BatchNorm2d(self.n_ch//4),\n",
    "                    nn.LeakyReLU(inplace=True),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4))\n",
    "        self.outc = OutConv(32, n_classes)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.reshape(x) \n",
    "        x = self.outc(x)\n",
    "        x = self.sig(x)\n",
    "        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n",
    "        return out\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, J = 3, 64, 66, 3\n",
    "NF = N*F\n",
    "eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n",
    "opts = {}\n",
    "opts['n_ch'] = [1,1]  \n",
    "opts['batch_size'] = 128\n",
    "opts['EM_iter'] = 201\n",
    "opts['lr'] = 0.001\n",
    "opts['n_epochs'] = 71\n",
    "opts['d_gamma'] = 8 \n",
    "n = 5  # for stopping \n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM3FT64_data4.pt').to(torch.cdouble)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n",
    "loss_iter, loss_tr = [], []\n",
    "model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "\"initial\"\n",
    "xtr = xtr[:I]\n",
    "Htr = torch.load('../data/nem_ss/tr18kHCI10M3_data4.pt').to(torch.cdouble)[:I]\n",
    "Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n",
    "gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma']) # shape of [J,1,8,8], cpu()\n",
    "\n",
    "#%%\n",
    "for epoch in range(opts['n_epochs']):  \n",
    "    xtr, Rbtr, gtr, Htr = myshuffle((xtr, Rbtr, gtr, Htr))  \n",
    "    res = xtr, Rbtr, gtr, Htr\n",
    "    data = Data.TensorDataset(*res)\n",
    "    tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n",
    "    for i, (x, Rb, g, Hhat) in enumerate(tr): # gamma [n_batch, 4, 4]\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        model.eval()\n",
    "\n",
    "        \"EM part\"\n",
    "        Rb, g, Hhat = Rb.cuda(), g.cuda(), Hhat.cuda()\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(g[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n",
    "\n",
    "        x = x.cuda()\n",
    "        g.requires_grad_()\n",
    "        optim_gamma = torch.optim.RAdam([g],\n",
    "                lr= glr,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n",
    "        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n",
    "        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n",
    "        ll_traj = []\n",
    "\n",
    "        for ii in range(opts['EM_iter']):\n",
    "            \"E-step\"\n",
    "            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n",
    "            shat = W.permute(2,0,1,3,4) @ x[...,None]\n",
    "            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n",
    "            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n",
    "            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n",
    "\n",
    "            \"M-step\"\n",
    "            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n",
    "            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n",
    "                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n",
    "            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n",
    "            Rb.imag = Rb.imag - Rb.imag\n",
    "\n",
    "            outs = []\n",
    "            for j in range(J):\n",
    "                outs.append(model(g[:,j]))\n",
    "            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "            vhat.real = threshold(out)\n",
    "            loss = loss_func(vhat, Rsshatnf.cuda())\n",
    "            optim_gamma.zero_grad()   \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n",
    "            optim_gamma.step()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            \"compute log-likelyhood\"\n",
    "            vhat = vhat.detach()\n",
    "            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n",
    "            ll_traj.append(ll.item())\n",
    "            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n",
    "            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n",
    "            #     print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print(f'batch {i} is done. ', 'Current time--', datetime.now())\n",
    "    \n",
    "        if i == 0 :\n",
    "            plt.figure()\n",
    "            plt.plot(ll_traj, '-x')\n",
    "            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n",
    "\n",
    "            for ii in range(J):\n",
    "                plt.figure()\n",
    "                plt.imshow(vhat[0,...,ii].real.cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'source of v{ii} in 1st sample from the 1st batch at epoch {epoch}')\n",
    "                plt.savefig(fig_loc + f'id{rid}_vj{ii}_epoch{epoch}')      \n",
    "\n",
    "        \"update variable\"\n",
    "        with torch.no_grad():\n",
    "            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n",
    "            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n",
    "            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n",
    "        g.requires_grad_(False)\n",
    "        model.train()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(g[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat.real = threshold(out)\n",
    "        optimizer.zero_grad()         \n",
    "        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n",
    "        loss = -ll\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        loss_iter.append(loss.detach().cpu().item())\n",
    "\n",
    "    print(f'done with epoch{epoch}')\n",
    "    plt.figure()\n",
    "    plt.plot(loss_iter, '-xr')\n",
    "    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n",
    "    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n",
    "\n",
    "    loss_tr.append(loss.detach().cpu().item())\n",
    "    plt.figure()\n",
    "    plt.plot(loss_tr, '-or')\n",
    "    plt.title(f'Loss fuction at epoch{epoch}')\n",
    "    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n",
    "\n",
    "    plt.close('all')  # to avoid warnings\n",
    "    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n",
    "    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n",
    "    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n",
    "\n",
    "    # if epoch >10 :\n",
    "    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n",
    "    #     if s1 - s2 < 0 :\n",
    "    #         print('break-1')\n",
    "    #         break\n",
    "    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n",
    "    #     if abs((s1-s2)/s1) < 5e-4 :\n",
    "    #         print('break-2')\n",
    "    #         break\n",
    "print('done')\n",
    "print('starting date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%@title 'n15c'\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#%%\n",
    "rid = 'n15c' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "from unet.unet_model import *\n",
    "class UNetHalf(nn.Module):\n",
    "    \"16 layers here\"\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        \"\"\"Only the up part of the unet\n",
    "        Args:\n",
    "            n_channels ([type]): [how many input channels=n_sources]\n",
    "            n_classes ([type]): [how many output classes=n_sources]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_ch = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.n_ch = int(128*1.5)\n",
    "\n",
    "        self.up = nn.Sequential(DoubleConv(n_channels, self.n_ch),\n",
    "                    MyUp(self.n_ch, self.n_ch//2),\n",
    "                    MyUp(self.n_ch//2, self.n_ch//4),\n",
    "                    MyUp(self.n_ch//4, self.n_ch//4))\n",
    "        self.reshape = nn.Sequential(\n",
    "                    nn.Conv2d(self.n_ch//4, self.n_ch//4, kernel_size=3, padding=(1,2)),\n",
    "                    nn.BatchNorm2d(self.n_ch//4),\n",
    "                    nn.LeakyReLU(inplace=True),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4))\n",
    "        self.outc = OutConv(self.n_ch//4, n_classes)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.reshape(x) \n",
    "        x = self.outc(x)\n",
    "        x = self.sig(x)\n",
    "        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n",
    "        return out\n",
    "\n",
    "I = 3000 # how many samples\n",
    "M, N, F, J = 6, 64, 66, 6\n",
    "NF = N*F\n",
    "eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n",
    "opts = {}\n",
    "opts['n_ch'] = [1,1]  \n",
    "opts['batch_size'] = 64\n",
    "opts['EM_iter'] = 201\n",
    "opts['lr'] = 0.01\n",
    "opts['n_epochs'] = 16\n",
    "opts['d_gamma'] = 8 \n",
    "n = 5  # for stopping \n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM6FT64_data4.pt').to(torch.cdouble)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n",
    "loss_iter, loss_tr = [], []\n",
    "model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "model = torch.load('../data/nem_ss/models/n8/model_ridn8_30.pt')\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "\"initial\"\n",
    "xtr = xtr[:I]\n",
    "Htr = torch.load('../data/nem_ss/tr18kHCI10M6_data4.pt').to(torch.cdouble)[:I]\n",
    "Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n",
    "gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma']) # shape of [J,1,8,8], cpu()\n",
    "\n",
    "#%%\n",
    "for epoch in range(opts['n_epochs']):  \n",
    "    xtr, Rbtr, gtr, Htr = myshuffle((xtr, Rbtr, gtr, Htr))  \n",
    "    res = xtr, Rbtr, gtr, Htr\n",
    "    data = Data.TensorDataset(*res)\n",
    "    tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n",
    "    for i, (x, Rb, g, Hhat) in enumerate(tr): # gamma [n_batch, 4, 4]\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        model.eval()\n",
    "\n",
    "        \"EM part\"\n",
    "        Rb, g, Hhat = Rb.cuda(), g.cuda(), Hhat.cuda()\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(g[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n",
    "\n",
    "        x = x.cuda()\n",
    "        g.requires_grad_()\n",
    "        optim_gamma = torch.optim.RAdam([g],\n",
    "                lr= glr,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n",
    "        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n",
    "        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n",
    "        ll_traj = []\n",
    "\n",
    "        for ii in range(opts['EM_iter']):\n",
    "            \"E-step\"\n",
    "            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n",
    "            shat = W.permute(2,0,1,3,4) @ x[...,None]\n",
    "            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n",
    "            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n",
    "            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n",
    "\n",
    "            \"M-step\"\n",
    "            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n",
    "            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n",
    "                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n",
    "            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n",
    "            Rb.imag = Rb.imag - Rb.imag\n",
    "\n",
    "            outs = []\n",
    "            for j in range(J):\n",
    "                outs.append(model(g[:,j]))\n",
    "            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "            vhat.real = threshold(out)\n",
    "            loss = loss_func(vhat, Rsshatnf.cuda())\n",
    "            optim_gamma.zero_grad()   \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n",
    "            optim_gamma.step()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            \"compute log-likelyhood\"\n",
    "            vhat = vhat.detach()\n",
    "            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n",
    "            ll_traj.append(ll.item())\n",
    "            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n",
    "            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n",
    "                # print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print(f'batch {i} is done. ', 'Current time--', datetime.now())\n",
    "    \n",
    "        if i == 0 :\n",
    "            plt.figure()\n",
    "            plt.plot(ll_traj, '-x')\n",
    "            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n",
    "\n",
    "            for ii in range(J):\n",
    "                plt.figure()\n",
    "                plt.imshow(vhat[0,...,ii].real.cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'source of v{ii} in 1st sample from the 1st batch at epoch {epoch}')\n",
    "                plt.savefig(fig_loc + f'id{rid}_vj{ii}_epoch{epoch}')      \n",
    "\n",
    "        \"update variable\"\n",
    "        with torch.no_grad():\n",
    "            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n",
    "            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n",
    "            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n",
    "        g.requires_grad_(False)\n",
    "        model.train()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(g[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat.real = threshold(out)\n",
    "        optimizer.zero_grad()         \n",
    "        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n",
    "        loss = -ll\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        loss_iter.append(loss.detach().cpu().item())\n",
    "\n",
    "    print(f'done with epoch{epoch}')\n",
    "    plt.figure()\n",
    "    plt.plot(loss_iter, '-xr')\n",
    "    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n",
    "    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n",
    "\n",
    "    loss_tr.append(loss.detach().cpu().item())\n",
    "    plt.figure()\n",
    "    plt.plot(loss_tr, '-or')\n",
    "    plt.title(f'Loss fuction at epoch{epoch}')\n",
    "    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n",
    "\n",
    "    plt.close('all')  # to avoid warnings\n",
    "    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n",
    "    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n",
    "    torch.save(Htr, mod_loc +f'Htr_rid{rid}_{epoch}.pt')    \n",
    "\n",
    "    # if epoch >10 :\n",
    "    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n",
    "    #     if s1 - s2 < 0 :\n",
    "    #         print('break-1')\n",
    "    #         break\n",
    "    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n",
    "    #     if abs((s1-s2)/s1) < 5e-4 :\n",
    "    #         print('break-2')\n",
    "    #         break\n",
    "print('done')\n",
    "print('starting date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%@title 'n20'\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#%%\n",
    "rid = 'n20' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "from unet.unet_model import *\n",
    "class UNetHalf(nn.Module):\n",
    "    \"16 layers here\"\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        \"\"\"Only the up part of the unet\n",
    "        Args:\n",
    "            n_channels ([type]): [how many input channels=n_sources]\n",
    "            n_classes ([type]): [how many output classes=n_sources]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_ch = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.n_ch = int(128*1.5)\n",
    "\n",
    "        self.up = nn.Sequential(DoubleConv(n_channels, self.n_ch),\n",
    "                    MyUp(self.n_ch, self.n_ch//2),\n",
    "                    MyUp(self.n_ch//2, self.n_ch//4),\n",
    "                    MyUp(self.n_ch//4, self.n_ch//4))\n",
    "        self.reshape = nn.Sequential(\n",
    "                    nn.Conv2d(self.n_ch//4, self.n_ch//4, kernel_size=3, padding=(1,2)),\n",
    "                    nn.BatchNorm2d(self.n_ch//4),\n",
    "                    nn.LeakyReLU(inplace=True),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4))\n",
    "        self.outc = OutConv(self.n_ch//4, n_classes)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.reshape(x) \n",
    "        x = self.outc(x)\n",
    "        x = self.sig(x)\n",
    "        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n",
    "        return out\n",
    "\n",
    "I = 18000 # how many samples\n",
    "M, N, F, J = 6, 64, 66, 6\n",
    "NF = N*F\n",
    "eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n",
    "opts = {}\n",
    "opts['n_ch'] = [1,1]  \n",
    "opts['batch_size'] = 64\n",
    "opts['EM_iter'] = 201\n",
    "opts['lr'] = 0.01\n",
    "opts['n_epochs'] = 5\n",
    "opts['d_gamma'] = 8 \n",
    "n = 5  # for stopping \n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM6FT64_data4.pt').to(torch.cdouble)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n",
    "loss_iter, loss_tr = [], []\n",
    "model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "model = torch.load('../data/nem_ss/models/n19/model_ridn19_67.pt')\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "\"initial\"\n",
    "xtr = xtr[:I]\n",
    "Htr = torch.load('../data/nem_ss/tr18kHCI10M6_data4.pt').to(torch.cdouble)[:I]\n",
    "Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n",
    "gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma']) # shape of [J,1,8,8], cpu()\n",
    "\n",
    "#%%\n",
    "for epoch in range(opts['n_epochs']):  \n",
    "    xtr, Rbtr, gtr, Htr = myshuffle((xtr, Rbtr, gtr, Htr))  \n",
    "    res = xtr, Rbtr, gtr, Htr\n",
    "    data = Data.TensorDataset(*res)\n",
    "    tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n",
    "    for i, (x, Rb, g, Hhat) in enumerate(tr): # gamma [n_batch, 4, 4]\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        model.eval()\n",
    "\n",
    "        \"EM part\"\n",
    "        Rb, g, Hhat = Rb.cuda(), g.cuda(), Hhat.cuda()\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(g[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n",
    "\n",
    "        x = x.cuda()\n",
    "        g.requires_grad_()\n",
    "        optim_gamma = torch.optim.RAdam([g],\n",
    "                lr= glr,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n",
    "        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n",
    "        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n",
    "        ll_traj = []\n",
    "\n",
    "        for ii in range(opts['EM_iter']):\n",
    "            \"E-step\"\n",
    "            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n",
    "            shat = W.permute(2,0,1,3,4) @ x[...,None]\n",
    "            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n",
    "            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n",
    "            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n",
    "\n",
    "            \"M-step\"\n",
    "            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n",
    "            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n",
    "                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n",
    "            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n",
    "            Rb.imag = Rb.imag - Rb.imag\n",
    "\n",
    "            outs = []\n",
    "            for j in range(J):\n",
    "                outs.append(model(g[:,j]))\n",
    "            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "            vhat.real = threshold(out)\n",
    "            loss = loss_func(vhat, Rsshatnf.cuda())\n",
    "            optim_gamma.zero_grad()   \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n",
    "            optim_gamma.step()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            \"compute log-likelyhood\"\n",
    "            vhat = vhat.detach()\n",
    "            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n",
    "            ll_traj.append(ll.item())\n",
    "            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n",
    "            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n",
    "                # print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print(f'batch {i} is done. ', 'Current time--', datetime.now())\n",
    "    \n",
    "        if i == 0 :\n",
    "            plt.figure()\n",
    "            plt.plot(ll_traj, '-x')\n",
    "            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n",
    "\n",
    "            for ii in range(J):\n",
    "                plt.figure()\n",
    "                plt.imshow(vhat[0,...,ii].real.cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'source of v{ii} in 1st sample from the 1st batch at epoch {epoch}')\n",
    "                plt.savefig(fig_loc + f'id{rid}_vj{ii}_epoch{epoch}')      \n",
    "\n",
    "        \"update variable\"\n",
    "        with torch.no_grad():\n",
    "            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n",
    "            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n",
    "            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n",
    "        g.requires_grad_(False)\n",
    "        model.train()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(g[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat.real = threshold(out)\n",
    "        optimizer.zero_grad()         \n",
    "        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n",
    "        loss = -ll\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        loss_iter.append(loss.detach().cpu().item())\n",
    "\n",
    "    print(f'done with epoch{epoch}')\n",
    "    plt.figure()\n",
    "    plt.plot(loss_iter, '-xr')\n",
    "    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n",
    "    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n",
    "\n",
    "    loss_tr.append(loss.detach().cpu().item())\n",
    "    plt.figure()\n",
    "    plt.plot(loss_tr, '-or')\n",
    "    plt.title(f'Loss fuction at epoch{epoch}')\n",
    "    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n",
    "\n",
    "    plt.close('all')  # to avoid warnings\n",
    "    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n",
    "    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n",
    "    torch.save(Htr, mod_loc +f'Htr_rid{rid}_{epoch}.pt')    \n",
    "\n",
    "    # if epoch >10 :\n",
    "    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n",
    "    #     if s1 - s2 < 0 :\n",
    "    #         print('break-1')\n",
    "    #         break\n",
    "    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n",
    "    #     if abs((s1-s2)/s1) < 5e-4 :\n",
    "    #         print('break-2')\n",
    "    #         break\n",
    "print('done')\n",
    "print('starting date time ', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%@title 'n24'\n",
    "from utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.set_printoptions(linewidth=160)\n",
    "from datetime import datetime\n",
    "print('starting date time ', datetime.now())\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#%%\n",
    "rid = 'n24' # running id\n",
    "fig_loc = '../data/nem_ss/figures/'\n",
    "mod_loc = '../data/nem_ss/models/'\n",
    "if not(os.path.isdir(fig_loc + f'/{rid}/')): \n",
    "    print('made a new folder')\n",
    "    os.mkdir(fig_loc + f'{rid}/')\n",
    "    os.mkdir(mod_loc + f'{rid}/')\n",
    "fig_loc = fig_loc + f'{rid}/'\n",
    "mod_loc = mod_loc + f'{rid}/'\n",
    "\n",
    "from unet.unet_model import *\n",
    "class UNetHalf(nn.Module):\n",
    "    \"16 layers here\"\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        \"\"\"Only the up part of the unet\n",
    "        Args:\n",
    "            n_channels ([type]): [how many input channels=n_sources]\n",
    "            n_classes ([type]): [how many output classes=n_sources]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_ch = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.n_ch = int(128*1.5)\n",
    "\n",
    "        self.up = nn.Sequential(DoubleConv(n_channels, self.n_ch),\n",
    "                    MyUp(self.n_ch, self.n_ch//2),\n",
    "                    MyUp(self.n_ch//2, self.n_ch//4),\n",
    "                    MyUp(self.n_ch//4, self.n_ch//4))\n",
    "        self.reshape = nn.Sequential(\n",
    "                    nn.Conv2d(self.n_ch//4, self.n_ch//4, kernel_size=3, padding=(1,2)),\n",
    "                    nn.BatchNorm2d(self.n_ch//4),\n",
    "                    nn.LeakyReLU(inplace=True),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4),\n",
    "                    DoubleConv(self.n_ch//4, self.n_ch//4))\n",
    "        self.outc = OutConv(self.n_ch//4, n_classes)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.reshape(x) \n",
    "        x = self.outc(x)\n",
    "        x = self.sig(x)\n",
    "        out = x/x.detach().amax(keepdim=True, dim=(-1,-2))\n",
    "        return out\n",
    "\n",
    "I = 1000 # how many samples\n",
    "M, N, F, J = 6, 64, 66, 6\n",
    "NF = N*F\n",
    "eps, delta, glr = 5e-4, 1, 0.01 # delta is scale for Rb, glr is gamma learning rate\n",
    "opts = {}\n",
    "opts['n_ch'] = [1,1]  \n",
    "opts['batch_size'] = 64\n",
    "opts['EM_iter'] = 201\n",
    "opts['lr'] = 0.01\n",
    "opts['n_epochs'] = 201\n",
    "opts['d_gamma'] = 8 \n",
    "n = 5  # for stopping \n",
    "\n",
    "d = torch.load('../data/nem_ss/tr18kM6FT64_data4.pt').to(torch.cdouble)\n",
    "xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n",
    "loss_iter, loss_tr = [], []\n",
    "model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n",
    "for w in model.parameters():\n",
    "    nn.init.normal_(w, mean=0., std=0.01)\n",
    "# model = torch.load('../data/nem_ss/models/n8/model_ridn8_30.pt')\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(),\n",
    "                lr= opts['lr'],\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "\n",
    "\"initial\"\n",
    "xtr = xtr[:I]\n",
    "Htr = torch.load('../data/nem_ss/tr18kHCI10M6_data4.pt').to(torch.cdouble)[:I]\n",
    "Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*delta\n",
    "gtr = torch.rand(I,J,1,opts['d_gamma'], opts['d_gamma']) # shape of [J,1,8,8], cpu()\n",
    "\n",
    "#%%\n",
    "for epoch in range(opts['n_epochs']):  \n",
    "    xtr, Rbtr, gtr, Htr = myshuffle((xtr, Rbtr, gtr, Htr))  \n",
    "    res = xtr, Rbtr, gtr, Htr\n",
    "    data = Data.TensorDataset(*res)\n",
    "    tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n",
    "    for i, (x, Rb, g, Hhat) in enumerate(tr): # gamma [n_batch, 4, 4]\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        model.eval()\n",
    "\n",
    "        \"EM part\"\n",
    "        Rb, g, Hhat = Rb.cuda(), g.cuda(), Hhat.cuda()\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(g[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat = out.to(torch.cdouble)  # shape of [I,N,F,J]\n",
    "\n",
    "        x = x.cuda()\n",
    "        g.requires_grad_()\n",
    "        optim_gamma = torch.optim.RAdam([g],\n",
    "                lr= glr,\n",
    "                betas=(0.9, 0.999), \n",
    "                eps=1e-8,\n",
    "                weight_decay=0)\n",
    "        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n",
    "        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n",
    "        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n",
    "        ll_traj = []\n",
    "\n",
    "        for ii in range(opts['EM_iter']):\n",
    "            \"E-step\"\n",
    "            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n",
    "            shat = W.permute(2,0,1,3,4) @ x[...,None]\n",
    "            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n",
    "            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n",
    "            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n",
    "\n",
    "            \"M-step\"\n",
    "            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n",
    "            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n",
    "                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n",
    "            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n",
    "            Rb.imag = Rb.imag - Rb.imag\n",
    "\n",
    "            outs = []\n",
    "            for j in range(J):\n",
    "                outs.append(model(g[:,j]))\n",
    "            out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "            vhat.real = threshold(out)\n",
    "            loss = loss_func(vhat, Rsshatnf.cuda())\n",
    "            optim_gamma.zero_grad()   \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_([g], max_norm=10)\n",
    "            optim_gamma.step()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            \"compute log-likelyhood\"\n",
    "            vhat = vhat.detach()\n",
    "            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n",
    "            ll_traj.append(ll.item())\n",
    "            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n",
    "            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n",
    "                # print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print(f'batch {i} is done. ', 'Current time--', datetime.now())\n",
    "    \n",
    "        if i == 0 :\n",
    "            plt.figure()\n",
    "            plt.plot(ll_traj, '-x')\n",
    "            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n",
    "            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n",
    "\n",
    "            for ii in range(J):\n",
    "                plt.figure()\n",
    "                plt.imshow(vhat[0,...,ii].real.cpu())\n",
    "                plt.colorbar()\n",
    "                plt.title(f'source of v{ii} in 1st sample from the 1st batch at epoch {epoch}')\n",
    "                plt.savefig(fig_loc + f'id{rid}_vj{ii}_epoch{epoch}')      \n",
    "\n",
    "        \"update variable\"\n",
    "        with torch.no_grad():\n",
    "            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n",
    "            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n",
    "            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n",
    "        g.requires_grad_(False)\n",
    "        model.train()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "        outs = []\n",
    "        for j in range(J):\n",
    "            outs.append(model(g[:,j]))\n",
    "        out = torch.cat(outs, dim=1).permute(0,2,3,1).to(torch.double)\n",
    "        vhat.real = threshold(out)\n",
    "        optimizer.zero_grad()         \n",
    "        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n",
    "        loss = -ll\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        loss_iter.append(loss.detach().cpu().item())\n",
    "\n",
    "    print(f'done with epoch{epoch}')\n",
    "    plt.figure()\n",
    "    plt.plot(loss_iter, '-xr')\n",
    "    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n",
    "    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n",
    "\n",
    "    loss_tr.append(loss.detach().cpu().item())\n",
    "    plt.figure()\n",
    "    plt.plot(loss_tr, '-or')\n",
    "    plt.title(f'Loss fuction at epoch{epoch}')\n",
    "    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n",
    "\n",
    "    plt.close('all')  # to avoid warnings\n",
    "    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n",
    "    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n",
    "    torch.save(Htr, mod_loc +f'Htr_rid{rid}_{epoch}.pt')    \n",
    "\n",
    "    # if epoch >10 :\n",
    "    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n",
    "    #     if s1 - s2 < 0 :\n",
    "    #         print('break-1')\n",
    "    #         break\n",
    "    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n",
    "    #     if abs((s1-s2)/s1) < 5e-4 :\n",
    "    #         print('break-2')\n",
    "    #         break\n",
    "print('done')\n",
    "print('starting date time ', datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c728c5ad72b5fd3c6e083c4badca00ca04470578383c0e9d983163c40aa43e1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
