{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[]nem_realdata_dynamic_record.ipynb","provenance":[{"file_id":"1rEeDE0AIi8lEjzIdIoiysB5PVijO7f1Z","timestamp":1627578797711},{"file_id":"1sL0NuzGnbFzk2Y5XY_n4woZ0c4lJBoEy","timestamp":1627388962384},{"file_id":"1QdweiBwMNieeGBdDFCzphgwJoB7PW0Xo","timestamp":1619782442433},{"file_id":"1W6TyLRUPhBNNN7bLUJ2u4rX4K7CBEjCU","timestamp":1619749565297},{"file_id":"1N5hj6oGZ8XxOExPPtxDCML9bchrleb2O","timestamp":1619749345089},{"file_id":"1vI54wk15P3rTcruo_pRVWX2GldhPqU1D","timestamp":1619744297962},{"file_id":"1HyayE3FINrKpEPXQ5hCl3yQ6S_gaJRF7","timestamp":1619187752515},{"file_id":"137mHbM4Vq4YtcOF9la2sOul4aNWZwRzn","timestamp":1619037563775}],"collapsed_sections":["NVnL-nCeBA3t","3jW2Qw0aymjk","uPI2xhUkPW6M","BqCh4vrkSS2C","w_il05ERz3u0","hyhzT1CIcERI","RsvZxNctBe2D"],"authorship_tag":"ABX9TyPpmgFYLsgSaMlipMa8R5/N"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"NVnL-nCeBA3t"},"source":["## Real data running history\n","3 neural networks for 3 components"]},{"cell_type":"code","metadata":{"id":"wtuWghQF4NBH","cellView":"form"},"source":["#@title rid=5000 the same initialization, warm start,warm shared Hhat, lr_gamma=0.01, 3000tr samples, trim=1\n","\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 5000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.stack([gtr[:,None] for j in range(J)], dim=1)\n","\n","model, optimizer = {}, {}\n","loss_iter, loss_tr = [], []\n","for j in range(J):\n","    model[j] = UNetHalf(opts['n_ch'], 1).cuda()\n","    optimizer[j] = optim.RAdam(model[j].parameters(),\n","                    lr= opts['lr'],\n","                    betas=(0.9, 0.999),\n","                    eps=1e-8,\n","                    weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for j in range(J):\n","        for param in model[j].parameters():\n","            param.requires_grad_(False)\n","        model[j].eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            # Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","            for j in range(J):\n","                out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update neural network\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","        for j in range(J):\n","            model[j].train()\n","            for param in model[j].parameters():\n","                param.requires_grad_(True)\n","            out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            optimizer[j].zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        for j in range(J):\n","            torch.nn.utils.clip_grad_norm_(model[j].parameters(), max_norm=1)\n","            optimizer[j].step()\n","            torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"nrIREPPmhvbC"},"source":["#@title rid=5100 the same initialization, cold start,warm shared Hhat, lr_gamma=0.01, 3000tr samples, trim=1\n","\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 5100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.stack([gtr[:,None] for j in range(J)], dim=1)\n","\n","model, optimizer = {}, {}\n","loss_iter, loss_tr = [], []\n","for j in range(J):\n","    model[j] = UNetHalf(opts['n_ch'], 1).cuda()\n","    optimizer[j] = optim.RAdam(model[j].parameters(),\n","                    lr= opts['lr'],\n","                    betas=(0.9, 0.999),\n","                    eps=1e-8,\n","                    weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for j in range(J):\n","        for param in model[j].parameters():\n","            param.requires_grad_(False)\n","        model[j].eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            # Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","            for j in range(J):\n","                out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        # #%% update neural network\n","        # with torch.no_grad():\n","        #     gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","        #     vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","        #     Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","        for j in range(J):\n","            model[j].train()\n","            for param in model[j].parameters():\n","                param.requires_grad_(True)\n","            out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            optimizer[j].zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        for j in range(J):\n","            torch.nn.utils.clip_grad_norm_(model[j].parameters(), max_norm=1)\n","            optimizer[j].step()\n","            torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EJQuTsL0TNfU","cellView":"form"},"source":["#@title rid=5200 the same initialization, cold start, cold not shared Hhat(due to mistak, it is cold shared Hhat)\n","\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 5200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.stack([gtr[:,None] for j in range(J)], dim=1)\n","\n","model, optimizer = {}, {}\n","loss_iter, loss_tr = [], []\n","for j in range(J):\n","    model[j] = UNetHalf(opts['n_ch'], 1).cuda()\n","    optimizer[j] = optim.RAdam(model[j].parameters(),\n","                    lr= opts['lr'],\n","                    betas=(0.9, 0.999),\n","                    eps=1e-8,\n","                    weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for j in range(J):\n","        for param in model[j].parameters():\n","            param.requires_grad_(False)\n","        model[j].eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            # Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","            for j in range(J):\n","                out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        # #%% update neural network\n","        # with torch.no_grad():\n","        #     gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","        #     vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","        #     Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","        #     Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","        for j in range(J):\n","            model[j].train()\n","            for param in model[j].parameters():\n","                param.requires_grad_(True)\n","            out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            optimizer[j].zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        for j in range(J):\n","            torch.nn.utils.clip_grad_norm_(model[j].parameters(), max_norm=1)\n","            optimizer[j].step()\n","            torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"MyCXm7Y5gEFk"},"source":["#@title rid=5201 the same initialization, cold start, cold not shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 5201 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.stack([gtr[:,None] for j in range(J)], dim=1)\n","\n","model, optimizer = {}, {}\n","loss_iter, loss_tr = [], []\n","for j in range(J):\n","    model[j] = UNetHalf(opts['n_ch'], 1).cuda()\n","    optimizer[j] = optim.RAdam(model[j].parameters(),\n","                    lr= opts['lr'],\n","                    betas=(0.9, 0.999),\n","                    eps=1e-8,\n","                    weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for j in range(J):\n","        for param in model[j].parameters():\n","            param.requires_grad_(False)\n","        model[j].eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","            for j in range(J):\n","                out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        # #%% update neural network\n","        # with torch.no_grad():\n","        #     gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","        #     vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","        #     Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","        #     Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","        for j in range(J):\n","            model[j].train()\n","            for param in model[j].parameters():\n","                param.requires_grad_(True)\n","            out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            optimizer[j].zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        for j in range(J):\n","            torch.nn.utils.clip_grad_norm_(model[j].parameters(), max_norm=1)\n","            optimizer[j].step()\n","            torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bMDmPtpF_2m0","cellView":"form"},"source":["#@title rid=5300 the same initialization, cold start,cold shared Hhat, lr_gamma=0.01, 3000tr samples, trim=1\n","\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 5300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.stack([gtr[:,None] for j in range(J)], dim=1)\n","\n","model, optimizer = {}, {}\n","loss_iter, loss_tr = [], []\n","for j in range(J):\n","    model[j] = UNetHalf(opts['n_ch'], 1).cuda()\n","    optimizer[j] = optim.RAdam(model[j].parameters(),\n","                    lr= opts['lr'],\n","                    betas=(0.9, 0.999),\n","                    eps=1e-8,\n","                    weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for j in range(J):\n","        for param in model[j].parameters():\n","            param.requires_grad_(False)\n","        model[j].eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr.cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            # Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","            for j in range(J):\n","                out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        # #%% update neural network\n","        # with torch.no_grad():\n","        #     gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","        #     vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","        #     Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","        for j in range(J):\n","            model[j].train()\n","            for param in model[j].parameters():\n","                param.requires_grad_(True)\n","            out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            optimizer[j].zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        for j in range(J):\n","            torch.nn.utils.clip_grad_norm_(model[j].parameters(), max_norm=1)\n","            optimizer[j].step()\n","            torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"az3WS8VeJ3ks","cellView":"form"},"source":["#@title rid=5400 warm start,warm not shared Hhat(due to mistak, it is warm not shared Hhat)\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 5400 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.stack([gtr[:,None] for j in range(J)], dim=1)\n","\n","model, optimizer = {}, {}\n","loss_iter, loss_tr = [], []\n","for j in range(J):\n","    model[j] = UNetHalf(opts['n_ch'], 1).cuda()\n","    optimizer[j] = optim.RAdam(model[j].parameters(),\n","                    lr= opts['lr'],\n","                    betas=(0.9, 0.999),\n","                    eps=1e-8,\n","                    weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for j in range(J):\n","        for param in model[j].parameters():\n","            param.requires_grad_(False)\n","        model[j].eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            # Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","            for j in range(J):\n","                out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update neural network\n","        with torch.no_grad():\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","        for j in range(J):\n","            model[j].train()\n","            for param in model[j].parameters():\n","                param.requires_grad_(True)\n","            out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            optimizer[j].zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        for j in range(J):\n","            torch.nn.utils.clip_grad_norm_(model[j].parameters(), max_norm=1)\n","            optimizer[j].step()\n","            torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"ezDYPNg_VZV5"},"source":["#@title rid=5401 the same initialization, warm start,warm not shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 5401 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.stack([gtr[:,None] for j in range(J)], dim=1)\n","\n","model, optimizer = {}, {}\n","loss_iter, loss_tr = [], []\n","for j in range(J):\n","    model[j] = UNetHalf(opts['n_ch'], 1).cuda()\n","    optimizer[j] = optim.RAdam(model[j].parameters(),\n","                    lr= opts['lr'],\n","                    betas=(0.9, 0.999),\n","                    eps=1e-8,\n","                    weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for j in range(J):\n","        for param in model[j].parameters():\n","            param.requires_grad_(False)\n","        model[j].eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","            for j in range(J):\n","                out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update neural network\n","        with torch.no_grad():\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        out = torch.randn(opts['batch_size'], N, F, J, device='cuda', dtype=torch.double)\n","        for j in range(J):\n","            model[j].train()\n","            for param in model[j].parameters():\n","                param.requires_grad_(True)\n","            out[..., j] = torch.sigmoid(model[j](g[:,j]).squeeze())\n","            optimizer[j].zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        for j in range(J):\n","            torch.nn.utils.clip_grad_norm_(model[j].parameters(), max_norm=1)\n","            optimizer[j].step()\n","            torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3jW2Qw0aymjk"},"source":["## single model\n","11- means one neural network with 3 channels ; 12- means one neural network with 1 channel\n","\n","The best result is 125240, similar one is 125243"]},{"cell_type":"code","metadata":{"id":"_xrsikAz2IB0","cellView":"form"},"source":["#@title rid=115200 cold start, cold not shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 115200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 3  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1) # shape of \n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.sigmoid(model(g)).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        # #%% update variable\n","        # with torch.no_grad():\n","        #     gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","        #     vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","        #     Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","        #     Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","        out = torch.sigmoid(model(g)).permute(0,2,3,1)\n","        optimizer.zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"87IWUIeX15DD","cellView":"form"},"source":["#@title rid=115300 cold start, cold shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 115300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 3  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1) # shape of \n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            # Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            out = torch.sigmoid(model(g)).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        # #%% update variable\n","        # with torch.no_grad():\n","        #     gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","        #     vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","        #     Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","        #     Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","        out = torch.sigmoid(model(g)).permute(0,2,3,1)\n","        optimizer.zero_grad() \n","        vhat.real = threshold(out)\n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rwz9o349M-I8"},"source":["#@title rid=125000 cold start, cold not shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        # #%% update variable\n","        # with torch.no_grad():\n","        #     gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","        #     vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","        #     Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","        #     Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WkjG2xQJlRYQ","cellView":"form"},"source":["#@title rid=125100 warm start, warm not shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UYMou3qsDCPe","cellView":"form"},"source":["#@title rid=125110 warm start, warm not shared Hhat, awgn20db\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=20, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"moLXl2evJCQh"},"source":["#@title rid125111 warm start, warm not shared Hhat, awgn20db, 100iter\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125111 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 101\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=20, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"abn0pzz4ErlA","cellView":"form"},"source":["#@title rid=125120 warm start, warm not shared Hhat, awgn15db\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125120 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"BmBJFFqKJTgC"},"source":["#@title rid125121 warm start, warm not shared Hhat, awgn15db, 100 iter\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125121 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 101\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dhta5U5WJ_ra","cellView":"form"},"source":["#@title rid125130 warm start, warm not shared Hhat, awgn10db, 100iter\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125130 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 101\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ouc0IlPeQRU0","cellView":"form"},"source":["#@title rid=125200 warm start, warm shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","\n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xtUNj7E1dtsj","cellView":"form"},"source":["#@title rid=125210 warm start, warm shared Hhat, added_noise*3\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125210 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.randn(J, 1, opts['d_gamma'], opts['d_gamma'])*3\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XLvtfOfBdxDm","cellView":"form"},"source":["#@title rid=125220 warm start, warm shared Hhat, added_noise\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125220 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.randn(J, 1, opts['d_gamma'], opts['d_gamma'])\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1O_oAwmId7vO","cellView":"form"},"source":["#@title rid=125230 warm start, warm shared Hhat, awgn_snr10\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125230 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iW5STi0u4ceR","cellView":"form"},"source":["#@title rid=125240 code is missing"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"29PDQV7EALnK","cellView":"form"},"source":["#@title rid=125241 warm start, warm shared Hhat, awgn_snr0\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125241 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=0, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"YoUhXuzGAR94"},"source":["#@title rid=125242 warm start, warm shared Hhat, awgn_snr5\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125242 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=5, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pC5d4B6SAXpb","cellView":"form"},"source":["#@title rid=125243 warm start, warm shared Hhat, awgn_snr15\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125243 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gCc7S-8KAg6U","cellView":"form"},"source":["#@title rid=125244 warm start, warm shared Hhat, awgn_snr20\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125244 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=20, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9cjHdElWwbts","cellView":"form"},"source":["#@title rid=125250 warm start, warm shared Hhat, awgn_snr10, gamma learning rate 0.01\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125250 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.01)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rkAAOE84jG8Y","cellView":"form"},"source":["#@title rid=125260 warm start, warm shared Hhat,awgn_snr10, gamma learning rate 0.01, changed model rate 0.005, em eps as 5e-4\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125260 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.005\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.01)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 8 and abs((ll_traj[ii] - ll_traj[ii-5])/ll_traj[ii-5]) <5e-4:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ahzy5wWXYsE5","cellView":"form"},"source":["#@title rid=125265 warm start, warm shared Hhat,awgn_snr10, EM iter 201, em eps as 5e-4\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125265 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 8 and abs((ll_traj[ii] - ll_traj[ii-5])/ll_traj[ii-5]) <5e-4:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"4oTLEz-A13-V"},"source":["#@title rid=125270 warm start, warm shared Hhat,awgn_snr10, gamma learning rate 0.005\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 125270 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 150\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.005)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3]) <1e-3:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uPI2xhUkPW6M"},"source":["## 13 series\n","1 channel, eps is 5e-4, EM iter is 201, overall iter is 71\n","\n","139- loading previous model as the starting point"]},{"cell_type":"code","metadata":{"id":"D2zaI_O7Pmuw","cellView":"form"},"source":["#@title rid130000 setting as rid=125243 warm start, warm shared Hhat, awgn_snr15\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 130000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mnvvt9uNm_c0","cellView":"form"},"source":["#@title rid130001 setting as rid=125243 warm start, warm shared Hhat, awgn_snr15\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 130001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_SAOIZSSe2Gh","cellView":"form"},"source":["#@title rid132000  16to100, warm start, warm shared Hhat, awgn_snr15\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf16to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 132000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 16 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"U05rQhMfswma"},"source":["#@title rid132100 , 16to100, warm start, warm shared Hhat, awgn_snr20\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf16to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 132100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 16 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=20, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"p5jDiXNLs5Q-"},"source":["#@title rid132200 , 16to100, warm start, warm shared Hhat, awgn_snr10\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf16to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 132200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 16 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"KS5r4s-Ds-Uc"},"source":["#@title rid132300 , 16to100, warm start, warm shared Hhat, awgn_snr5\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf16to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 132300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 16 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=5, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"652DnBhwtExs"},"source":["#@title rid133000 , stack half unet, warm start, warm shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_stack\n","torch.manual_seed(1)\n","\n","rid = 133000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.randn(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","from skimage.transform import resize\n","xx_all = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))[:,None]\n","loss_iter, loss_tr = [], []\n","model = UNetHalf8to100_stack(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        xx = xx_all[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"APOMrT69rGLs"},"source":["#@title rid133010 , stack half unet, warm start, warm shared Hhat, 256 inner channels\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_stack_256 as HUnet\n","torch.manual_seed(1)\n","\n","rid = 133010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.randn(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","from skimage.transform import resize\n","xx_all = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))[:,None]\n","loss_iter, loss_tr = [], []\n","model = HUnet(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        xx = xx_all[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o3KUa7E3zFi3","cellView":"form"},"source":["#@title rid133100 , stack half unet structure2, warm start, warm shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_stack2\n","torch.manual_seed(1)\n","\n","rid = 133100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.randn(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","from skimage.transform import resize\n","xx_all = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))[:,None]\n","loss_iter, loss_tr = [], []\n","model = UNetHalf8to100_stack2(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        xx = xx_all[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RARDxOuvrTQP","cellView":"form"},"source":["#@title rid133110 , stack half unet structure2, warm start, warm shared Hhat, 256 inner channels\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_stack2_256 as HUnet\n","torch.manual_seed(1)\n","\n","rid = 133110 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.randn(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","from skimage.transform import resize\n","xx_all = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))[:,None]\n","loss_iter, loss_tr = [], []\n","model = HUnet(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        xx = xx_all[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((xx, g[:,j]), dim=-2))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HwL3UT1JOC5n","cellView":"form"},"source":["#@title rid133200, interpolate, half unet, warm start, warm shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_stack3\n","torch.manual_seed(1)\n","\n","rid = 133200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.randn(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","from skimage.transform import resize\n","xx_all = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))[:,None, None]\n","loss_iter, loss_tr = [], []\n","model = UNetHalf8to100_stack3(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        inp = torch.rand(opts['batch_size'], J, 1, opts['d_gamma'],opts['d_gamma']*2).cuda()\n","        inp[...,0::2] = xx_all[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            inp = inp.detach()\n","            for j in range(J):\n","                inp[:,j,...,1::2] = g[:, j]\n","                outs.append(torch.sigmoid(model(inp[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        inp = inp.detach()\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(inp[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"WJWsiQBXM_jx"},"source":["#@title rid133201 , interpolate, half unet, warm start, warm shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_stack3\n","torch.manual_seed(1)\n","\n","rid = 133201 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.randn(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","from skimage.transform import resize\n","# xx_all = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    # order=1, preserve_range=True ))[:,None, None]\n","xx_all = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","loss_iter, loss_tr = [], []\n","model = UNetHalf8to100_stack3(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        inp = torch.rand(opts['batch_size'], J, 1, opts['d_gamma'],opts['d_gamma']*2).cuda()\n","        inp[...,0::2] = xx_all[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            inp = inp.detach()\n","            for j in range(J):\n","                inp[:,j,...,1::2] = g[:, j]\n","                outs.append(torch.sigmoid(model(inp[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        inp = inp.detach()\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(inp[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7TpSr7fzTxnf","cellView":"form"},"source":["#@title rid133210 ,interpolate, half unet, warm start, warm shared Hhat, 256 inner channels\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_stack3_256 as HUnet\n","torch.manual_seed(1)\n","\n","rid = 133210 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.randn(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","from skimage.transform import resize\n","# xx_all = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    # order=1, preserve_range=True ))[:,None, None]\n","xx_all = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","loss_iter, loss_tr = [], []\n","model = HUnet(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        inp = torch.rand(opts['batch_size'], J, 1, opts['d_gamma'],opts['d_gamma']*2).cuda()\n","        inp[...,0::2] = xx_all[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            inp = inp.detach()\n","            for j in range(J):\n","                inp[:,j,...,1::2] = g[:, j]\n","                outs.append(torch.sigmoid(model(inp[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        inp = inp.detach()\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(inp[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"Fq4z20Nmcc8U"},"source":["#@title rid135000 warm start, warm shared Hhat, awgn_snr15, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 130000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XFb-ZHCGSm5r","cellView":"form"},"source":["#@title rid135100 warm start, warm shared Hhat, awgn_snr10, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 135100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"5YSTb70VFQ7r"},"source":["#@title rid135110 warm start, warm shared Hhat, awgn_snr10, 2 more layers, 256 inner channels\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 135110 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"WqL4MAdjMzHk"},"source":["#@title rid135111 warm start, warm shared Hhat, awgn_snr10, 2 more layers, 256 inner channels\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 135111 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=10, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"hDINoHcjNJhs"},"source":["#@title rid135120 warm start, warm shared Hhat, awgn_snr15, 2 more layers, 256 inner channels\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 135120 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"lEpdOxt7NMtP"},"source":["#@title rid135130 warm start, warm shared Hhat, awgn_snr5, 2 more layers, 256 inner channels\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 135130 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=5, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"mmhjldaOSuC-"},"source":["#@title rid135200 warm start, warm shared Hhat, awgn_snr5, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 135200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=5, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"yZyYOEEzSxDj"},"source":["#@title rid135300 warm start, warm shared Hhat, awgn_snr20, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 135300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=20, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1tsojMUvTIbz","cellView":"form"},"source":["#@title rid136000  partially covered with noise, awgn_snr10, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 136000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 10\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","#%%\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qMxTfNpFTOMe","cellView":"form"},"source":["\n","#@title rid136100 awgn_snr10, gtr partially covered with noise\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 136100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 10\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","#%%\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"0c--DxdzzDZC"},"source":["#@title rid136200 partially covered with noise, awgn_snr15, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 136200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 15\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"YHcUJXhKzEy8"},"source":["#@title rid136300 partially covered with noise, awgn_snr5, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 136300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 5\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"bPgsZ8HczJpy"},"source":["#@title rid136400 partially covered with noise, awgn_snr20, 2 more layers\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 136400 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 20\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cx7EOtCXBzuE","cellView":"form"},"source":["#@title rid137000  partially covered with noise, awgn_snr10, 19 layers (5 more)\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_19 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 137000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 10\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","#%%\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"9Jpo11dtooxs"},"source":["#@title rid137100  partially covered with noise, awgn_snr15, 19 layers (5 more)\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_19 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 137100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 15\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","#%%\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"_H9CY9x1oqLz"},"source":["#@title rid137200  partially covered with noise, awgn_snr5, 19 layers (5 more)\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_19 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 137200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","snr = 5\n","cover = 4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","#%%\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.zeros(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    temp = awgn(gtr[0,j,0], snr=snr, seed=j) - gtr[0,j,0] \n","    added_noise[j,0,0+j*2:cover+2*j, 0+j*2:cover+2*j] = temp[0+j*2:cover+2*j, 0+j*2:cover+2*j]\n","gtr = gtr + added_noise\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FfEDIViYSq6B","cellView":"form"},"source":["#@title rid139000 warm start, warm shared Hhat, awgn_snr15, loaded125240\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 139000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","# model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","model = torch.load('../data/nem_ss/models/model_rid125240.pt')\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","Hhat = torch.load('../data/nem_ss/models/Hhat_rid125240.pt')\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fd8iT5ThUa3c","cellView":"form"},"source":["#@title rid139100 warm start, warm shared Hhat, awgn_snr15, loaded125240, g step 5e-3\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 139100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","# model = UNetHalf(opts['n_ch'], opts['n_ch']).cuda()\n","model = torch.load('../data/nem_ss/models/model_rid125240.pt')\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","added_noise = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    added_noise[j,0] = awgn(gtr[0,j,0], snr=15, seed=j) - gtr[0,j,0] \n","gtr = gtr + added_noise\n","Hhat = torch.load('../data/nem_ss/models/Hhat_rid125240.pt')\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.005)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BqCh4vrkSS2C"},"source":["## 14 series using new structures\n","140-2channel, $\\gamma$ as resized mixture; <br/> \n","141-2channel, $\\gamma$ as random noise; similar as 140<br/> \n","142-update Hhat individually; -- too slow cannot finish <br/> \n","143-update Hhat as one, which was I supposed to do before; -- similar result as before <br/>\n","144-added batchnorm before sigmoid/ normalize Hhat; -- too slow, barely finish<br/>\n"]},{"cell_type":"code","metadata":{"cellView":"form","id":"ma_I7AomJgtq"},"source":["#@title rid140000 , unet8to100 warm start, warm shared Hhat\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNet8to100\n","torch.manual_seed(1)\n","\n","rid = 140000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = 1  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).abs().repeat(I,1,1,1,1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNet8to100(opts['n_ch'], opts['n_ch']).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        xx = x[...,0].abs()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(xx[:,None], g[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(xx[:,None], g[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ikHO0O2CNz7T","cellView":"form"},"source":["#@title rid140100 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GG-lPXAF5hVO","cellView":"form"},"source":["#@title rid140110 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, 128 inner channel\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140110 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lx6KhQH83w8k","cellView":"form"},"source":["#@title rid140120 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140120 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"Th87MSoE36Os"},"source":["#@title rid140121 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside, save the temp results\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140121 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","        print('Hhat.norm()', Hhat.cpu().norm())\n","        print('Rb.norm.max and Rb.norm()', Rb.cpu().norm(dim=(-1,-2)).mean(), Rb.cpu().norm())\n","        print('v.max.mean for all sources, vhat.norm()',vhat.detach().cpu().real.amax(dim=(1,2)).mean(dim=0), vhat.detach().cpu().norm())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"8RytjcyaOsz9"},"source":["#@title rid140122 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside, save the temp results\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140122 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","        print('Hhat.norm()', Hhat.cpu().norm())\n","        print('Rb.norm.max and Rb.norm()', Rb.cpu().norm(dim=(-1,-2)).mean(), Rb.cpu().norm())\n","        print('v.max.mean for all sources, vhat.norm()',vhat.detach().cpu().real.amax(dim=(1,2)).mean(dim=0), vhat.detach().cpu().norm())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eN9QO3hx3YNX","cellView":"form"},"source":["#@title rid140130 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside and vj/vj.max\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140130 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"RtbJ-9McLQsW"},"source":["#@title rid140131 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside and vj/vj.max\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140131 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print('Hhat.norm()', Hhat.cpu().norm())\n","        print(' vhat.norm()', vhat.detach().cpu().norm())\n","    \n","        print(f'batch {i} is done \\n')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jc1kRTNzPu2Q","cellView":"form"},"source":["#@title rid140132 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside and vj/vj.max, another run, saving temp results\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140132 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print('Hhat.norm()', Hhat.cpu().norm())\n","        print(' vhat.norm()', vhat.detach().cpu().norm())\n","    \n","        print(f'batch {i} is done \\n')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fxTow9_6MAQm","cellView":"form"},"source":["#@title rid140133 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside and vj/vj.max, another run\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140133 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print('Hhat.norm()', Hhat.cpu().norm())\n","        print(' vhat.norm()', vhat.detach().cpu().norm())\n","    \n","        print(f'batch {i} is done \\n')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"0slb_cpCs7BF"},"source":["#@title rid140140 warm start, warm shared Hhat, 16 layers, 2 channel input, label as random noise, sigmoid inside and vj/vj.max, another run, changed step size 2times for the model -- did not run\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140140 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.002\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","        print('Hhat.norm()', Hhat.cpu().norm())\n","        print(' vhat.norm()', vhat.detach().cpu().norm())\n","    \n","        print(f'batch {i} is done \\n')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c0G5_8MN3IkS","cellView":"form"},"source":["#@title rid140200 warm start, warm shared Hhat, 16 layers, 2 channel input,  label as basis\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    lb[j,0] = basis[:, j]\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tVrFGAvp5n7T","cellView":"form"},"source":["#@title rid140210 warm start, warm shared Hhat, 16 layers, 2 channel input,  label as basis, 128 inner channel\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_morelayers as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140210 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    lb[j,0] = basis[:, j]\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb[:,j]), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RROdQz1k3I-z","cellView":"form"},"source":["#@title rid140300 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis with FC 9*8\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_16_FC as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]]).cuda()\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = g[:,j]\n","                ins[:,j, :,:,8] = basis[:,j]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = g[:,j]\n","            ins[:,j, :,:,8] = basis[:,j]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ie9k1XEz3N-F","cellView":"form"},"source":["#@title rid140301 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis with FC 9*8\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_16_FC as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140301 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]]).cuda()\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = g[:,j]\n","                ins[:,j, :,:,8] = basis[:,j]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = g[:,j]\n","            ins[:,j, :,:,8] = basis[:,j]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wuHiNaW_4zxx","cellView":"form"},"source":["#@title rid140310 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis with FC 9*8, 128 inner channel\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_16_FC_128 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140310 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]]).cuda()\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = g[:,j]\n","                ins[:,j, :,:,8] = basis[:,j]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = g[:,j]\n","            ins[:,j, :,:,8] = basis[:,j]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g3ZPWOJG1tbQ","cellView":"form"},"source":["#@title rid140400 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis using conv\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_stack1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140400 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]]).cuda()\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = g[:,j]\n","                ins[:,j, :,:,8] = basis[:,j]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = g[:,j]\n","            ins[:,j, :,:,8] = basis[:,j]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ypOyVhJoRpuU","cellView":"form"},"source":["#@title rid141100 warm start, warm shared Hhat, 16 layers, 2 channel input, gamma=label as random noise\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 141100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MuH2cQhrFbDU","cellView":"form"},"source":["#@title rid141101 warm start, warm shared Hhat, 16 layers, 2 channel input, gamma=label as random noise\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 141101 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 51\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e4Ou25wxo-5n","cellView":"form"},"source":["#@title rid141102 warm start, warm shared Hhat, 16 layers, 2 channel input, gamma=label as random noise -- just 11 epochs because stopping critierion is wrong\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 141102 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[epoch-5:epoch-2])/3, sum(loss_tr[epoch-2:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        if (s1-s2)/s1 < 5e-4 :\n","            print('break-2')\n","            break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"ywv0UoJ4q0x7"},"source":["#@title rid141103 warm start, warm shared Hhat, 16 layers, 2 channel input, gamma=label as random noise\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 141103 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')    \n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[epoch-5:epoch-2])/3, sum(loss_tr[epoch-2:])/3\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     if (s1-s2)/s1 < 5e-4 :\n","    #         print('break-2')\n","    #         break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mxvid5X_SDRf","cellView":"form"},"source":["#@title rid141200 warm start, warm shared Hhat, 16 layers, 2 channel input,  gamma=label as basis\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 141200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    gtr[j,0] = basis[:, j]\n","gtr = gtr.repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OMo_td8jVs-U","cellView":"form"},"source":["#@title rid141300 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis with FC 9*8\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_16_FC as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 141300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,1,opts['d_gamma'])\n","for j in range(J):\n","    gtr[j,0] = basis[:, j]\n","gtr = gtr.repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = lb0\n","                ins[:,j, :,:,8] = g[:,j,0]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = lb0\n","            ins[:,j, :,:,8] = g[:,j,0]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"PKxm-7e0V-FI"},"source":["#@title rid141301 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis with FC 9*8\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_16_FC as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 141301 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,1,opts['d_gamma'])\n","for j in range(J):\n","    gtr[j,0] = basis[:, j]\n","gtr = gtr.repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = lb0\n","                ins[:,j, :,:,8] = g[:,j,0]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = lb0\n","            ins[:,j, :,:,8] = g[:,j,0]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BJ1goPwDWKpu","cellView":"form"},"source":["#@title rid141400 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis using conv\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_stack1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 141400 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,1,opts['d_gamma'])\n","for j in range(J):\n","    gtr[j,0] = basis[:, j]\n","gtr = gtr.repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = lb0\n","                ins[:,j, :,:,8] = g[:,j,0]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = lb0\n","            ins[:,j, :,:,8] = g[:,j,0]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n2NZFUIsV_qb","cellView":"form"},"source":["#@title rid141401 warm start, warm shared Hhat, 16 layers, 2 channel input, stack basis using conv\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_stack1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 141401 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [1,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,1,opts['d_gamma'])\n","for j in range(J):\n","    gtr[j,0] = basis[:, j]\n","gtr = gtr.repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","            for j in range(J):\n","                ins[:,j, :,:,:8] = lb0\n","                ins[:,j, :,:,8] = g[:,j,0]\n","                outs.append(torch.sigmoid(model(ins[:,j])))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        ins = torch.rand(opts['batch_size'], J, 1, 8, 9).cuda()\n","        for j in range(J):\n","            ins[:,j, :,:,:8] = lb0\n","            ins[:,j, :,:,8] = g[:,j,0]\n","            outs.append(torch.sigmoid(model(ins[:,j])))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X3h1cXAkGrMM","cellView":"form"},"source":["#@title rid142000 based on rid140130, changed Hhat to seperate one -- could not finished within max time\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 142000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Htr, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AC7NiGgLGt-g","cellView":"form"},"source":["#@title rid142100 based on 140120, changed Hhat to seperate ones -- could not finished within max time\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 142100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Htr, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HuyTL5vv2kKN","cellView":"form"},"source":["#@title rid142200 based on rid141100, changed Hhat as Htr -- could not finished within max time\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 142200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Htr, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Z8y54ba2sAh","cellView":"form"},"source":["#@title rid 142300 based on rid141200 changed Hhat to Htr -- could not finished within max time\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256 as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 142300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","\n","basis = torch.tensor([[ 0.4840, -0.4543,  0.2743],\n","                        [-0.4399,  0.0608,  0.6416],\n","                        [-0.5242, -0.5769, -0.4785],\n","                        [-0.1857,  0.1105,  0.1568],\n","                        [ 0.2695,  0.4053, -0.1714],\n","                        [-0.1912,  0.4966, -0.3817],\n","                        [ 0.2268,  0.0365, -0.0287],\n","                        [ 0.3199, -0.1808, -0.2892]])\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma'])\n","for j in range(J):\n","    gtr[j,0] = basis[:, j]\n","gtr = gtr.repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Htr, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"rDhkyNLtOpCz"},"source":["#@title rid143000 based on 140120, changed Hhat to M*J\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 143000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Hhat = Hhat.mean(0)\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"rVLqLif-hI5g"},"source":["#@title rid143100 based on 140120, changed Hhat to M*J before NN\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 143100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Hhat = Hhat.mean(0)\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KBGUPf3khL5e","cellView":"form"},"source":["#@title rid144000 similar to r140120 with batch norm before sigmoid inside -- much slower, 70 epoches is not enough\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 140120 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XnwsA-GGn_HP","cellView":"form"},"source":["#@title rid144001 similar to r140120 with batch norm before sigmoid inside, 100 epoch\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 100\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"HhPCL3AzklZA"},"source":["#@title rid144002 similar to r140120 with batch norm before sigmoid inside, 150 epoch\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 144002 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 150\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"s7pdVpD2rop6"},"source":["#@title rid144003, load the result of 144001 as initial\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144003 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 70\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","# model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","model = torch.load('../data/nem_ss/models/model_rid144001.pt')\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('Hhat_rid144001.pt')\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KGjxHE2YoK5-","cellView":"form"},"source":["#@title rid144010 similar to r140120 with batch norm before sigmoid inside, double learning rate\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.002\n","opts['n_epochs'] = 100\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"xjQQ16SMr5ee"},"source":["#@title rid144011, load the result of 144010 as initial\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144011 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.002\n","opts['n_epochs'] = 70\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","# model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","model = torch.load('../data/nem_ss/models/model_rid144010.pt')\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","# Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Hhat = torch.load('../data/nem_ss/models/Hhat_rid144010.pt')\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"SMRJqMVX2zgE"},"source":["#@title rid144020 similar to r140120 with batch norm before sigmoid inside, triple learning rate\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144020 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.003\n","opts['n_epochs'] = 70\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IArHbiAj26iK","cellView":"form"},"source":["#@title rid144030 similar to r140120 with batch norm before sigmoid inside, *5 learning rate\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144030 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.005\n","opts['n_epochs'] = 70\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"F_CPQUlK9Agj"},"source":["#@title rid144040 similar to r140120 with batch norm before sigmoid inside, *7 learning rate\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144040 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.007\n","opts['n_epochs'] = 70\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-o9eCmap9A-9","cellView":"form"},"source":["#@title rid144050 similar to r140120 with batch norm before sigmoid inside, *10 learning rate\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144050 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.01\n","opts['n_epochs'] = 70\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Tfh9KrQhOWR","cellView":"form"},"source":["#@title rid144100 based on 140120, changed Hhat to M*J before NN with batch-norm before sigmoid -- result is very similar to 144000\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 143000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Hhat = Hhat.mean(0)\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"Nck9V_kmvS3B"},"source":["#@title rid144200 similar to r140120 with batch norm before sigmoid inside, with Hj normalized\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = Hhat/((Hhat.abs()**2).sum(1)**0.5)[:,None,:]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kKSSnCblvYdl","cellView":"form"},"source":["#@title rid144300 just as rid140120 with Hj normalized\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144300 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = Hhat/((Hhat.abs()**2).sum(1)**0.5)[:,None,:]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"1JGaDZoMonZ9"},"source":["#@title rid144400 based on 140120, changed Hhat to M*J AFTER NN -- should be marked as 143-...\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144400 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        if not (epoch==0 and i==0): Hhat = Hhat.mean(0)\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Hhat = Hhat.mean(0)\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HUa7cOJ9g1kG","cellView":"form"},"source":["#%%\n","#@title rid144500 relu as last layer with Hj normalized\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_relu as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144500 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = Hhat/((Hhat.abs()**2).sum(1)**0.5)[:,None,:]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')\n","#%%\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RhhnONHRg6S9","cellView":"form"},"source":["#@title rid144520 relu as last layer with Hj normalized, gradiant clip 0.5\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_relu as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 144520 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.002\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Hhat = Hhat/((Hhat.abs()**2).sum(1)**0.5)[:,None,:]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7asrenmVHxwA","cellView":"form"},"source":["#@title r145000 based on rid140130, 3 fewer batch norm to make it faster\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_lsbn as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 145000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"5CgNnL30QEUg"},"source":["#@title r145001 based on rid140130, 3 fewer batch norm to make it faster\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_lsbn as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 145001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o1FZghn9JDcF","cellView":"form"},"source":["#@title r145002 based on rid140130, 3 fewer batch norm to make it faster\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_lsbn as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 145002 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-6:-3])/3, sum(loss_tr[-3:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZII0tMY6JXHE","cellView":"form"},"source":["#@title r145100 based on rid140120, 3 fewer batch norm to make it faster -- not really working well\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_lsbn as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 145100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"RFuBLusSQNiP"},"source":["#@title r145101 based on rid140120, 3 fewer batch norm to make it faster\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_lsbn as UNetHalf\n","torch.manual_seed(1)\n","\n","rid = 145101 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(outs)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        outs = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(outs)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(model, f'model_rid{rid}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IL2C-7QgSA_m","cellView":"form"},"source":["#@title rid146000 less batch norm with bn before sigmoid, with stopping criteria 5e-4 -- learning rate too small, stopped\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_lsbn2 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 146000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[epoch-5:epoch-2])/3, sum(loss_tr[epoch-2:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        if (s1-s2)/s1 < 5e-4 :\n","            print('break-2')\n","            break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VM8tmI3wSEnj","cellView":"form"},"source":["#@title rid146010 less batch norm with bn before sigmoid, without stopping criteria 5e-4-- learning rate too small, stopped\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_lsbn2 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 146010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')    \n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[epoch-5:epoch-2])/3, sum(loss_tr[epoch-2:])/3\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     if (s1-s2)/s1 < 5e-4 :\n","    #         print('break-2')\n","    #         break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kxsYP4l2TOKh","cellView":"form"},"source":["#@title rid146110 warm start, warm shared Hhat, 16 layers, 2 channel input, gamma=label as random noise-- learning rate too small, stopped\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_lsbn2 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 146110 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XxomcurZT2Jn","cellView":"form"},"source":["#@title rid146111 warm start, warm shared Hhat, 16 layers, 2 channel input, gamma=label as random noise -- learning rate too small, stopped\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_lsbn2 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 146111 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","lb = torch.load('../data/nem_ss/xx_all_8by8.pt') # shape of [I,1,1,8,8]\n","lb = lb/lb.amax(dim=[3,4])[...,None,None]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","gtr = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(I, 1, 1, 1, 1)\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","        lb0 = lb[i*opts['batch_size']:(i+1)*opts['batch_size'], 0].cuda()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(torch.sigmoid(model(torch.cat((g[:,j], lb0), dim=1))))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"jwCeIVAfoX45"},"source":["#@title rid147000 similar to r140120 with batch norm before sigmoid inside  -- last layer with batchnorm or batchnor+relu are very slow\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig2 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 147000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[epoch-5:epoch-2])/3, sum(loss_tr[epoch-2:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"n0bMRtzfoepU"},"source":["#@title rid148000 similar to r140120 with batch norm before sigmoid inside -- last layer with batchnorm or batchnor+relu are very slow\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig3 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 148000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, f'loss_rid{rid}.pt')\n","    torch.save(model, f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[epoch-5:epoch-2])/3, sum(loss_tr[epoch-2:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zPjjM6pldFaJ","cellView":"form"},"source":["#@title rid149000 similar to r140120 with batch norm before sigmoid inside --last layer 1 conv changed to 3 conv\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig4 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 149000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-6:-3])/3, sum(loss_tr[-3:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"rTB0hXG9H2-m"},"source":["#@title rid149100 sigmoid changed to -- 1-conv relu+vj/vj.max \n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_2 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 149100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-6:-3])/3, sum(loss_tr[-3:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4o7usneqH6ce","cellView":"form"},"source":["#@title rid149200 sigmoid changed to 3-conv relu+vj/vj.max  -- shows up nan error\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_3 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 149200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-6:-3])/3, sum(loss_tr[-3:])/3\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w_il05ERz3u0"},"source":["## 15 series using new structures\n","basically, record all the models and loss, so far 150000_35 is chosen for the 3 class final data. Hope to defeat overfitting(after too many epoches vj is too small) and simple stopping criteria.\n","\n"]},{"cell_type":"code","metadata":{"id":"VyJGL4ru4Nb0","cellView":"form"},"source":["#@title rid150000 similar to 140100, but vj/vjmax_detach\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 150000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QUQvzApz6efF","cellView":"form"},"source":["#@title rid150100 similar to 149000, but vj/vjmax_detach\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_bnsig5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 150100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"AYau35bEZsW0"},"source":["#@title rid151000, same as rid140120, but record all the models\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 151000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    # if epoch >20 :\n","    #     s1, s2 = sum(loss_tr[((-2*n-10):(-n-10)])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"Caz1899ObdVo"},"source":["#@title rid151001, same as rid140120, but with stopping criteria and recording\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 151001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.rand(J,1,opts['d_gamma'],opts['d_gamma']).repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >20 :\n","        s1, s2 = sum(loss_tr[(-2*n-10):(-n-10)])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nigmG0MhF3Rq","cellView":"form"},"source":["#@title rid152000, same as rid140120, with validation\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_256_sig as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 152000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","d, *_ = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","xval = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xval[:200])\n","val = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gval = torch.tensor(resize(xval[...,0].abs(), [xval.shape[0],opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","g = torch.load('../data/nem_ss/gval_500.pt')\n","gval = g[:200]/g[:200].amax(dim=[1,2])[...,None,None]  #standardization \n","gval = torch.cat([gval[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I_val,J,1,8,8]\n","\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr, loss_val = [], [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","l = torch.load('../data/nem_ss/140100_lb.pt')\n","lb = l.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","    \n","    if epoch > 20:\n","        loss_val.append(val_run(val, gval, model, lb))\n","        torch.cuda.empty_cache()\n","        plt.figure()\n","        plt.plot(loss_val, '-or')\n","        plt.title(f'Val loss fuction up to epoch{epoch}')\n","        plt.savefig(fig_loc + f'id{rid}_ValLoss_epoch{epoch}')\n","        torch.save(loss_val, mod_loc +f'loss_val_rid{rid}.pt')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')\n","    \n","\n","    # if epoch >20 :\n","    #     s1, s2 = sum(loss_tr[(-2*n-10):(-n-10)])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hyhzT1CIcERI"},"source":["## 16 series -- working on the 6 classes data"]},{"cell_type":"code","metadata":{"id":"SU5mjShWcO0F","cellView":"form"},"source":["#@title rid160000 based on rid150000 for 6 classes\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 160000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"56FAuehPEraW"},"source":["#@title rid160001 based on rid150000 for 6 classes, batch size 64 is too big to run on colab\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_5 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 160001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jP_zaqohpWuK"},"source":["#@title rid160100 based on rid150000 for 6 classes\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 160100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"WrDopGHuvk4l"},"source":["#@title rid160200 based on rid150000 for 6 classes\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_7 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 160200 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"7WNqA_lGvit3"},"source":["#@title rid160201 based on rid150000 for 6 classes\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_7 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 160201 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 32\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM6FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# from skimage.transform import resize\n","# gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","#     order=1, preserve_range=True ))\n","# gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","# gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","gtr = torch.load('../data/nem_ss/gtr_c6_IJ188.pt') # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    if epoch >10 :\n","        s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","        if s1 - s2 < 0 :\n","            print('break-1')\n","            break\n","        print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","        if abs((s1-s2)/s1) < 5e-4 :\n","            print('break-2')\n","            break\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 17 series\n","170 Based on vj/vj.detach().max(), only train existing classes, really weakly setting\n","171 Unsupervised setting, trying to explore vj could be all zero for J<M, for the future weakly setting\n"],"metadata":{"id":"RsvZxNctBe2D"}},{"cell_type":"code","metadata":{"id":"XoZ7E4bDME0I","cellView":"form"},"source":["#@title rid170000 based on rid160100 for 6 classes\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","def batch_process(model, g, lb, idx):\n","    \"\"\"This function try to process forward pass in a batch way\n","\n","    Args:\n","        model (object): [Neural network]\n","        g (gamma): [shape of [I,J,1,8,8]]\n","        lb (regularizer): [shape of[I,J,1,8,8]]\n","        idx (which g,lb to use): [shape of [?,2]]\n","    \"\"\"\n","    G = g[idx[:,0], idx[:,1]]  # shape of [?<I,?<J,1, 8, 8]\n","    L = lb[idx[:,0], idx[:,1]]\n","    inputs = torch.cat((G, L), dim=2).reshape(-1, 2, L.shape[-2], L.shape[-1])\n","    outs = []\n","    bs, i = 30, 0  # batch size\n","    while i*bs <= inputs.shape[0]:\n","        outs.append(model(inputs[i*bs:i*bs+bs]).squeeze())\n","        i += 1 \n","    res = torch.cat(outs).reshape(G.shape[0], G.shape[1],100,100)\n","    return res\n","\n","rid = 170000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 2850 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 50\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # how many samples to average for stopping \n","\n","d, lb = torch.load('../data/nem_ss/weakly50percomb_tr3kM6FT100_xlb.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","lbs = torch.zeros(I, J)\n","for i, v in enumerate(lb):\n","    lbs[i*50:(i+1)*50, v] = 1\n","\"shuffle data\"\n","ind = torch.randperm(I)\n","xtr, lbs = xtr[ind], lbs[ind]\n","data = Data.TensorDataset(xtr, lbs)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.ones(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.ones(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x, y) in enumerate(tr): # x is data, y is label\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat@Rs.permute(1,2,0,3,4)@Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj, s = [], y.sum() # s means how many components in one batch\n","        idx = torch.nonzero(y)\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4)@Hhat.transpose(-1,-2).conj()@Rx.inverse()  #shape of [N,F,I,J,M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - \\\n","                (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF #shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            \"calculate vj\"\n","            out = torch.ones(vhat.shape, device=vhat.device)*1e-20\n","            out[idx[:,0],:,:,idx[:,1]] = batch_process(model, g, lb, idx)[:,0]\n","            vhat.real = threshold(out)\n","            \n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_lh(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.detach().item()/s)\n","            if torch.isnan(ll_traj[-1]) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        out = torch.ones(vhat.shape, device=vhat.device)*1e-20\n","        out[idx[:,0],:,:,idx[:,1]] = batch_process(model, g, lb, idx)[:,0]\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        loss = loss_func(vhat, Rsshatnf.cuda())\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item()/s)\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item()/s)\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title rid170100 based on rid170000 for 6 classes, not shared H\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import UNetHalf8to100_vjto1_6 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","def batch_process(model, g, lb, idx):\n","    \"\"\"This function try to process forward pass in a batch way\n","\n","    Args:\n","        model (object): [Neural network]\n","        g (gamma): [shape of [I,J,1,8,8]]\n","        lb (regularizer): [shape of[I,J,1,8,8]]\n","        idx (which g,lb to use): [shape of [?,2]]\n","    \"\"\"\n","    G = g[idx[:,0], idx[:,1]]  # shape of [?<I,?<J,1, 8, 8]\n","    L = lb[idx[:,0], idx[:,1]]\n","    inputs = torch.cat((G, L), dim=2).reshape(-1, 2, L.shape[-2], L.shape[-1])\n","    outs = []\n","    bs, i = 30, 0  # batch size\n","    while i*bs <= inputs.shape[0]:\n","        outs.append(model(inputs[i*bs:i*bs+bs]).squeeze())\n","        i += 1 \n","    res = torch.cat(outs).reshape(G.shape[0], G.shape[1],100,100)\n","    return res\n","\n","rid = 170100 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 2850 # how many samples\n","M, N, F, J = 6, 100, 100, 6\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 50\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5  # how many samples to average for stopping \n","\n","d, lb = torch.load('../data/nem_ss/weakly50percomb_tr3kM6FT100_xlb.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","lbs = torch.zeros(I, J)\n","for i, v in enumerate(lb):\n","    lbs[i*50:(i+1)*50, v] = 1\n","\"shuffle data\"\n","ind = torch.randperm(I)\n","xtr, lbs = xtr[ind], lbs[ind]\n","data = Data.TensorDataset(xtr, lbs)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","from skimage.transform import resize\n","gtr = torch.tensor(resize(xtr[...,0].abs(), [I,opts['d_gamma'],opts['d_gamma']],\\\n","    order=1, preserve_range=True ))\n","gtr = gtr/gtr.amax(dim=[1,2])[...,None,None]  #standardization \n","gtr = torch.cat([gtr[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I,J,1,8,8]\n","\n","loss_iter, loss_tr = [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","Htr = torch.ones(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","# Hhat = torch.ones(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","lb = torch.load('../data/nem_ss/lb_c6_J188.pt') # shape of [J,1,8,8], cpu()\n","lb = lb.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x, y) in enumerate(tr): # x is data, y is label\n","        Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat@Rs.permute(1,2,0,3,4)@Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj, s = [], y.sum() # s means how many components in one batch\n","        idx = torch.nonzero(y)\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4)@Hhat.transpose(-1,-2).conj()@Rx.inverse()  #shape of [N,F,I,J,M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - \\\n","                (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF #shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            \"calculate vj\"\n","            out = torch.ones(vhat.shape, device=vhat.device)*1e-20\n","            out[idx[:,0],:,:,idx[:,1]] = batch_process(model, g, lb, idx)[:,0]\n","            vhat.real = threshold(out)\n","            \n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_lh(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.detach().item()/s)\n","            if torch.isnan(ll_traj[-1]) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        out = torch.ones(vhat.shape, device=vhat.device)*1e-20\n","        out[idx[:,0],:,:,idx[:,1]] = batch_process(model, g, lb, idx)[:,0]\n","        vhat.real = threshold(out)\n","        optimizer.zero_grad()         \n","        loss = loss_func(vhat, Rsshatnf.cuda())\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item()/s)\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item()/s)\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction at epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')    \n","    # if epoch >10 :\n","    #     s1, s2 = sum(loss_tr[-n*2:-n])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"],"metadata":{"cellView":"form","id":"uDswEqGw3iYG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title rid171000, relu, ceiling=1\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import Model171000 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 171000 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","# d, *_ = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","# xval = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","# data = Data.TensorDataset(xval[:200])\n","# val = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# # from skimage.transform import resize\n","# # gval = torch.tensor(resize(xval[...,0].abs(), [xval.shape[0],opts['d_gamma'],opts['d_gamma']],\\\n","# #     order=1, preserve_range=True ))\n","# g = torch.load('../data/nem_ss/gval_500.pt')\n","# gval = g[:200]/g[:200].amax(dim=[1,2])[...,None,None]  #standardization \n","# gval = torch.cat([gval[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I_val,J,1,8,8]\n","\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr, loss_val = [], [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","l = torch.load('../data/nem_ss/140100_lb.pt')\n","lb = l.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out, ceiling=1)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out,ceiling=1)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","    \n","    # if epoch > 20:\n","    #     loss_val.append(val_run(val, gval, model, lb))\n","    #     torch.cuda.empty_cache()\n","    #     plt.figure()\n","    #     plt.plot(loss_val, '-or')\n","    #     plt.title(f'Val loss fuction up to epoch{epoch}')\n","    #     plt.savefig(fig_loc + f'id{rid}_ValLoss_epoch{epoch}')\n","    #     torch.save(loss_val, mod_loc +f'loss_val_rid{rid}.pt')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')\n","    \n","\n","    # if epoch >20 :\n","    #     s1, s2 = sum(loss_tr[(-2*n-10):(-n-10)])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"],"metadata":{"cellView":"form","id":"0A9dyp6B3mmU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title rid171001, relu, ceiling=1, not*3\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import Model171000 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 171001 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","# d, *_ = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","# xval = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","# data = Data.TensorDataset(xval[:200])\n","# val = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# # from skimage.transform import resize\n","# # gval = torch.tensor(resize(xval[...,0].abs(), [xval.shape[0],opts['d_gamma'],opts['d_gamma']],\\\n","# #     order=1, preserve_range=True ))\n","# g = torch.load('../data/nem_ss/gval_500.pt')\n","# gval = g[:200]/g[:200].amax(dim=[1,2])[...,None,None]  #standardization \n","# gval = torch.cat([gval[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I_val,J,1,8,8]\n","\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr, loss_val = [], [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","l = torch.load('../data/nem_ss/140100_lb.pt')\n","lb = l.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out, ceiling=1)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out,ceiling=1)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","    \n","    # if epoch > 20:\n","    #     loss_val.append(val_run(val, gval, model, lb))\n","    #     torch.cuda.empty_cache()\n","    #     plt.figure()\n","    #     plt.plot(loss_val, '-or')\n","    #     plt.title(f'Val loss fuction up to epoch{epoch}')\n","    #     plt.savefig(fig_loc + f'id{rid}_ValLoss_epoch{epoch}')\n","    #     torch.save(loss_val, mod_loc +f'loss_val_rid{rid}.pt')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')\n","    \n","\n","    # if epoch >20 :\n","    #     s1, s2 = sum(loss_tr[(-2*n-10):(-n-10)])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"],"metadata":{"cellView":"form","id":"Bfxp7NsO9qNg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title rid171010, e^x, ceiling=1\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import Model171010 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 171010 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","# d, *_ = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","# xval = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","# data = Data.TensorDataset(xval[:200])\n","# val = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# # from skimage.transform import resize\n","# # gval = torch.tensor(resize(xval[...,0].abs(), [xval.shape[0],opts['d_gamma'],opts['d_gamma']],\\\n","# #     order=1, preserve_range=True ))\n","# g = torch.load('../data/nem_ss/gval_500.pt')\n","# gval = g[:200]/g[:200].amax(dim=[1,2])[...,None,None]  #standardization \n","# gval = torch.cat([gval[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I_val,J,1,8,8]\n","\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr, loss_val = [], [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","l = torch.load('../data/nem_ss/140100_lb.pt')\n","lb = l.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.001)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out, ceiling=1)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out,ceiling=1)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","    \n","    # if epoch > 20:\n","    #     loss_val.append(val_run(val, gval, model, lb))\n","    #     torch.cuda.empty_cache()\n","    #     plt.figure()\n","    #     plt.plot(loss_val, '-or')\n","    #     plt.title(f'Val loss fuction up to epoch{epoch}')\n","    #     plt.savefig(fig_loc + f'id{rid}_ValLoss_epoch{epoch}')\n","    #     torch.save(loss_val, mod_loc +f'loss_val_rid{rid}.pt')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')\n","    \n","\n","    # if epoch >20 :\n","    #     s1, s2 = sum(loss_tr[(-2*n-10):(-n-10)])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"],"metadata":{"cellView":"form","id":"xAnhPbwk9rZc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title rid171011, e^x, ceiling=1, gamma rate=0.01\n","from utils import *\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","plt.rcParams['figure.dpi'] = 100\n","torch.set_printoptions(linewidth=160)\n","torch.set_default_dtype(torch.double)\n","from unet.unet_model import Model171010 as UNetHalf\n","from datetime import datetime\n","print('starting date time ', datetime.now())\n","torch.manual_seed(1)\n","\n","rid = 171011 # running id\n","fig_loc = '../data/nem_ss/figures/'\n","mod_loc = '../data/nem_ss/models/'\n","if not(os.path.isdir(fig_loc + f'/rid{rid}/')): \n","    print('made a new folder')\n","    os.mkdir(fig_loc + f'rid{rid}/')\n","    os.mkdir(mod_loc + f'rid{rid}/')\n","fig_loc = fig_loc + f'rid{rid}/'\n","mod_loc = mod_loc + f'rid{rid}/'\n","\n","I = 3000 # how many samples\n","M, N, F, J = 3, 100, 100, 3\n","NF = N*F\n","eps = 5e-4\n","opts = {}\n","opts['n_ch'] = [2,1]  \n","opts['batch_size'] = 64\n","opts['EM_iter'] = 201\n","opts['lr'] = 0.001\n","opts['n_epochs'] = 71\n","opts['d_gamma'] = 8 \n","n = 5\n","\n","d = torch.load('../data/nem_ss/tr3kM3FT100.pt')\n","xtr = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]).permute(0,2,3,1)# [sample, N, F, channel]\n","data = Data.TensorDataset(xtr)\n","tr = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","\n","# d, *_ = torch.load('../data/nem_ss/val500M3FT100_xsh.pt')\n","# xval = (d/d.abs().amax(dim=(1,2,3))[:,None,None,None]*3).permute(0,2,3,1)# [sample, N, F, channel]\n","# data = Data.TensorDataset(xval[:200])\n","# val = Data.DataLoader(data, batch_size=opts['batch_size'], drop_last=True)\n","# # from skimage.transform import resize\n","# # gval = torch.tensor(resize(xval[...,0].abs(), [xval.shape[0],opts['d_gamma'],opts['d_gamma']],\\\n","# #     order=1, preserve_range=True ))\n","# g = torch.load('../data/nem_ss/gval_500.pt')\n","# gval = g[:200]/g[:200].amax(dim=[1,2])[...,None,None]  #standardization \n","# gval = torch.cat([gval[:,None] for j in range(J)], dim=1)[:,:,None] # shape of [I_val,J,1,8,8]\n","\n","gtr = torch.load('../data/nem_ss/xx_all_8by8.pt')\n","gtr = gtr/gtr.amax(dim=[3,4])[...,None,None]\n","gtr = torch.cat([gtr for j in range(J)], dim=1)\n","\n","loss_iter, loss_tr, loss_val = [], [], []\n","model = UNetHalf(opts['n_ch'][0], opts['n_ch'][1]).cuda()\n","optimizer = optim.RAdam(model.parameters(),\n","                lr= opts['lr'],\n","                betas=(0.9, 0.999), \n","                eps=1e-8,\n","                weight_decay=0)\n","\"initial\"\n","vtr = torch.randn(N, F, J).abs().to(torch.cdouble).repeat(I, 1, 1, 1)\n","# Htr = torch.randn(M, J).to(torch.cdouble).repeat(I, 1, 1)\n","Hhat = torch.randn(M, J).to(torch.cdouble).cuda()\n","Rbtr = torch.ones(I, M).diag_embed().to(torch.cdouble)*100\n","l = torch.load('../data/nem_ss/140100_lb.pt')\n","lb = l.repeat(opts['batch_size'], 1, 1, 1, 1).cuda()\n","\n","for epoch in range(opts['n_epochs']):    \n","    for param in model.parameters():\n","        param.requires_grad_(False)\n","    model.eval()\n","\n","    for i, (x,) in enumerate(tr): # gamma [n_batch, 4, 4]\n","        #%% EM part\n","        # Hhat = Htr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        vhat = vtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()        \n","        Rb = Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda()\n","        g = gtr[i*opts['batch_size']:(i+1)*opts['batch_size']].cuda().requires_grad_()\n","\n","        x = x.cuda()\n","        optim_gamma = torch.optim.SGD([g], lr=0.01)\n","        Rxxhat = (x[...,None] @ x[..., None, :].conj()).sum((1,2))/NF\n","        Rs = vhat.diag_embed() # shape of [I, N, F, J, J]\n","        Rx = Hhat @ Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() + Rb # shape of [N,F,I,M,M]\n","        ll_traj = []\n","\n","        for ii in range(opts['EM_iter']):\n","            \"E-step\"\n","            W = Rs.permute(1,2,0,3,4) @ Hhat.transpose(-1,-2).conj() @ Rx.inverse()  # shape of [N, F, I, J, M]\n","            shat = W.permute(2,0,1,3,4) @ x[...,None]\n","            Rsshatnf = shat @ shat.transpose(-1,-2).conj() + Rs - (W@Hhat@Rs.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n","            Rsshat = Rsshatnf.sum([1,2])/NF # shape of [I, J, J]\n","            Rxshat = (x[..., None] @ shat.transpose(-1,-2).conj()).sum((1,2))/NF # shape of [I, M, J]\n","\n","            \"M-step\"\n","            Hhat = Rxshat @ Rsshat.inverse() # shape of [I, M, J]\n","            # Hhat = (Rxshat @ Rsshat.inverse()).mean(0) # shape of [M, J]\n","            Rb = Rxxhat - Hhat@Rxshat.transpose(-1,-2).conj() - \\\n","                Rxshat@Hhat.transpose(-1,-2).conj() + Hhat@Rsshat@Hhat.transpose(-1,-2).conj()\n","            Rb = Rb.diagonal(dim1=-1, dim2=-2).diag_embed()\n","            Rb.imag = Rb.imag - Rb.imag\n","\n","            # vj = Rsshatnf.diagonal(dim1=-1, dim2=-2)\n","            # vj.imag = vj.imag - vj.imag\n","            outs = []\n","            for j in range(J):\n","                outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","            out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","            vhat.real = threshold(out, ceiling=1)\n","            loss = loss_func(vhat, Rsshatnf.cuda())\n","            optim_gamma.zero_grad()   \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_([g], max_norm=1)\n","            optim_gamma.step()\n","            torch.cuda.empty_cache()\n","            \n","            \"compute log-likelyhood\"\n","            vhat = vhat.detach()\n","            ll, Rs, Rx = log_likelihood(x, vhat, Hhat, Rb)\n","            ll_traj.append(ll.item())\n","            if torch.isnan(torch.tensor(ll_traj[-1])) : input('nan happened')\n","            if ii > 5 and abs((ll_traj[ii] - ll_traj[ii-3])/ll_traj[ii-3])<eps:\n","                print(f'EM early stop at iter {ii}, batch {i}, epoch {epoch}')\n","                break\n","    \n","        print(f'batch {i} is done')\n","        if i == 0 :\n","            plt.figure()\n","            plt.plot(ll_traj, '-x')\n","            plt.title(f'the log-likelihood of the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_log-likelihood_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,0].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'1st source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj1_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,1].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'2nd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj2_epoch{epoch}')\n","\n","            plt.figure()\n","            plt.imshow(vhat[0,...,2].real.cpu())\n","            plt.colorbar()\n","            plt.title(f'3rd source of vj in first sample from the first batch at epoch {epoch}')\n","            plt.savefig(fig_loc + f'id{rid}_vj3_epoch{epoch}')\n","\n","        #%% update variable\n","        with torch.no_grad():\n","            gtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = g.cpu()\n","            vtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = vhat.cpu()\n","            # Htr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Hhat.cpu()\n","            Rbtr[i*opts['batch_size']:(i+1)*opts['batch_size']] = Rb.cpu()\n","        g.requires_grad_(False)\n","        model.train()\n","        for param in model.parameters():\n","            param.requires_grad_(True)\n","\n","        outs = []\n","        for j in range(J):\n","            outs.append(model(torch.cat((g[:,j], lb[:,j]), dim=1)))\n","        out = torch.cat(outs, dim=1).permute(0,2,3,1)\n","        vhat.real = threshold(out,ceiling=1)\n","        optimizer.zero_grad()         \n","        ll, *_ = log_likelihood(x, vhat, Hhat, Rb)\n","        loss = -ll\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        torch.cuda.empty_cache()\n","        loss_iter.append(loss.detach().cpu().item())\n","\n","    print(f'done with epoch{epoch}')\n","    plt.figure()\n","    plt.plot(loss_iter, '-xr')\n","    plt.title(f'Loss fuction of all the iterations up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFunAll_epoch{epoch}')\n","\n","    loss_tr.append(loss.detach().cpu().item())\n","    plt.figure()\n","    plt.plot(loss_tr, '-or')\n","    plt.title(f'Loss fuction up to epoch{epoch}')\n","    plt.savefig(fig_loc + f'id{rid}_LossFun_epoch{epoch}')\n","    \n","    # if epoch > 20:\n","    #     loss_val.append(val_run(val, gval, model, lb))\n","    #     torch.cuda.empty_cache()\n","    #     plt.figure()\n","    #     plt.plot(loss_val, '-or')\n","    #     plt.title(f'Val loss fuction up to epoch{epoch}')\n","    #     plt.savefig(fig_loc + f'id{rid}_ValLoss_epoch{epoch}')\n","    #     torch.save(loss_val, mod_loc +f'loss_val_rid{rid}.pt')\n","\n","    plt.close('all')  # to avoid warnings\n","    torch.save(loss_tr, mod_loc +f'loss_rid{rid}.pt')\n","    torch.save(model, mod_loc +f'model_rid{rid}_{epoch}.pt')\n","    torch.save(Hhat, mod_loc +f'Hhat_rid{rid}_{epoch}.pt')\n","    \n","\n","    # if epoch >20 :\n","    #     s1, s2 = sum(loss_tr[(-2*n-10):(-n-10)])/n, sum(loss_tr[-n:])/n\n","    #     if s1 - s2 < 0 :\n","    #         print('break-1')\n","    #         break\n","    #     print(f'{epoch}-abs((s1-s2)/s1):', abs((s1-s2)/s1))\n","    #     if abs((s1-s2)/s1) < 5e-4 :\n","    #         print('break-2')\n","    #         break"],"metadata":{"id":"mlESL3Qj-5ZM","cellView":"form"},"execution_count":null,"outputs":[]}]}